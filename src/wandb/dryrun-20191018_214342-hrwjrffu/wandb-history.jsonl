{"train-loss": 6.714214324951172, "train-mrr": {"_type": "histogram", "values": [618, 16, 9, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "bins": [0.0015408321050927043, 0.032742682844400406, 0.06394453346729279, 0.09514638036489487, 0.12634822726249695, 0.15755008161067963, 0.1887519210577011, 0.2199537754058838, 0.2511556148529053, 0.28235748410224915, 0.31355932354927063, 0.3447611629962921, 0.375963032245636, 0.40716487169265747, 0.43836671113967896, 0.4695685803890228, 0.5007703900337219, 0.5319722890853882, 0.5631741285324097, 0.5943759679794312, 0.6255778074264526, 0.6567796468734741, 0.6879814863204956, 0.7191833853721619, 0.7503852248191833, 0.7815870642662048, 0.8127889037132263, 0.8439907431602478, 0.8751925826072693, 0.9063944816589355, 0.937596321105957, 0.9687981605529785, 1.0]}, "_runtime": 149.1134946346283, "_timestamp": 1571435170.2395966, "_step": 0}
{"epoch": 0, "train-loss": 6.1735187198804775, "train-mrr": 0.04049374591546712, "train-time-sec": 18.197546243667603, "val-loss": 6.302980627332415, "val-mrr": 0.032014782035743795, "val-time-sec": 5.369245767593384, "_runtime": 168.3700635433197, "_timestamp": 1571435189.4961624, "_step": 1}
{"train-loss": 5.42209529876709, "train-mrr": {"_type": "histogram", "values": [406, 77, 37, 25, 9, 4, 12, 13, 0, 0, 19, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32], "bins": [0.0015847861068323255, 0.032785262912511826, 0.06398573517799377, 0.09518621116876602, 0.12638668715953827, 0.1575871706008911, 0.18878763914108276, 0.21998810768127441, 0.25118857622146606, 0.2823890745639801, 0.31358954310417175, 0.3447900116443634, 0.37599048018455505, 0.4071909785270691, 0.43839144706726074, 0.4695919156074524, 0.500792384147644, 0.5319928526878357, 0.5631933212280273, 0.594393789768219, 0.6255943179130554, 0.6567947864532471, 0.6879952549934387, 0.7191957235336304, 0.750396192073822, 0.7815966606140137, 0.8127971291542053, 0.843997597694397, 0.8751981258392334, 0.906398594379425, 0.9375990629196167, 0.9687995314598083, 1.0]}, "_runtime": 171.84781765937805, "_timestamp": 1571435192.9739168, "_step": 2}
{"epoch": 1, "train-loss": 4.939659149750419, "train-mrr": 0.15393078960303877, "train-time-sec": 14.178159713745117, "val-loss": 6.090662002563477, "val-mrr": 0.06168794250488281, "val-time-sec": 5.029137849807739, "_runtime": 190.74088382720947, "_timestamp": 1571435211.8669825, "_step": 3}
{"train-loss": 4.133771896362305, "train-mrr": {"_type": "histogram", "values": [199, 75, 62, 36, 19, 24, 19, 26, 0, 0, 40, 0, 0, 0, 0, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102], "bins": [0.001742160296998918, 0.03293771669268608, 0.06413327157497406, 0.09532883018255234, 0.12652438879013062, 0.1577199548482895, 0.18891550600528717, 0.22011105716228485, 0.2513066232204437, 0.2825021743774414, 0.3136977255344391, 0.34489330649375916, 0.37608885765075684, 0.4072844088077545, 0.4384799599647522, 0.4696755111217499, 0.5008710622787476, 0.5320666432380676, 0.5632622241973877, 0.594457745552063, 0.6256533265113831, 0.6568488478660583, 0.6880444288253784, 0.7192400097846985, 0.7504355311393738, 0.7816311120986938, 0.8128266334533691, 0.8440222144126892, 0.8752177953720093, 0.9064133167266846, 0.9376088976860046, 0.9688044190406799, 1.0]}, "_runtime": 194.0945737361908, "_timestamp": 1571435215.2206733, "_step": 4}
{"epoch": 2, "train-loss": 3.7943550918413247, "train-mrr": 0.31564737122194425, "train-time-sec": 14.150066137313843, "val-loss": 5.96215843473162, "val-mrr": 0.09625789382431534, "val-time-sec": 5.030535936355591, "_runtime": 212.96742510795593, "_timestamp": 1571435234.0935237, "_step": 5}
{"train-loss": 3.135270357131958, "train-mrr": {"_type": "histogram", "values": [100, 51, 46, 41, 18, 26, 25, 41, 0, 0, 39, 0, 0, 0, 0, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 199], "bins": [0.0026385225355625153, 0.03380606696009636, 0.0649736151099205, 0.09614115953445435, 0.1273087114095688, 0.15847624838352203, 0.18964380025863647, 0.22081135213375092, 0.25197890400886536, 0.2831464409828186, 0.31431397795677185, 0.3454815447330475, 0.37664908170700073, 0.407816618680954, 0.4389841556549072, 0.47015172243118286, 0.5013192892074585, 0.5324867963790894, 0.563654363155365, 0.5948218703269958, 0.6259894371032715, 0.6571570038795471, 0.688324511051178, 0.7194920778274536, 0.7506596446037292, 0.7818271517753601, 0.8129947185516357, 0.8441622853279114, 0.8753297924995422, 0.9064973592758179, 0.9376649260520935, 0.9688324332237244, 1.0]}, "_runtime": 216.31262946128845, "_timestamp": 1571435237.438728, "_step": 6}
{"epoch": 3, "train-loss": 2.8294260087220566, "train-mrr": 0.47869984094115803, "train-time-sec": 14.175302267074585, "val-loss": 6.097715146200997, "val-mrr": 0.12778396438766312, "val-time-sec": 5.020669937133789, "_runtime": 235.1964728832245, "_timestamp": 1571435256.322572, "_step": 7}
{"train-loss": 2.344555377960205, "train-mrr": {"_type": "histogram", "values": [48, 51, 28, 29, 10, 10, 19, 34, 0, 0, 55, 0, 0, 0, 0, 81, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 285], "bins": [0.0019762846641242504, 0.033164527267217636, 0.06435276567935944, 0.09554100781679153, 0.12672924995422363, 0.15791748464107513, 0.18910573422908783, 0.22029396891593933, 0.251482218503952, 0.2826704680919647, 0.31385868787765503, 0.3450469374656677, 0.3762351870536804, 0.4074234068393707, 0.4386116564273834, 0.4697999060153961, 0.5009881258010864, 0.5321763753890991, 0.5633646249771118, 0.5945528745651245, 0.6257411241531372, 0.6569293737411499, 0.6881175637245178, 0.7193058133125305, 0.7504940629005432, 0.7816823124885559, 0.8128705620765686, 0.8440588116645813, 0.875247061252594, 0.9064352512359619, 0.9376235008239746, 0.9688117504119873, 1.0]}, "_runtime": 238.53416776657104, "_timestamp": 1571435259.6602666, "_step": 8}
{"epoch": 4, "train-loss": 2.12992104758387, "train-mrr": 0.6092324849514659, "train-time-sec": 14.128997564315796, "val-loss": 6.186687197004046, "val-mrr": 0.14916168497944926, "val-time-sec": 5.021295070648193, "_runtime": 257.37870597839355, "_timestamp": 1571435278.5048046, "_step": 9}
{"train-loss": 1.6950942277908325, "train-mrr": {"_type": "histogram", "values": [26, 23, 13, 15, 8, 13, 18, 25, 0, 0, 49, 0, 0, 0, 0, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 383], "bins": [0.0044843051582574844, 0.03559416905045509, 0.06670403480529785, 0.09781390428543091, 0.12892377376556396, 0.16003362834453583, 0.19114349782466888, 0.22225336730480194, 0.2533632218837738, 0.28447309136390686, 0.3155829608440399, 0.346692830324173, 0.37780269980430603, 0.4089125692844391, 0.44002240896224976, 0.4711322784423828, 0.5022421479225159, 0.5333520174026489, 0.564461886882782, 0.595571756362915, 0.6266816258430481, 0.6577914953231812, 0.6889013648033142, 0.7200112342834473, 0.7511211037635803, 0.7822309136390686, 0.8133407831192017, 0.8444506525993347, 0.8755605220794678, 0.9066703915596008, 0.9377802610397339, 0.9688901305198669, 1.0]}, "_runtime": 260.74395179748535, "_timestamp": 1571435281.8700507, "_step": 10}
{"epoch": 5, "train-loss": 1.6186766365300054, "train-mrr": 0.702274246470984, "train-time-sec": 14.168895244598389, "val-loss": 6.488330214364188, "val-mrr": 0.17117069152161316, "val-time-sec": 5.025956630706787, "_runtime": 279.63162755966187, "_timestamp": 1571435300.7577264, "_step": 11}
{"train-loss": 1.2125866413116455, "train-mrr": {"_type": "histogram", "values": [9, 14, 9, 7, 5, 8, 12, 25, 0, 0, 38, 0, 0, 0, 0, 79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 444], "bins": [0.006211180239915848, 0.03726708143949509, 0.06832297891378403, 0.09937888383865356, 0.1304347813129425, 0.16149067878723145, 0.19254659116268158, 0.22360248863697052, 0.25465837121009827, 0.2857142984867096, 0.31677019596099854, 0.3478260934352875, 0.3788819909095764, 0.40993788838386536, 0.4409937858581543, 0.47204968333244324, 0.5031055808067322, 0.5341615080833435, 0.5652173757553101, 0.5962733030319214, 0.6273291707038879, 0.6583850979804993, 0.6894409656524658, 0.7204968929290771, 0.7515528202056885, 0.782608687877655, 0.8136646151542664, 0.8447204828262329, 0.8757764101028442, 0.9068322777748108, 0.9378882050514221, 0.9689440727233887, 1.0]}, "_runtime": 283.0076107978821, "_timestamp": 1571435304.1337101, "_step": 12}
{"epoch": 6, "train-loss": 1.238422383432803, "train-mrr": 0.7747016483166544, "train-time-sec": 14.190964937210083, "val-loss": 6.811928244999477, "val-mrr": 0.17788123464060354, "val-time-sec": 5.026867389678955, "_runtime": 301.9113304615021, "_timestamp": 1571435323.0374293, "_step": 13}
{"train-loss": 0.9185735583305359, "train-mrr": {"_type": "histogram", "values": [8, 9, 4, 8, 5, 8, 0, 10, 0, 0, 25, 0, 0, 0, 0, 91, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 482], "bins": [0.016949152573943138, 0.04766949266195297, 0.07838983088731766, 0.10911016911268234, 0.13983051478862762, 0.1705508530139923, 0.201271191239357, 0.23199152946472168, 0.26271185278892517, 0.29343220591545105, 0.32415252923965454, 0.3548728823661804, 0.3855932056903839, 0.4163135588169098, 0.44703391194343567, 0.47775423526763916, 0.508474588394165, 0.5391949415206909, 0.569915235042572, 0.6006355881690979, 0.6313559412956238, 0.6620762944221497, 0.6927965879440308, 0.7235169410705566, 0.7542372941970825, 0.7849576473236084, 0.8156779408454895, 0.8463982939720154, 0.8771186470985413, 0.9078390002250671, 0.9385592937469482, 0.9692796468734741, 1.0]}, "_runtime": 305.19480752944946, "_timestamp": 1571435326.3209064, "_step": 14}
{"epoch": 7, "train-loss": 0.9576634505520696, "train-mrr": 0.8257314583768812, "train-time-sec": 14.205710172653198, "val-loss": 7.131208774021694, "val-mrr": 0.18874941781850962, "val-time-sec": 5.055156230926514, "_runtime": 324.1494905948639, "_timestamp": 1571435345.2755895, "_step": 15}
{"train-loss": 0.7546549439430237, "train-mrr": {"_type": "histogram", "values": [1, 8, 2, 2, 1, 8, 5, 17, 0, 0, 22, 0, 0, 0, 0, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 519], "bins": [0.004098360426723957, 0.03522028774023056, 0.06634221225976944, 0.09746413677930832, 0.1285860687494278, 0.15970799326896667, 0.19082991778850555, 0.22195184230804443, 0.2530737817287445, 0.2841956913471222, 0.31531763076782227, 0.34643954038619995, 0.3775614798069, 0.4086833894252777, 0.4398053288459778, 0.47092726826667786, 0.5020492076873779, 0.5331711173057556, 0.5642930269241333, 0.595414936542511, 0.6265369057655334, 0.6576588153839111, 0.6887807250022888, 0.7199026346206665, 0.751024603843689, 0.7821465134620667, 0.8132684230804443, 0.8443903923034668, 0.8755123019218445, 0.9066342115402222, 0.9377561211585999, 0.9688780903816223, 1.0]}, "_runtime": 327.43795943260193, "_timestamp": 1571435348.5640588, "_step": 16}
{"epoch": 8, "train-loss": 0.7899706039739691, "train-mrr": 0.8572929295568562, "train-time-sec": 14.199668884277344, "val-loss": 7.478824315752302, "val-mrr": 0.19236109605726304, "val-time-sec": 5.04452657699585, "_runtime": 346.3668568134308, "_timestamp": 1571435367.4929557, "_step": 17}
{"train-loss": 0.654825747013092, "train-mrr": {"_type": "histogram", "values": [4, 3, 5, 5, 1, 2, 3, 7, 0, 0, 13, 0, 0, 0, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 549], "bins": [0.0028901733458042145, 0.034049853682518005, 0.0652095377445221, 0.09636922180652618, 0.12752890586853027, 0.15868858993053436, 0.18984825909137726, 0.22100794315338135, 0.25216764211654663, 0.2833273112773895, 0.3144869804382324, 0.3456466794013977, 0.3768063485622406, 0.4079660475254059, 0.4391257166862488, 0.47028541564941406, 0.5014451146125793, 0.5326047539710999, 0.5637644529342651, 0.5949241518974304, 0.6260837912559509, 0.6572434902191162, 0.6884031891822815, 0.7195628881454468, 0.7507225275039673, 0.7818822264671326, 0.8130419254302979, 0.8442015647888184, 0.8753612637519836, 0.9065209627151489, 0.9376806616783142, 0.9688403010368347, 1.0]}, "_runtime": 349.64875507354736, "_timestamp": 1571435370.7748537, "_step": 18}
{"epoch": 9, "train-loss": 0.6628069385238315, "train-mrr": 0.8805457890233068, "train-time-sec": 14.192821264266968, "val-loss": 7.562115805489676, "val-mrr": 0.19584468866704585, "val-time-sec": 5.022902727127075, "_runtime": 368.557293176651, "_timestamp": 1571435389.683392, "_step": 19}
{"train-loss": 0.4794447720050812, "train-mrr": {"_type": "histogram", "values": [3, 3, 1, 0, 3, 0, 4, 6, 0, 0, 14, 0, 0, 0, 0, 55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 561], "bins": [0.008196720853447914, 0.03919057175517082, 0.07018442451953888, 0.10117828100919724, 0.1321721374988556, 0.16316598653793335, 0.1941598355770111, 0.22515368461608887, 0.2561475336551666, 0.2871413826942444, 0.31813523173332214, 0.3491291105747223, 0.38012295961380005, 0.4111168086528778, 0.44211065769195557, 0.4731045067310333, 0.5040983557701111, 0.5350922346115112, 0.5660860538482666, 0.5970799326896667, 0.6280737519264221, 0.6590676307678223, 0.6900614500045776, 0.7210553288459778, 0.7520492076873779, 0.7830430269241333, 0.8140369057655334, 0.8450307250022888, 0.876024603843689, 0.9070184230804443, 0.9380123019218445, 0.9690061211585999, 1.0]}, "_runtime": 371.83513712882996, "_timestamp": 1571435392.961236, "_step": 20}
{"epoch": 10, "train-loss": 0.596103376020556, "train-mrr": 0.8928211061611622, "train-time-sec": 14.169588327407837, "val-loss": 8.05098635809762, "val-mrr": 0.1991444306425996, "val-time-sec": 5.026949405670166, "_runtime": 390.7232747077942, "_timestamp": 1571435411.849374, "_step": 21}
{"train-loss": 0.48705148696899414, "train-mrr": {"_type": "histogram", "values": [3, 0, 5, 2, 3, 1, 4, 3, 0, 0, 12, 0, 0, 0, 0, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 572], "bins": [0.002923976629972458, 0.034082602709531784, 0.06524122506380081, 0.09639985114336014, 0.12755848467350006, 0.1587171107530594, 0.1898757368326187, 0.22103436291217804, 0.25219297409057617, 0.2833516001701355, 0.3145102262496948, 0.34566885232925415, 0.3768274784088135, 0.4079861044883728, 0.43914473056793213, 0.47030335664749146, 0.5014619827270508, 0.5326206088066101, 0.5637792348861694, 0.5949378609657288, 0.6260964870452881, 0.6572551131248474, 0.6884137392044067, 0.7195723652839661, 0.7507309913635254, 0.7818896174430847, 0.813048243522644, 0.8442068696022034, 0.8753654956817627, 0.906524121761322, 0.9376827478408813, 0.9688413739204407, 1.0]}, "_runtime": 394.0020639896393, "_timestamp": 1571435415.1281626, "_step": 22}
{"epoch": 11, "train-loss": 0.5127571477838184, "train-mrr": 0.9083535001110472, "train-time-sec": 14.13495659828186, "val-loss": 8.162717124394009, "val-mrr": 0.20333973576472356, "val-time-sec": 5.0211663246154785, "_runtime": 412.8488755226135, "_timestamp": 1571435433.9749742, "_step": 23}
{"train-loss": 0.38177451491355896, "train-mrr": {"_type": "histogram", "values": [1, 2, 1, 0, 1, 3, 0, 5, 0, 0, 15, 0, 0, 0, 0, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 578], "bins": [0.0181818176060915, 0.04886363446712494, 0.07954545319080353, 0.11022727191448212, 0.1409090906381607, 0.1715909093618393, 0.20227272808551788, 0.23295454680919647, 0.26363635063171387, 0.29431816935539246, 0.32499998807907104, 0.35568180680274963, 0.3863636255264282, 0.4170454442501068, 0.4477272629737854, 0.478409081697464, 0.5090909004211426, 0.5397727489471436, 0.5704545378684998, 0.6011363863945007, 0.6318181753158569, 0.6625000238418579, 0.6931818127632141, 0.7238636612892151, 0.7545454502105713, 0.7852272987365723, 0.8159090876579285, 0.8465909361839294, 0.8772727251052856, 0.9079545736312866, 0.9386363625526428, 0.9693182110786438, 1.0]}, "_runtime": 416.12609815597534, "_timestamp": 1571435437.2521968, "_step": 24}
{"epoch": 12, "train-loss": 0.4676380734080854, "train-mrr": 0.9159105077635086, "train-time-sec": 14.119267225265503, "val-loss": 7.876578712463379, "val-mrr": 0.2076455336350661, "val-time-sec": 5.018172740936279, "_runtime": 434.9565770626068, "_timestamp": 1571435456.082676, "_step": 25}
{"train-loss": 0.3483404517173767, "train-mrr": {"_type": "histogram", "values": [2, 0, 0, 4, 0, 3, 4, 0, 0, 9, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589], "bins": [0.05000000074505806, 0.07968749850988388, 0.109375, 0.13906249403953552, 0.16875000298023224, 0.19843749701976776, 0.22812500596046448, 0.2578125, 0.2874999940395355, 0.31718748807907104, 0.34687501192092896, 0.3765625059604645, 0.40625, 0.4359374940395355, 0.46562498807907104, 0.49531251192092896, 0.5249999761581421, 0.5546875, 0.5843750238418579, 0.614062488079071, 0.643750011920929, 0.6734374761581421, 0.703125, 0.7328125238418579, 0.762499988079071, 0.792187511920929, 0.8218749761581421, 0.8515625, 0.8812500238418579, 0.910937488079071, 0.940625011920929, 0.9703124761581421, 1.0]}, "_runtime": 438.24905371665955, "_timestamp": 1571435459.3751526, "_step": 26}
{"epoch": 13, "train-loss": 0.42301100492477417, "train-mrr": 0.9246937219115803, "train-time-sec": 14.149748086929321, "val-loss": 8.180615874699184, "val-mrr": 0.2086468985421317, "val-time-sec": 5.001425504684448, "_runtime": 457.0847885608673, "_timestamp": 1571435478.2108874, "_step": 27}
{"train-loss": 0.3546498119831085, "train-mrr": {"_type": "histogram", "values": [1, 1, 0, 3, 3, 4, 0, 4, 0, 0, 11, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 584], "bins": [0.02857142873108387, 0.05892857164144516, 0.0892857164144516, 0.11964285373687744, 0.15000000596046448, 0.18035714328289032, 0.21071428060531616, 0.2410714328289032, 0.27142858505249023, 0.3017857074737549, 0.3321428596973419, 0.36250001192092896, 0.3928571343421936, 0.42321428656578064, 0.4535714387893677, 0.4839285612106323, 0.5142857432365417, 0.5446428656578064, 0.574999988079071, 0.6053571701049805, 0.6357142925262451, 0.6660714149475098, 0.6964285969734192, 0.7267857193946838, 0.7571428418159485, 0.7875000238418579, 0.8178571462631226, 0.8482142686843872, 0.8785714507102966, 0.9089285731315613, 0.9392856955528259, 0.9696428775787354, 1.0]}, "_runtime": 460.36588501930237, "_timestamp": 1571435481.4919837, "_step": 28}
{"epoch": 14, "train-loss": 0.3876190308643424, "train-mrr": 0.9304069289395642, "train-time-sec": 14.095552921295166, "val-loss": 8.466924217769078, "val-mrr": 0.2139518526517428, "val-time-sec": 4.99457049369812, "_runtime": 479.1490275859833, "_timestamp": 1571435500.2751265, "_step": 29}
{"Examples-Test": {"_type": "table", "columns": ["Rank", "Language", "Query", "Code"], "data": [[68, "python", "unpack_4to8", "```python\ndef unpack_4to8(data):\n    \"\"\" Promote 2-bit unisgned data into 8-bit unsigned data.\n\n    Args:\n        data: Numpy array with dtype == uint8\n\n    Notes:\n        # The process is this:\n        # ABCDEFGH [Bits of one 4+4-bit value]\n        # 00000000ABCDEFGH [astype(uint16)]\n        # 0000ABCDEFGH0000 [<< 4]\n        # 0000ABCDXXXXEFGH [bitwise 'or' of previous two lines]\n        # 0000111100001111 [0x0F0F]\n        # 0000ABCD0000EFGH [bitwise 'and' of previous two lines]\n        # ABCD0000EFGH0000 [<< 4]\n        # which effectively pads the two 4-bit values with zeros on the right\n        # Note: This technique assumes LSB-first ordering\n    \"\"\"\n\n    tmpdata = data.astype(np.int16)  # np.empty(upshape, dtype=np.int16)\n    tmpdata = (tmpdata | (tmpdata << 4)) & 0x0F0F\n    # tmpdata = tmpdata << 4 # Shift into high bits to avoid needing to sign extend\n    updata = tmpdata.byteswap()\n    return updata.view(data.dtype)\n```"], [73, "python", "parse_mim2gene", "```python\ndef parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry\n```"], [211, "python", "CreditNoteController.cancellation_fee", "```python\ndef cancellation_fee(self, percentage):\n        ''' Generates an invoice with a cancellation fee, and applies\n        credit to the invoice.\n\n        percentage (Decimal): The percentage of the credit note to turn into\n        a cancellation fee. Must be 0 <= percentage <= 100.\n        '''\n\n        # Local import to fix import cycles. Can we do better?\n        from .invoice import InvoiceController\n\n        assert(percentage >= 0 and percentage <= 100)\n\n        cancellation_fee = self.credit_note.value * percentage / 100\n        due = datetime.timedelta(days=1)\n        item = [(\"Cancellation fee\", cancellation_fee)]\n        invoice = InvoiceController.manual_invoice(\n            self.credit_note.invoice.user, due, item\n        )\n\n        if not invoice.is_paid:\n            self.apply_to_invoice(invoice)\n\n        return InvoiceController(invoice)\n```"], [28, "python", "update_panel", "```python\ndef update_panel(store, panel_name, csv_lines, option):\n    \"\"\"Update an existing gene panel with genes.\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        panel_name(str)\n        csv_lines(iterable(str)): Stream with genes\n        option(str): 'add' or 'replace'\n\n    Returns:\n        panel_obj(dict)\n    \"\"\"\n    new_genes= []\n    panel_obj = store.gene_panel(panel_name)\n    if panel_obj is None:\n        return None\n    try:\n        new_genes = parse_genes(csv_lines) # a list of gene dictionaries containing gene info\n    except SyntaxError as error:\n        flash(error.args[0], 'danger')\n        return None\n\n    # if existing genes are to be replaced by those in csv_lines\n    if option == 'replace':\n        # all existing genes should be deleted\n        for gene in panel_obj['genes']:\n            #create extra key to use in pending actions:\n            gene['hgnc_symbol'] = gene['symbol']\n            store.add_pending(panel_obj, gene, action='delete', info=None)\n\n    for new_gene in new_genes:\n        if not new_gene['hgnc_id']:\n            flash(\"gene missing hgnc id: {}\".format(new_gene['hgnc_symbol']),'danger')\n            continue\n        gene_obj = store.hgnc_gene(new_gene['hgnc_id'])\n        if gene_obj is None:\n            flash(\"gene not found: {} - {}\".format(new_gene['hgnc_id'], new_gene['hgnc_symbol']),'danger')\n            continue\n        if new_gene['hgnc_symbol'] and gene_obj['hgnc_symbol'] != new_gene['hgnc_symbol']:\n            flash(\"symbol mis-match: {0} | {1}\".format(\n                gene_obj['hgnc_symbol'], new_gene['hgnc_symbol']), 'warning')\n\n        info_data = {\n            'disease_associated_transcripts': new_gene['transcripts'],\n            'reduced_penetrance': new_gene['reduced_penetrance'],\n            'mosaicism': new_gene['mosaicism'],\n            'inheritance_models': new_gene['inheritance_models'],\n            'database_entry_version': new_gene['database_entry_version'],\n        }\n        if option == 'replace': # there will be no existing genes for sure, because we're replacing them all\n            action = 'add'\n        else: # add option. Add if genes is not existing. otherwise edit it\n            existing_genes = {gene['hgnc_id'] for gene in panel_obj['genes']}\n            action = 'edit' if gene_obj['hgnc_id'] in existing_genes else 'add'\n        store.add_pending(panel_obj, gene_obj, action=action, info=info_data)\n\n    return panel_obj\n```"], [26, "python", "MinHashLSHForest.index", "```python\ndef index(self):\n        '''\n        Index all the keys added so far and make them searchable.\n        '''\n        for i, hashtable in enumerate(self.hashtables):\n            self.sorted_hashtables[i] = [H for H in hashtable.keys()]\n            self.sorted_hashtables[i].sort()\n```"], [3, "python", "Eaf.generate_ts_id", "```python\ndef generate_ts_id(self, time=None):\n        \"\"\"Generate the next timeslot id, this function is mainly used\n        internally\n\n        :param int time: Initial time to assign to the timeslot.\n        :raises ValueError: If the time is negative.\n        \"\"\"\n        if time and time < 0:\n            raise ValueError('Time is negative...')\n        if not self.maxts:\n            valid_ts = [int(''.join(filter(str.isdigit, a)))\n                        for a in self.timeslots]\n            self.maxts = max(valid_ts + [1])+1\n        else:\n            self.maxts += 1\n        ts = 'ts{:d}'.format(self.maxts)\n        self.timeslots[ts] = time\n        return ts\n```"], [10, "python", "BandwidthLimitedStream.read", "```python\ndef read(self, amount):\n        \"\"\"Read a specified amount\n\n        Reads will only be throttled if bandwidth limiting is enabled.\n        \"\"\"\n        if not self._bandwidth_limiting_enabled:\n            return self._fileobj.read(amount)\n\n        # We do not want to be calling consume on every read as the read\n        # amounts can be small causing the lock of the leaky bucket to\n        # introduce noticeable overhead. So instead we keep track of\n        # how many bytes we have seen and only call consume once we pass a\n        # certain threshold.\n        self._bytes_seen += amount\n        if self._bytes_seen < self._bytes_threshold:\n            return self._fileobj.read(amount)\n\n        self._consume_through_leaky_bucket()\n        return self._fileobj.read(amount)\n```"], [33, "python", "Meter.extractMonthTariff", "```python\ndef extractMonthTariff(self, month):\n        \"\"\" Extract the tariff for a single month from the meter object buffer.\n\n        Args:\n            month (int):  A :class:`~ekmmeters.Months` value or range(Extents.Months).\n\n        Returns:\n            tuple: The eight tariff period totals for month. The return tuple breaks out as follows:\n\n            ================= ======================================\n            kWh_Tariff_1      kWh for tariff period 1 over month.\n            kWh_Tariff_2      kWh for tariff period 2 over month\n            kWh_Tariff_3      kWh for tariff period 3 over month\n            kWh_Tariff_4      kWh for tariff period 4 over month\n            kWh_Tot           Total kWh over requested month\n            Rev_kWh_Tariff_1  Rev kWh for tariff period 1 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 2 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 3 over month\n            Rev_kWh_Tariff_4  Rev kWh for tariff period 4 over month\n            Rev_kWh_Tot       Total Rev kWh over requested month\n            ================= ======================================\n\n        \"\"\"\n        ret = namedtuple(\"ret\", [\"Month\", Field.kWh_Tariff_1, Field.kWh_Tariff_2, Field.kWh_Tariff_3,\n                         Field.kWh_Tariff_4, Field.kWh_Tot, Field.Rev_kWh_Tariff_1,\n                         Field.Rev_kWh_Tariff_2, Field.Rev_kWh_Tariff_3,\n                         Field.Rev_kWh_Tariff_4, Field.Rev_kWh_Tot])\n        month += 1\n        ret.Month = str(month)\n        if (month < 1) or (month > Extents.Months):\n            ret.kWh_Tariff_1 = ret.kWh_Tariff_2 = ret.kWh_Tariff_3 = ret.kWh_Tariff_4 = str(0)\n            ret.Rev_kWh_Tariff_1 = ret.Rev_kWh_Tariff_2 = ret.Rev_kWh_Tariff_3 = ret.Rev_kWh_Tariff_4 = str(0)\n            ret.kWh_Tot = ret.Rev_kWh_Tot = str(0)\n            ekm_log(\"Out of range(Extents.Months) month = \" + str(month))\n            return ret\n\n        base_str = \"Month_\" + str(month) + \"_\"\n        ret.kWh_Tariff_1 = self.m_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.kWh_Tariff_2 = self.m_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.kWh_Tariff_3 = self.m_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.kWh_Tariff_4 = self.m_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.kWh_Tot = self.m_mons[base_str + \"Tot\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_1 = self.m_rev_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_2 = self.m_rev_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_3 = self.m_rev_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_4 = self.m_rev_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.Rev_kWh_Tot = self.m_rev_mons[base_str + \"Tot\"][MeterData.StringValue]\n        return ret\n```"], [9, "python", "BigQueryCursor.executemany", "```python\ndef executemany(self, operation, seq_of_parameters):\n        \"\"\"\n        Execute a BigQuery query multiple times with different parameters.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param seq_of_parameters: List of dictionary parameters to substitute into the\n            query.\n        :type seq_of_parameters: list\n        \"\"\"\n        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)\n```"], [10, "python", "TypeSystem.convert_from_binary", "```python\ndef convert_from_binary(self, binvalue, type, **kwargs):\n        \"\"\"\n        Convert binary data to type 'type'.\n\n        'type' must have a convert_binary function.  If 'type'\n        supports size checking, the size function is called to ensure\n        that binvalue is the correct size for deserialization\n        \"\"\"\n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)\n```"], [181, "python", "read_data", "```python\ndef read_data(data_file, dataformat, name_mode):\n    \"\"\"\n    Load data_file described by a dataformat dict.\n\n    Parameters\n    ----------\n    data_file : str\n        Path to data file, including extension.\n    dataformat : dict\n        A dataformat dict, see example below.\n    name_mode : str\n        How to identyfy sample names. If 'file_names' uses the\n        input name of the file, stripped of the extension. If\n        'metadata_names' uses the 'name' attribute of the 'meta'\n        sub-dictionary in dataformat. If any other str, uses this\n        str as the sample name.\n\n    Example\n    -------\n    >>>\n    {'genfromtext_args': {'delimiter': ',',\n                          'skip_header': 4},  # passed directly to np.genfromtxt\n     'column_id': {'name_row': 3,  # which row contains the column names\n                   'delimiter': ',',  # delimeter between column names\n                   'timecolumn': 0,  # which column contains the 'time' variable\n                   'pattern': '([A-z]{1,2}[0-9]{1,3})'},  # a regex pattern which captures the column names\n     'meta_regex': {  # a dict of (line_no: ([descriptors], [regexs])) pairs\n                    0: (['path'], '(.*)'),\n                    2: (['date', 'method'],  # MUST include date\n                     '([A-Z][a-z]+ [0-9]+ [0-9]{4}[ ]+[0-9:]+ [amp]+).* ([A-z0-9]+\\.m)')\n                   }\n    }\n\n    Returns\n    -------\n    sample, analytes, data, meta : tuple\n    \"\"\"\n    with open(data_file) as f:\n        lines = f.readlines()\n\n    if 'meta_regex' in dataformat.keys():\n        meta = Bunch()\n        for k, v in dataformat['meta_regex'].items():\n            try:\n                out = re.search(v[-1], lines[int(k)]).groups()\n            except:\n                raise ValueError('Failed reading metadata when applying:\\n  regex: {}\\nto\\n  line: {}'.format(v[-1], lines[int(k)]))\n            for i in np.arange(len(v[0])):\n                meta[v[0][i]] = out[i]\n    else:\n        meta = {}\n\n    # sample name\n    if name_mode == 'file_names':\n        sample = os.path.basename(data_file).split('.')[0]\n    elif name_mode == 'metadata_names':\n        sample = meta['name']\n    else:\n        sample = name_mode\n\n    # column and analyte names\n    columns = np.array(lines[dataformat['column_id']['name_row']].strip().split(\n        dataformat['column_id']['delimiter']))\n    if 'pattern' in dataformat['column_id'].keys():\n        pr = re.compile(dataformat['column_id']['pattern'])\n        analytes = [pr.match(c).groups()[0] for c in columns if pr.match(c)]\n\n    # do any required pre-formatting\n    if 'preformat_replace' in dataformat.keys():\n        with open(data_file) as f:\n            fbuffer = f.read()\n        for k, v in dataformat['preformat_replace'].items():\n            fbuffer = re.sub(k, v, fbuffer)\n        # dead data\n        read_data = np.genfromtxt(BytesIO(fbuffer.encode()),\n                                  **dataformat['genfromtext_args']).T\n    else:\n        # read data\n        read_data = np.genfromtxt(data_file,\n                                  **dataformat['genfromtext_args']).T\n\n    # data dict\n    dind = np.zeros(read_data.shape[0], dtype=bool)\n    for a in analytes:\n        dind[columns == a] = True\n\n    data = Bunch()\n    data['Time'] = read_data[dataformat['column_id']['timecolumn']]\n\n    # deal with time units\n    if 'time_unit' in dataformat['column_id']:\n        if isinstance(dataformat['column_id']['time_unit'], (float, int)):\n            time_mult = dataformat['column_id']['time_unit']\n        elif isinstance(dataformat['column_id']['time_unit'], str):\n            unit_multipliers = {'ms': 1/1000,\n                                'min': 60/1,\n                                's': 1}\n            try:\n                time_mult = unit_multipliers[dataformat['column_id']['time_unit']]\n            except:\n                raise ValueError(\"In dataformat: time_unit must be a number, 'ms', 'min' or 's'\")\n        data['Time'] *= time_mult\n        \n    # convert raw data into counts\n    # TODO: Is this correct? Should actually be per-analyte dwell?\n    # if 'unit' in dataformat:\n    #     if dataformat['unit'] == 'cps':\n    #         tstep = data['Time'][1] - data['Time'][0]\n    #         read_data[dind] *= tstep\n    #     else:\n    #         pass\n    data['rawdata'] = Bunch(zip(analytes, read_data[dind]))\n    data['total_counts'] = np.nansum(read_data[dind], 0)\n\n    return sample, analytes, data, meta\n```"], [11, "python", "DrawElement.top", "```python\ndef top(self):\n        \"\"\" Constructs the top line of the element\"\"\"\n        ret = self.top_format % self.top_connect.center(\n            self.width, self.top_pad)\n        if self.right_fill:\n            ret = ret.ljust(self.right_fill, self.top_pad)\n        if self.left_fill:\n            ret = ret.rjust(self.left_fill, self.top_pad)\n        ret = ret.center(self.layer_width, self.top_bck)\n        return ret\n```"], [733, "python", "GoogleCloudBucketHelper.google_cloud_to_local", "```python\ndef google_cloud_to_local(self, file_name):\n        \"\"\"\n        Checks whether the file specified by file_name is stored in Google Cloud\n        Storage (GCS), if so, downloads the file and saves it locally. The full\n        path of the saved file will be returned. Otherwise the local file_name\n        will be returned immediately.\n\n        :param file_name: The full path of input file.\n        :type file_name: str\n        :return: The full path of local file.\n        :rtype: str\n        \"\"\"\n        if not file_name.startswith('gs://'):\n            return file_name\n\n        # Extracts bucket_id and object_id by first removing 'gs://' prefix and\n        # then split the remaining by path delimiter '/'.\n        path_components = file_name[self.GCS_PREFIX_LENGTH:].split('/')\n        if len(path_components) < 2:\n            raise Exception(\n                'Invalid Google Cloud Storage (GCS) object path: {}'\n                .format(file_name))\n\n        bucket_id = path_components[0]\n        object_id = '/'.join(path_components[1:])\n        local_file = '/tmp/dataflow{}-{}'.format(str(uuid.uuid4())[:8],\n                                                 path_components[-1])\n        self._gcs_hook.download(bucket_id, object_id, local_file)\n\n        if os.stat(local_file).st_size > 0:\n            return local_file\n        raise Exception(\n            'Failed to download Google Cloud Storage (GCS) object: {}'\n            .format(file_name))\n```"], [64, "python", "get_single_list_nodes_data", "```python\ndef get_single_list_nodes_data(li, meta_data):\n    \"\"\"\n    Find consecutive li tags that have content that have the same list id.\n    \"\"\"\n    yield li\n    w_namespace = get_namespace(li, 'w')\n    current_numId = get_numId(li, w_namespace)\n    starting_ilvl = get_ilvl(li, w_namespace)\n    el = li\n    while True:\n        el = el.getnext()\n        if el is None:\n            break\n        # If the tag has no content ignore it.\n        if not has_text(el):\n            continue\n\n        # Stop the lists if you come across a list item that should be a\n        # heading.\n        if _is_top_level_upper_roman(el, meta_data):\n            break\n\n        if (\n                is_li(el, meta_data) and\n                (starting_ilvl > get_ilvl(el, w_namespace))):\n            break\n\n        new_numId = get_numId(el, w_namespace)\n        if new_numId is None or new_numId == -1:\n            # Not a p tag or a list item\n            yield el\n            continue\n        # If the list id of the next tag is different that the previous that\n        # means a new list being made (not nested)\n        if current_numId != new_numId:\n            # Not a subsequent list.\n            break\n        if is_last_li(el, meta_data, current_numId):\n            yield el\n            break\n        yield el\n```"], [161, "python", "Node.select", "```python\ndef select(self, selector):\n        \"\"\"\n        Like :meth:`find_all`, but takes a CSS selector string as input.\n        \"\"\"\n        op = operator.methodcaller('select', selector)\n        return self._wrap_multi(op)\n```"], [2, "python", "SqlDatabaseManagementService.list_quotas", "```python\ndef list_quotas(self, server_name):\n        '''\n        Gets quotas for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_quotas_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServerQuota)\n```"], [88, "python", "SolveBioAuth.logout", "```python\ndef logout(self):\n        \"\"\"Revoke the token and remove the cookie.\"\"\"\n        if self._oauth_client_secret:\n            try:\n                oauth_token = flask.request.cookies[self.TOKEN_COOKIE_NAME]\n                # Revoke the token\n                requests.post(\n                    urljoin(self._api_host, self.OAUTH2_REVOKE_TOKEN_PATH),\n                    data={\n                        'client_id': self._oauth_client_id,\n                        'client_secret': self._oauth_client_secret,\n                        'token': oauth_token\n                    })\n            except:\n                pass\n\n        response = flask.redirect('/')\n        self.clear_cookies(response)\n        return response\n```"], [517, "python", "Context.integrity_negotiated", "```python\ndef integrity_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if integrity protection (signing) has been negotiated in this context, False\n        otherwise. If this property is True, you can use :meth:`get_mic` to sign messages with a\n        message integrity code (MIC), which the peer application can verify.\n        \"\"\"\n        return (\n            self.flags & C.GSS_C_INTEG_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )\n```"], [6, "python", "QasmSimulatorPy._get_statevector", "```python\ndef _get_statevector(self):\n        \"\"\"Return the current statevector in JSON Result spec format\"\"\"\n        vec = np.reshape(self._statevector, 2 ** self._number_of_qubits)\n        # Expand complex numbers\n        vec = np.stack([vec.real, vec.imag], axis=1)\n        # Truncate small values\n        vec[abs(vec) < self._chop_threshold] = 0.0\n        return vec\n```"], [3, "python", "OpenIdMixin.authenticate_redirect", "```python\ndef authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))\n```"], [2, "python", "NotebookManager.save_new_notebook", "```python\ndef save_new_notebook(self, data, name=None, format=u'json'):\n        \"\"\"Save a new notebook and return its notebook_id.\n\n        If a name is passed in, it overrides any values in the notebook data\n        and the value in the data is updated to use that value.\n        \"\"\"\n        if format not in self.allowed_formats:\n            raise web.HTTPError(415, u'Invalid notebook format: %s' % format)\n\n        try:\n            nb = current.reads(data.decode('utf-8'), format)\n        except:\n            raise web.HTTPError(400, u'Invalid JSON data')\n\n        if name is None:\n            try:\n                name = nb.metadata.name\n            except AttributeError:\n                raise web.HTTPError(400, u'Missing notebook name')\n        nb.metadata.name = name\n\n        notebook_id = self.new_notebook_id(name)\n        self.save_notebook_object(notebook_id, nb)\n        return notebook_id\n```"], [323, "python", "PassManager.passes", "```python\ndef passes(self):\n        \"\"\"\n        Returns a list structure of the appended passes and its options.\n\n        Returns (list): The appended passes.\n        \"\"\"\n        ret = []\n        for pass_ in self.working_list:\n            ret.append(pass_.dump_passes())\n        return ret\n```"], [3, "python", "DatabaseConnection.read", "```python\ndef read(self, path, params=None):\n        \"\"\"Read the result at the given path (GET) from the CRUD API, using the optional params dictionary\n        as url parameters.\"\"\"\n        return self.handleresult(self.r.get(urljoin(self.url + CRUD_PATH,\n                                                    path),\n                                            params=params))\n```"], [11, "python", "Session.evaluate_script", "```python\ndef evaluate_script(self, script, *args):\n        \"\"\"\n        Evaluate the given JavaScript and return the result. Be careful when using this with\n        scripts that return complex objects, such as jQuery statements. :meth:`execute_script`\n        might be a better alternative.\n\n        Args:\n            script (str): A string of JavaScript to evaluate.\n            *args: Variable length argument list to pass to the executed JavaScript string.\n\n        Returns:\n            object: The result of the evaluated JavaScript (may be driver specific).\n        \"\"\"\n\n        args = [arg.base if isinstance(arg, Base) else arg for arg in args]\n        result = self.driver.evaluate_script(script, *args)\n        return self._wrap_element_script_result(result)\n```"], [279, "python", "get_all_boundary_algorithms", "```python\ndef get_all_boundary_algorithms():\n    \"\"\"Gets all the possible boundary algorithms in MSAF.\n\n    Returns\n    -------\n    algo_ids : list\n        List of all the IDs of boundary algorithms (strings).\n    \"\"\"\n    algo_ids = []\n    for name in msaf.algorithms.__all__:\n        module = eval(msaf.algorithms.__name__ + \".\" + name)\n        if module.is_boundary_type:\n            algo_ids.append(module.algo_id)\n    return algo_ids\n```"], [3, "python", "Expression.op_and", "```python\ndef op_and(self, *elements):\n        \"\"\"Update the ``Expression`` by joining the specified additional\n        ``elements`` using an \"AND\" ``Operator``\n\n        Args:\n            *elements (BaseExpression): The ``Expression`` and/or\n                ``Constraint`` elements which the \"AND\" ``Operator`` applies\n                to.\n\n        Returns:\n            Expression: ``self`` or related ``Expression``.\n        \"\"\"\n        expression = self.add_operator(Operator(';'))\n        for element in elements:\n            expression.add_element(element)\n        return expression\n```"], [47, "python", "LinkyClient._get_data", "```python\ndef _get_data(self, p_p_resource_id, start_date=None, end_date=None):\n        \"\"\"Get data.\"\"\"\n\n        data = {\n            '_' + REQ_PART + '_dateDebut': start_date,\n            '_' + REQ_PART + '_dateFin': end_date\n        }\n\n        params = {\n            'p_p_id': REQ_PART,\n            'p_p_lifecycle': 2,\n            'p_p_state': 'normal',\n            'p_p_mode': 'view',\n            'p_p_resource_id': p_p_resource_id,\n            'p_p_cacheability': 'cacheLevelPage',\n            'p_p_col_id': 'column-1',\n            'p_p_col_pos': 1,\n            'p_p_col_count': 3\n        }\n\n        try:\n            raw_res = self._session.post(DATA_URL,\n                                         data=data,\n                                         params=params,\n                                         allow_redirects=False,\n                                         timeout=self._timeout)\n\n            if 300 <= raw_res.status_code < 400:\n                raw_res = self._session.post(DATA_URL,\n                                             data=data,\n                                             params=params,\n                                             allow_redirects=False,\n                                             timeout=self._timeout)\n        except OSError as e:\n            raise PyLinkyError(\"Could not access enedis.fr: \" + str(e))\n\n        if raw_res.text is \"\":\n            raise PyLinkyError(\"No data\")\n\n        if 302 == raw_res.status_code and \"/messages/maintenance.html\" in raw_res.text:\n            raise PyLinkyError(\"Site in maintenance\")\n\n        try:\n            json_output = raw_res.json()\n        except (OSError, json.decoder.JSONDecodeError, simplejson.errors.JSONDecodeError) as e:\n            raise PyLinkyError(\"Impossible to decode response: \" + str(e) + \"\\nResponse was: \" + str(raw_res.text))\n\n        if json_output.get('etat').get('valeur') == 'erreur':\n            raise PyLinkyError(\"Enedis.fr answered with an error: \" + str(json_output))\n\n        return json_output.get('graphe')\n```"], [182, "python", "clinvar", "```python\ndef clinvar(institute_id, case_name, variant_id):\n    \"\"\"Build a clinVar submission form for a variant.\"\"\"\n    data = controllers.clinvar_export(store, institute_id, case_name, variant_id)\n    if request.method == 'GET':\n        return data\n    else: #POST\n        form_dict = request.form.to_dict()\n        submission_objects = set_submission_objects(form_dict) # A tuple of submission objects (variants and casedata objects)\n\n        # Add submission data to an open clinvar submission object,\n        # or create a new if no open submission is found in database\n        open_submission = store.get_open_clinvar_submission(current_user.email, institute_id)\n        updated_submission = store.add_to_submission(open_submission['_id'], submission_objects)\n\n        # Redirect to clinvar submissions handling page, and pass it the updated_submission_object\n        return redirect(url_for('cases.clinvar_submissions', institute_id=institute_id))\n```"], [2, "python", "MozillaClubParser.__get_event_fields", "```python\ndef __get_event_fields(self):\n        \"\"\"Get the events fields (columns) from the cells received.\"\"\"\n\n        event_fields = {}\n        # The cells in the first row are the column names\n        # Check that the columns names are the same we have as template\n        # Create the event template from the data retrieved\n        while self.ncell < len(self.cells):\n            cell = self.cells[self.ncell]\n            row = cell['gs$cell']['row']\n            if int(row) > 1:\n                # When the row number >1 the column row is finished\n                break\n            ncol = int(cell['gs$cell']['col'])\n            name = cell['content']['$t']\n            event_fields[ncol] = name\n            if ncol in EVENT_TEMPLATE:\n                if event_fields[ncol] != EVENT_TEMPLATE[ncol]:\n                    logger.warning(\"Event template changed in spreadsheet %s vs %s\",\n                                   name, EVENT_TEMPLATE[ncol])\n            else:\n                logger.warning(\"Event template changed in spreadsheet. New column: %s\", name)\n\n            self.ncell += 1\n        return event_fields\n```"], [11, "python", "center_eigenvalue_diff", "```python\ndef center_eigenvalue_diff(mat):\n    \"\"\"Compute the eigvals of mat and then find the center eigval difference.\"\"\"\n    N = len(mat)\n    evals = np.sort(la.eigvals(mat))\n    diff = np.abs(evals[N/2] - evals[N/2-1])\n    return diff\n```"], [9, "python", "MongoHook.delete_one", "```python\ndef delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)\n```"], [3, "python", "Mesh.set_fields", "```python\ndef set_fields(self, fields = None, **kwargs):\n    \"\"\"\n    Sets the fields.\n    \"\"\"\n    self.fields = []\n    if fields != None:\n      for field in fields: \n        self.fields.append(field)\n```"], [2, "python", "ResourceManager.addFromTex", "```python\ndef addFromTex(self,name,img,category):\n        \"\"\"\n        Adds a new texture from the given image.\n        \n        ``img`` may be any object that supports Pyglet-style copying in form of the ``blit_to_texture()`` method.\n        \n        This can be used to add textures that come from non-file sources, e.g. Render-to-texture.\n        \"\"\"\n        texreg = self.categoriesTexBin[category].add(img)\n        #texreg = texreg.get_transform(True,True) # Mirrors the image due to how pyglets coordinate system works\n        # Strange behaviour, sometimes needed and sometimes not\n        \n        self.categories[category][name]=texreg\n        target = texreg.target\n        texid = texreg.id\n        texcoords = texreg.tex_coords\n        \n        # Prevents texture bleeding with texture sizes that are powers of 2, else weird lines may appear at certain angles.\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST_MIPMAP_LINEAR)\n        glGenerateMipmap(GL_TEXTURE_2D)\n        \n        out = target,texid,texcoords\n        self.categoriesTexCache[category][name]=out\n        return out\n```"], [1, "python", "InstallRequirement.populate_link", "```python\ndef populate_link(self, finder, upgrade):\n        \"\"\"Ensure that if a link can be found for this, that it is found.\n\n        Note that self.link may still be None - if Upgrade is False and the\n        requirement is already installed.\n        \"\"\"\n        if self.link is None:\n            self.link = finder.find_requirement(self, upgrade)\n```"], [19, "python", "MultivariateNormalTriL.params_size", "```python\ndef params_size(event_size, name=None):\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    with tf.compat.v1.name_scope(name, 'MultivariateNormalTriL_params_size',\n                                 [event_size]):\n      return event_size + event_size * (event_size + 1) // 2\n```"], [7, "python", "ConnectionHandler.use_music_service", "```python\ndef use_music_service(self, service_name, api_key):\n        \"\"\"\n        Sets the current music service to service_name.\n\n        :param str service_name: Name of the music service\n        :param str api_key: Optional API key if necessary\n        \"\"\"\n\n        try:\n            self.current_music = self.music_services[service_name]\n        except KeyError:\n            if service_name == 'youtube':\n                self.music_services['youtube'] = Youtube()\n                self.current_music = self.music_services['youtube']\n            elif service_name == 'soundcloud':\n                self.music_services['soundcloud'] = Soundcloud(api_key=api_key)\n                self.current_music = self.music_services['soundcloud']\n            else:\n                log.error('Music service name is not recognized.')\n```"], [86, "python", "Striplog.invert", "```python\ndef invert(self, copy=False):\n        \"\"\"\n        Inverts the striplog, changing its order and the order of its contents.\n\n        Operates in place by default.\n\n        Args:\n            copy (bool): Whether to operate in place or make a copy.\n\n        Returns:\n            None if operating in-place, or an inverted copy of the striplog\n                if not.\n        \"\"\"\n        if copy:\n            return Striplog([i.invert(copy=True) for i in self])\n        else:\n            for i in self:\n                i.invert()\n            self.__sort()\n            o = self.order\n            self.order = {'depth': 'elevation', 'elevation': 'depth'}[o]\n            return\n```"], [352, "python", "VCGPrinter.edge", "```python\ndef edge(self, from_node, to_node, edge_type=\"\", **args):\n        \"\"\"draw an edge from a node to another.\n        \"\"\"\n        self._stream.write(\n            '%s%sedge: {sourcename:\"%s\" targetname:\"%s\"'\n            % (self._indent, edge_type, from_node, to_node)\n        )\n        self._write_attributes(EDGE_ATTRS, **args)\n        self._stream.write(\"}\\n\")\n```"], [226, "python", "pdf_case_report", "```python\ndef pdf_case_report(institute_id, case_name):\n    \"\"\"Download a pdf report for a case\"\"\"\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.case_report_content(store, institute_obj, case_obj)\n\n    # add coverage report on the bottom of this report\n    if current_app.config.get('SQLALCHEMY_DATABASE_URI'):\n        data['coverage_report'] = controllers.coverage_report_contents(store, institute_obj, case_obj, request.url_root)\n\n    # workaround to be able to print the case pedigree to pdf\n    if case_obj.get('madeline_info') is not None:\n        with open(os.path.join(cases_bp.static_folder, 'madeline.svg'), 'w') as temp_madeline:\n            temp_madeline.write(case_obj['madeline_info'])\n\n    html_report = render_template('cases/case_report.html', institute=institute_obj, case=case_obj, format='pdf', **data)\n    return render_pdf(HTML(string=html_report), download_filename=case_obj['display_name']+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')\n```"], [11, "python", "TaskInstance.are_dependencies_met", "```python\ndef are_dependencies_met(\n            self,\n            dep_context=None,\n            session=None,\n            verbose=False):\n        \"\"\"\n        Returns whether or not all the conditions are met for this task instance to be run\n        given the context for the dependencies (e.g. a task instance being force run from\n        the UI will ignore some dependencies).\n\n        :param dep_context: The execution context that determines the dependencies that\n            should be evaluated.\n        :type dep_context: DepContext\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param verbose: whether log details on failed dependencies on\n            info or debug log level\n        :type verbose: bool\n        \"\"\"\n        dep_context = dep_context or DepContext()\n        failed = False\n        verbose_aware_logger = self.log.info if verbose else self.log.debug\n        for dep_status in self.get_failed_dep_statuses(\n                dep_context=dep_context,\n                session=session):\n            failed = True\n\n            verbose_aware_logger(\n                \"Dependencies not met for %s, dependency '%s' FAILED: %s\",\n                self, dep_status.dep_name, dep_status.reason\n            )\n\n        if failed:\n            return False\n\n        verbose_aware_logger(\"Dependencies all met for %s\", self)\n        return True\n```"], [11, "python", "map_method", "```python\ndef map_method(method,object_list,*argseq,**kw):\n    \"\"\"map_method(method,object_list,*args,**kw) -> list\n\n    Return a list of the results of applying the methods to the items of the\n    argument sequence(s).  If more than one sequence is given, the method is\n    called with an argument list consisting of the corresponding item of each\n    sequence. All sequences must be of the same length.\n\n    Keyword arguments are passed verbatim to all objects called.\n\n    This is Python code, so it's not nearly as fast as the builtin map().\"\"\"\n\n    out_list = []\n    idx = 0\n    for object in object_list:\n        try:\n            handler = getattr(object, method)\n        except AttributeError:\n            out_list.append(None)\n        else:\n            if argseq:\n                args = map(lambda lst:lst[idx],argseq)\n                #print 'ob',object,'hand',handler,'ar',args # dbg\n                out_list.append(handler(args,**kw))\n            else:\n                out_list.append(handler(**kw))\n        idx += 1\n    return out_list\n```"], [40, "python", "Gerrit.parse_reviews", "```python\ndef parse_reviews(raw_data):\n        \"\"\"Parse a Gerrit reviews list.\"\"\"\n\n        # Join isolated reviews in JSON in array for parsing\n        items_raw = \"[\" + raw_data.replace(\"\\n\", \",\") + \"]\"\n        items_raw = items_raw.replace(\",]\", \"]\")\n        items = json.loads(items_raw)\n        reviews = []\n\n        for item in items:\n            if 'project' in item.keys():\n                reviews.append(item)\n\n        return reviews\n```"], [10, "python", "start", "```python\ndef start(dashboards, once, secrets):\n    \"\"\"Display a dashboard from the dashboard file(s) provided in the DASHBOARDS\n       Paths and/or URLs for dashboards (URLs must secrets with http or https)\n    \"\"\"\n\n    if secrets is None:\n        secrets = os.path.join(os.path.expanduser(\"~\"), \"/.doodledashboard/secrets\")\n\n    try:\n        loaded_secrets = try_read_secrets_file(secrets)\n    except InvalidSecretsException as err:\n        click.echo(get_error_message(err, default=\"Secrets file is invalid\"), err=True)\n        raise click.Abort()\n\n    read_configs = [\"\"\"\n    dashboard:\n      display:\n        type: console\n    \"\"\"]\n    for dashboard_file in dashboards:\n        read_configs.append(read_file(dashboard_file))\n\n    dashboard_config = DashboardConfigReader(initialise_component_loader(), loaded_secrets)\n\n    try:\n        dashboard = read_dashboard_from_config(dashboard_config, read_configs)\n    except YAMLError as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    try:\n        DashboardValidator().validate(dashboard)\n    except ValidationException as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    explain_dashboard(dashboard)\n\n    click.echo(\"Dashboard running...\")\n\n    while True:\n        try:\n            DashboardRunner(dashboard).cycle()\n        except SecretNotFound as err:\n            click.echo(get_error_message(err, default=\"Datafeed didn't have required secret\"), err=True)\n            raise click.Abort()\n\n        if once:\n            break\n```"], [20, "python", "OAuth2RequestValidator.authenticate_client", "```python\ndef authenticate_client(self, request, *args, **kwargs):\n        \"\"\"Authenticate itself in other means.\n\n        Other means means is described in `Section 3.2.1`_.\n\n        .. _`Section 3.2.1`: http://tools.ietf.org/html/rfc6749#section-3.2.1\n        \"\"\"\n        client_id, client_secret = self._get_client_creds_from_request(request)\n        log.debug('Authenticate client %r', client_id)\n\n        client = self._clientgetter(client_id)\n        if not client:\n            log.debug('Authenticate client failed, client not found.')\n            return False\n\n        request.client = client\n\n        # http://tools.ietf.org/html/rfc6749#section-2\n        # The client MAY omit the parameter if the client secret is an empty string.\n        if hasattr(client, 'client_secret') and client.client_secret != client_secret:\n            log.debug('Authenticate client failed, secret not match.')\n            return False\n\n        log.debug('Authenticate client success.')\n        return True\n```"], [1, "python", "Trajectory.f_get_parameters", "```python\ndef f_get_parameters(self, fast_access=False, copy=True):\n        \"\"\" Returns a dictionary containing the full parameter names as keys and the parameters\n         or the parameter data items as values.\n\n\n        :param fast_access:\n\n            Determines whether the parameter objects or their values are returned\n            in the dictionary.\n\n        :param copy:\n\n            Whether the original dictionary or a shallow copy is returned.\n            If you want the real dictionary please do not modify it at all!\n            Not Copying and fast access do not work at the same time! Raises ValueError\n            if fast access is true and copy false.\n\n        :return: Dictionary containing the parameters.\n\n        :raises: ValueError\n\n        \"\"\"\n        return self._return_item_dictionary(self._parameters, fast_access, copy)\n```"], [6, "python", "load_config", "```python\ndef load_config(under_test=False, custom=None):  # pragma: no cover\n    \"\"\"\n    Load the configuration.\n\n    :param under_test:\n        Tell us if we only have to load the configuration file (True)\n        or load the configuration file and initate the output directory\n        if it does not exist (False).\n    :type under_test: bool\n\n    :param custom:\n        A dict with the configuration index (from .PyFunceble.yaml) to update.\n    :type custom: dict\n\n    .. warning::\n        If :code:`custom` is given, the given :code:`dict` overwrite\n        the last value of the given configuration indexes.\n    \"\"\"\n\n    if \"config_loaded\" not in INTERN:\n        # The configuration was not already loaded.\n\n        # We load and download the different configuration file if they are non\n        # existant.\n        Load(CURRENT_DIRECTORY)\n\n        if not under_test:\n            # If we are not under test which means that we want to save informations,\n            # we initiate the directory structure.\n            DirectoryStructure()\n\n        # We save that the configuration was loaded.\n        INTERN.update({\"config_loaded\": True})\n\n        if custom and isinstance(custom, dict):\n            # The given configuration is not None or empty.\n            # and\n            # It is a dict.\n\n            # We update the configuration index.\n            CONFIGURATION.update(custom)\n```"], [81, "python", "FourDirectionalMoveController.registerEventHandlers", "```python\ndef registerEventHandlers(self):\n        \"\"\"\n        Registers needed keybinds and schedules the :py:meth:`update` Method.\n        \n        You can control what keybinds are used via the :confval:`controls.controls.forward` etc. Configuration Values.\n        \"\"\"\n        # Forward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.forward\"],\"peng3d:actor.%s.player.controls.forward\"%self.actor.uuid,self.on_fwd_down,False)\n        # Backward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.backward\"],\"peng3d:actor.%s.player.controls.backward\"%self.actor.uuid,self.on_bwd_down,False)\n        # Strafe Left\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.strafeleft\"],\"peng3d:actor.%s.player.controls.strafeleft\"%self.actor.uuid,self.on_left_down,False)\n        # Strafe Right\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.straferight\"],\"peng3d:actor.%s.player.controls.straferight\"%self.actor.uuid,self.on_right_down,False)\n        pyglet.clock.schedule_interval(self.update,1.0/60)\n```"], [1, "python", "FrameIdentifierVisitor.visit_Name", "```python\ndef visit_Name(self, node):\n        \"\"\"All assignments to names go through this function.\"\"\"\n        if node.ctx == 'store':\n            self.identifiers.declared_locally.add(node.name)\n        elif node.ctx == 'param':\n            self.identifiers.declared_parameter.add(node.name)\n        elif node.ctx == 'load' and not \\\n             self.identifiers.is_declared(node.name):\n            self.identifiers.undeclared.add(node.name)\n```"], [182, "python", "Client._call", "```python\ndef _call(self, method, params=None, request_id=None):\n        \"\"\" Calls the JSON-RPC endpoint. \"\"\"\n        params = params or []\n\n        # Determines which 'id' value to use and increment the counter associated with the current\n        # client instance if applicable.\n        rid = request_id or self._id_counter\n        if request_id is None:\n            self._id_counter += 1\n\n        # Prepares the payload and the headers that will be used to forge the request.\n        payload = {'jsonrpc': '2.0', 'method': method, 'params': params, 'id': rid}\n        headers = {'Content-Type': 'application/json'}\n        scheme = 'https' if self.tls else 'http'\n        url = '{}://{}:{}'.format(scheme, self.host, self.port)\n\n        # Calls the JSON-RPC endpoint!\n        try:\n            response = self.session.post(url, headers=headers, data=json.dumps(payload))\n            response.raise_for_status()\n        except HTTPError:\n            raise TransportError(\n                'Got unsuccessful response from server (status code: {})'.format(\n                    response.status_code),\n                response=response)\n\n        # Ensures the response body can be deserialized to JSON.\n        try:\n            response_data = response.json()\n        except ValueError as e:\n            raise ProtocolError(\n                'Unable to deserialize response body: {}'.format(e), response=response)\n\n        # Properly handles potential errors.\n        if response_data.get('error'):\n            code = response_data['error'].get('code', '')\n            message = response_data['error'].get('message', '')\n            raise ProtocolError(\n                'Error[{}] {}'.format(code, message), response=response, data=response_data)\n        elif 'result' not in response_data:\n            raise ProtocolError(\n                'Response is empty (result field is missing)', response=response,\n                data=response_data)\n\n        return response_data['result']\n```"], [38, "python", "Client._update_secrets", "```python\ndef _update_secrets(self):\n        '''update secrets will update metadata needed for pull and search\n        '''\n        self.token = self._required_get_and_update('SREGISTRY_GITLAB_TOKEN')\n        self.headers[\"Private-Token\"] = self.token\n```"]]}, "_runtime": 509.8564660549164, "_timestamp": 1571435530.982565, "_step": 30}
{"Examples-FuncNameTest": {"_type": "table", "columns": ["Rank", "Language", "Query", "Code"], "data": [[152, "python", "unpack_4to8", "```python\ndef unpack_4to8(data):\n    \"\"\" Promote 2-bit unisgned data into 8-bit unsigned data.\n\n    Args:\n        data: Numpy array with dtype == uint8\n\n    Notes:\n        # The process is this:\n        # ABCDEFGH [Bits of one 4+4-bit value]\n        # 00000000ABCDEFGH [astype(uint16)]\n        # 0000ABCDEFGH0000 [<< 4]\n        # 0000ABCDXXXXEFGH [bitwise 'or' of previous two lines]\n        # 0000111100001111 [0x0F0F]\n        # 0000ABCD0000EFGH [bitwise 'and' of previous two lines]\n        # ABCD0000EFGH0000 [<< 4]\n        # which effectively pads the two 4-bit values with zeros on the right\n        # Note: This technique assumes LSB-first ordering\n    \"\"\"\n\n    tmpdata = data.astype(np.int16)  # np.empty(upshape, dtype=np.int16)\n    tmpdata = (tmpdata | (tmpdata << 4)) & 0x0F0F\n    # tmpdata = tmpdata << 4 # Shift into high bits to avoid needing to sign extend\n    updata = tmpdata.byteswap()\n    return updata.view(data.dtype)\n```"], [78, "python", "parse_mim2gene", "```python\ndef parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry\n```"], [97, "python", "CreditNoteController.cancellation_fee", "```python\ndef cancellation_fee(self, percentage):\n        ''' Generates an invoice with a cancellation fee, and applies\n        credit to the invoice.\n\n        percentage (Decimal): The percentage of the credit note to turn into\n        a cancellation fee. Must be 0 <= percentage <= 100.\n        '''\n\n        # Local import to fix import cycles. Can we do better?\n        from .invoice import InvoiceController\n\n        assert(percentage >= 0 and percentage <= 100)\n\n        cancellation_fee = self.credit_note.value * percentage / 100\n        due = datetime.timedelta(days=1)\n        item = [(\"Cancellation fee\", cancellation_fee)]\n        invoice = InvoiceController.manual_invoice(\n            self.credit_note.invoice.user, due, item\n        )\n\n        if not invoice.is_paid:\n            self.apply_to_invoice(invoice)\n\n        return InvoiceController(invoice)\n```"], [33, "python", "update_panel", "```python\ndef update_panel(store, panel_name, csv_lines, option):\n    \"\"\"Update an existing gene panel with genes.\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        panel_name(str)\n        csv_lines(iterable(str)): Stream with genes\n        option(str): 'add' or 'replace'\n\n    Returns:\n        panel_obj(dict)\n    \"\"\"\n    new_genes= []\n    panel_obj = store.gene_panel(panel_name)\n    if panel_obj is None:\n        return None\n    try:\n        new_genes = parse_genes(csv_lines) # a list of gene dictionaries containing gene info\n    except SyntaxError as error:\n        flash(error.args[0], 'danger')\n        return None\n\n    # if existing genes are to be replaced by those in csv_lines\n    if option == 'replace':\n        # all existing genes should be deleted\n        for gene in panel_obj['genes']:\n            #create extra key to use in pending actions:\n            gene['hgnc_symbol'] = gene['symbol']\n            store.add_pending(panel_obj, gene, action='delete', info=None)\n\n    for new_gene in new_genes:\n        if not new_gene['hgnc_id']:\n            flash(\"gene missing hgnc id: {}\".format(new_gene['hgnc_symbol']),'danger')\n            continue\n        gene_obj = store.hgnc_gene(new_gene['hgnc_id'])\n        if gene_obj is None:\n            flash(\"gene not found: {} - {}\".format(new_gene['hgnc_id'], new_gene['hgnc_symbol']),'danger')\n            continue\n        if new_gene['hgnc_symbol'] and gene_obj['hgnc_symbol'] != new_gene['hgnc_symbol']:\n            flash(\"symbol mis-match: {0} | {1}\".format(\n                gene_obj['hgnc_symbol'], new_gene['hgnc_symbol']), 'warning')\n\n        info_data = {\n            'disease_associated_transcripts': new_gene['transcripts'],\n            'reduced_penetrance': new_gene['reduced_penetrance'],\n            'mosaicism': new_gene['mosaicism'],\n            'inheritance_models': new_gene['inheritance_models'],\n            'database_entry_version': new_gene['database_entry_version'],\n        }\n        if option == 'replace': # there will be no existing genes for sure, because we're replacing them all\n            action = 'add'\n        else: # add option. Add if genes is not existing. otherwise edit it\n            existing_genes = {gene['hgnc_id'] for gene in panel_obj['genes']}\n            action = 'edit' if gene_obj['hgnc_id'] in existing_genes else 'add'\n        store.add_pending(panel_obj, gene_obj, action=action, info=info_data)\n\n    return panel_obj\n```"], [19, "python", "MinHashLSHForest.index", "```python\ndef index(self):\n        '''\n        Index all the keys added so far and make them searchable.\n        '''\n        for i, hashtable in enumerate(self.hashtables):\n            self.sorted_hashtables[i] = [H for H in hashtable.keys()]\n            self.sorted_hashtables[i].sort()\n```"], [92, "python", "Eaf.generate_ts_id", "```python\ndef generate_ts_id(self, time=None):\n        \"\"\"Generate the next timeslot id, this function is mainly used\n        internally\n\n        :param int time: Initial time to assign to the timeslot.\n        :raises ValueError: If the time is negative.\n        \"\"\"\n        if time and time < 0:\n            raise ValueError('Time is negative...')\n        if not self.maxts:\n            valid_ts = [int(''.join(filter(str.isdigit, a)))\n                        for a in self.timeslots]\n            self.maxts = max(valid_ts + [1])+1\n        else:\n            self.maxts += 1\n        ts = 'ts{:d}'.format(self.maxts)\n        self.timeslots[ts] = time\n        return ts\n```"], [4, "python", "BandwidthLimitedStream.read", "```python\ndef read(self, amount):\n        \"\"\"Read a specified amount\n\n        Reads will only be throttled if bandwidth limiting is enabled.\n        \"\"\"\n        if not self._bandwidth_limiting_enabled:\n            return self._fileobj.read(amount)\n\n        # We do not want to be calling consume on every read as the read\n        # amounts can be small causing the lock of the leaky bucket to\n        # introduce noticeable overhead. So instead we keep track of\n        # how many bytes we have seen and only call consume once we pass a\n        # certain threshold.\n        self._bytes_seen += amount\n        if self._bytes_seen < self._bytes_threshold:\n            return self._fileobj.read(amount)\n\n        self._consume_through_leaky_bucket()\n        return self._fileobj.read(amount)\n```"], [89, "python", "Meter.extractMonthTariff", "```python\ndef extractMonthTariff(self, month):\n        \"\"\" Extract the tariff for a single month from the meter object buffer.\n\n        Args:\n            month (int):  A :class:`~ekmmeters.Months` value or range(Extents.Months).\n\n        Returns:\n            tuple: The eight tariff period totals for month. The return tuple breaks out as follows:\n\n            ================= ======================================\n            kWh_Tariff_1      kWh for tariff period 1 over month.\n            kWh_Tariff_2      kWh for tariff period 2 over month\n            kWh_Tariff_3      kWh for tariff period 3 over month\n            kWh_Tariff_4      kWh for tariff period 4 over month\n            kWh_Tot           Total kWh over requested month\n            Rev_kWh_Tariff_1  Rev kWh for tariff period 1 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 2 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 3 over month\n            Rev_kWh_Tariff_4  Rev kWh for tariff period 4 over month\n            Rev_kWh_Tot       Total Rev kWh over requested month\n            ================= ======================================\n\n        \"\"\"\n        ret = namedtuple(\"ret\", [\"Month\", Field.kWh_Tariff_1, Field.kWh_Tariff_2, Field.kWh_Tariff_3,\n                         Field.kWh_Tariff_4, Field.kWh_Tot, Field.Rev_kWh_Tariff_1,\n                         Field.Rev_kWh_Tariff_2, Field.Rev_kWh_Tariff_3,\n                         Field.Rev_kWh_Tariff_4, Field.Rev_kWh_Tot])\n        month += 1\n        ret.Month = str(month)\n        if (month < 1) or (month > Extents.Months):\n            ret.kWh_Tariff_1 = ret.kWh_Tariff_2 = ret.kWh_Tariff_3 = ret.kWh_Tariff_4 = str(0)\n            ret.Rev_kWh_Tariff_1 = ret.Rev_kWh_Tariff_2 = ret.Rev_kWh_Tariff_3 = ret.Rev_kWh_Tariff_4 = str(0)\n            ret.kWh_Tot = ret.Rev_kWh_Tot = str(0)\n            ekm_log(\"Out of range(Extents.Months) month = \" + str(month))\n            return ret\n\n        base_str = \"Month_\" + str(month) + \"_\"\n        ret.kWh_Tariff_1 = self.m_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.kWh_Tariff_2 = self.m_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.kWh_Tariff_3 = self.m_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.kWh_Tariff_4 = self.m_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.kWh_Tot = self.m_mons[base_str + \"Tot\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_1 = self.m_rev_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_2 = self.m_rev_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_3 = self.m_rev_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_4 = self.m_rev_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.Rev_kWh_Tot = self.m_rev_mons[base_str + \"Tot\"][MeterData.StringValue]\n        return ret\n```"], [38, "python", "BigQueryCursor.executemany", "```python\ndef executemany(self, operation, seq_of_parameters):\n        \"\"\"\n        Execute a BigQuery query multiple times with different parameters.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param seq_of_parameters: List of dictionary parameters to substitute into the\n            query.\n        :type seq_of_parameters: list\n        \"\"\"\n        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)\n```"], [2, "python", "TypeSystem.convert_from_binary", "```python\ndef convert_from_binary(self, binvalue, type, **kwargs):\n        \"\"\"\n        Convert binary data to type 'type'.\n\n        'type' must have a convert_binary function.  If 'type'\n        supports size checking, the size function is called to ensure\n        that binvalue is the correct size for deserialization\n        \"\"\"\n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)\n```"], [33, "python", "read_data", "```python\ndef read_data(data_file, dataformat, name_mode):\n    \"\"\"\n    Load data_file described by a dataformat dict.\n\n    Parameters\n    ----------\n    data_file : str\n        Path to data file, including extension.\n    dataformat : dict\n        A dataformat dict, see example below.\n    name_mode : str\n        How to identyfy sample names. If 'file_names' uses the\n        input name of the file, stripped of the extension. If\n        'metadata_names' uses the 'name' attribute of the 'meta'\n        sub-dictionary in dataformat. If any other str, uses this\n        str as the sample name.\n\n    Example\n    -------\n    >>>\n    {'genfromtext_args': {'delimiter': ',',\n                          'skip_header': 4},  # passed directly to np.genfromtxt\n     'column_id': {'name_row': 3,  # which row contains the column names\n                   'delimiter': ',',  # delimeter between column names\n                   'timecolumn': 0,  # which column contains the 'time' variable\n                   'pattern': '([A-z]{1,2}[0-9]{1,3})'},  # a regex pattern which captures the column names\n     'meta_regex': {  # a dict of (line_no: ([descriptors], [regexs])) pairs\n                    0: (['path'], '(.*)'),\n                    2: (['date', 'method'],  # MUST include date\n                     '([A-Z][a-z]+ [0-9]+ [0-9]{4}[ ]+[0-9:]+ [amp]+).* ([A-z0-9]+\\.m)')\n                   }\n    }\n\n    Returns\n    -------\n    sample, analytes, data, meta : tuple\n    \"\"\"\n    with open(data_file) as f:\n        lines = f.readlines()\n\n    if 'meta_regex' in dataformat.keys():\n        meta = Bunch()\n        for k, v in dataformat['meta_regex'].items():\n            try:\n                out = re.search(v[-1], lines[int(k)]).groups()\n            except:\n                raise ValueError('Failed reading metadata when applying:\\n  regex: {}\\nto\\n  line: {}'.format(v[-1], lines[int(k)]))\n            for i in np.arange(len(v[0])):\n                meta[v[0][i]] = out[i]\n    else:\n        meta = {}\n\n    # sample name\n    if name_mode == 'file_names':\n        sample = os.path.basename(data_file).split('.')[0]\n    elif name_mode == 'metadata_names':\n        sample = meta['name']\n    else:\n        sample = name_mode\n\n    # column and analyte names\n    columns = np.array(lines[dataformat['column_id']['name_row']].strip().split(\n        dataformat['column_id']['delimiter']))\n    if 'pattern' in dataformat['column_id'].keys():\n        pr = re.compile(dataformat['column_id']['pattern'])\n        analytes = [pr.match(c).groups()[0] for c in columns if pr.match(c)]\n\n    # do any required pre-formatting\n    if 'preformat_replace' in dataformat.keys():\n        with open(data_file) as f:\n            fbuffer = f.read()\n        for k, v in dataformat['preformat_replace'].items():\n            fbuffer = re.sub(k, v, fbuffer)\n        # dead data\n        read_data = np.genfromtxt(BytesIO(fbuffer.encode()),\n                                  **dataformat['genfromtext_args']).T\n    else:\n        # read data\n        read_data = np.genfromtxt(data_file,\n                                  **dataformat['genfromtext_args']).T\n\n    # data dict\n    dind = np.zeros(read_data.shape[0], dtype=bool)\n    for a in analytes:\n        dind[columns == a] = True\n\n    data = Bunch()\n    data['Time'] = read_data[dataformat['column_id']['timecolumn']]\n\n    # deal with time units\n    if 'time_unit' in dataformat['column_id']:\n        if isinstance(dataformat['column_id']['time_unit'], (float, int)):\n            time_mult = dataformat['column_id']['time_unit']\n        elif isinstance(dataformat['column_id']['time_unit'], str):\n            unit_multipliers = {'ms': 1/1000,\n                                'min': 60/1,\n                                's': 1}\n            try:\n                time_mult = unit_multipliers[dataformat['column_id']['time_unit']]\n            except:\n                raise ValueError(\"In dataformat: time_unit must be a number, 'ms', 'min' or 's'\")\n        data['Time'] *= time_mult\n        \n    # convert raw data into counts\n    # TODO: Is this correct? Should actually be per-analyte dwell?\n    # if 'unit' in dataformat:\n    #     if dataformat['unit'] == 'cps':\n    #         tstep = data['Time'][1] - data['Time'][0]\n    #         read_data[dind] *= tstep\n    #     else:\n    #         pass\n    data['rawdata'] = Bunch(zip(analytes, read_data[dind]))\n    data['total_counts'] = np.nansum(read_data[dind], 0)\n\n    return sample, analytes, data, meta\n```"], [15, "python", "DrawElement.top", "```python\ndef top(self):\n        \"\"\" Constructs the top line of the element\"\"\"\n        ret = self.top_format % self.top_connect.center(\n            self.width, self.top_pad)\n        if self.right_fill:\n            ret = ret.ljust(self.right_fill, self.top_pad)\n        if self.left_fill:\n            ret = ret.rjust(self.left_fill, self.top_pad)\n        ret = ret.center(self.layer_width, self.top_bck)\n        return ret\n```"], [2, "python", "GoogleCloudBucketHelper.google_cloud_to_local", "```python\ndef google_cloud_to_local(self, file_name):\n        \"\"\"\n        Checks whether the file specified by file_name is stored in Google Cloud\n        Storage (GCS), if so, downloads the file and saves it locally. The full\n        path of the saved file will be returned. Otherwise the local file_name\n        will be returned immediately.\n\n        :param file_name: The full path of input file.\n        :type file_name: str\n        :return: The full path of local file.\n        :rtype: str\n        \"\"\"\n        if not file_name.startswith('gs://'):\n            return file_name\n\n        # Extracts bucket_id and object_id by first removing 'gs://' prefix and\n        # then split the remaining by path delimiter '/'.\n        path_components = file_name[self.GCS_PREFIX_LENGTH:].split('/')\n        if len(path_components) < 2:\n            raise Exception(\n                'Invalid Google Cloud Storage (GCS) object path: {}'\n                .format(file_name))\n\n        bucket_id = path_components[0]\n        object_id = '/'.join(path_components[1:])\n        local_file = '/tmp/dataflow{}-{}'.format(str(uuid.uuid4())[:8],\n                                                 path_components[-1])\n        self._gcs_hook.download(bucket_id, object_id, local_file)\n\n        if os.stat(local_file).st_size > 0:\n            return local_file\n        raise Exception(\n            'Failed to download Google Cloud Storage (GCS) object: {}'\n            .format(file_name))\n```"], [260, "python", "get_single_list_nodes_data", "```python\ndef get_single_list_nodes_data(li, meta_data):\n    \"\"\"\n    Find consecutive li tags that have content that have the same list id.\n    \"\"\"\n    yield li\n    w_namespace = get_namespace(li, 'w')\n    current_numId = get_numId(li, w_namespace)\n    starting_ilvl = get_ilvl(li, w_namespace)\n    el = li\n    while True:\n        el = el.getnext()\n        if el is None:\n            break\n        # If the tag has no content ignore it.\n        if not has_text(el):\n            continue\n\n        # Stop the lists if you come across a list item that should be a\n        # heading.\n        if _is_top_level_upper_roman(el, meta_data):\n            break\n\n        if (\n                is_li(el, meta_data) and\n                (starting_ilvl > get_ilvl(el, w_namespace))):\n            break\n\n        new_numId = get_numId(el, w_namespace)\n        if new_numId is None or new_numId == -1:\n            # Not a p tag or a list item\n            yield el\n            continue\n        # If the list id of the next tag is different that the previous that\n        # means a new list being made (not nested)\n        if current_numId != new_numId:\n            # Not a subsequent list.\n            break\n        if is_last_li(el, meta_data, current_numId):\n            yield el\n            break\n        yield el\n```"], [13, "python", "Node.select", "```python\ndef select(self, selector):\n        \"\"\"\n        Like :meth:`find_all`, but takes a CSS selector string as input.\n        \"\"\"\n        op = operator.methodcaller('select', selector)\n        return self._wrap_multi(op)\n```"], [23, "python", "SqlDatabaseManagementService.list_quotas", "```python\ndef list_quotas(self, server_name):\n        '''\n        Gets quotas for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_quotas_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServerQuota)\n```"], [292, "python", "SolveBioAuth.logout", "```python\ndef logout(self):\n        \"\"\"Revoke the token and remove the cookie.\"\"\"\n        if self._oauth_client_secret:\n            try:\n                oauth_token = flask.request.cookies[self.TOKEN_COOKIE_NAME]\n                # Revoke the token\n                requests.post(\n                    urljoin(self._api_host, self.OAUTH2_REVOKE_TOKEN_PATH),\n                    data={\n                        'client_id': self._oauth_client_id,\n                        'client_secret': self._oauth_client_secret,\n                        'token': oauth_token\n                    })\n            except:\n                pass\n\n        response = flask.redirect('/')\n        self.clear_cookies(response)\n        return response\n```"], [5, "python", "Context.integrity_negotiated", "```python\ndef integrity_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if integrity protection (signing) has been negotiated in this context, False\n        otherwise. If this property is True, you can use :meth:`get_mic` to sign messages with a\n        message integrity code (MIC), which the peer application can verify.\n        \"\"\"\n        return (\n            self.flags & C.GSS_C_INTEG_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )\n```"], [16, "python", "QasmSimulatorPy._get_statevector", "```python\ndef _get_statevector(self):\n        \"\"\"Return the current statevector in JSON Result spec format\"\"\"\n        vec = np.reshape(self._statevector, 2 ** self._number_of_qubits)\n        # Expand complex numbers\n        vec = np.stack([vec.real, vec.imag], axis=1)\n        # Truncate small values\n        vec[abs(vec) < self._chop_threshold] = 0.0\n        return vec\n```"], [1, "python", "OpenIdMixin.authenticate_redirect", "```python\ndef authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))\n```"], [1, "python", "NotebookManager.save_new_notebook", "```python\ndef save_new_notebook(self, data, name=None, format=u'json'):\n        \"\"\"Save a new notebook and return its notebook_id.\n\n        If a name is passed in, it overrides any values in the notebook data\n        and the value in the data is updated to use that value.\n        \"\"\"\n        if format not in self.allowed_formats:\n            raise web.HTTPError(415, u'Invalid notebook format: %s' % format)\n\n        try:\n            nb = current.reads(data.decode('utf-8'), format)\n        except:\n            raise web.HTTPError(400, u'Invalid JSON data')\n\n        if name is None:\n            try:\n                name = nb.metadata.name\n            except AttributeError:\n                raise web.HTTPError(400, u'Missing notebook name')\n        nb.metadata.name = name\n\n        notebook_id = self.new_notebook_id(name)\n        self.save_notebook_object(notebook_id, nb)\n        return notebook_id\n```"], [616, "python", "PassManager.passes", "```python\ndef passes(self):\n        \"\"\"\n        Returns a list structure of the appended passes and its options.\n\n        Returns (list): The appended passes.\n        \"\"\"\n        ret = []\n        for pass_ in self.working_list:\n            ret.append(pass_.dump_passes())\n        return ret\n```"], [38, "python", "DatabaseConnection.read", "```python\ndef read(self, path, params=None):\n        \"\"\"Read the result at the given path (GET) from the CRUD API, using the optional params dictionary\n        as url parameters.\"\"\"\n        return self.handleresult(self.r.get(urljoin(self.url + CRUD_PATH,\n                                                    path),\n                                            params=params))\n```"], [1, "python", "Session.evaluate_script", "```python\ndef evaluate_script(self, script, *args):\n        \"\"\"\n        Evaluate the given JavaScript and return the result. Be careful when using this with\n        scripts that return complex objects, such as jQuery statements. :meth:`execute_script`\n        might be a better alternative.\n\n        Args:\n            script (str): A string of JavaScript to evaluate.\n            *args: Variable length argument list to pass to the executed JavaScript string.\n\n        Returns:\n            object: The result of the evaluated JavaScript (may be driver specific).\n        \"\"\"\n\n        args = [arg.base if isinstance(arg, Base) else arg for arg in args]\n        result = self.driver.evaluate_script(script, *args)\n        return self._wrap_element_script_result(result)\n```"], [31, "python", "get_all_boundary_algorithms", "```python\ndef get_all_boundary_algorithms():\n    \"\"\"Gets all the possible boundary algorithms in MSAF.\n\n    Returns\n    -------\n    algo_ids : list\n        List of all the IDs of boundary algorithms (strings).\n    \"\"\"\n    algo_ids = []\n    for name in msaf.algorithms.__all__:\n        module = eval(msaf.algorithms.__name__ + \".\" + name)\n        if module.is_boundary_type:\n            algo_ids.append(module.algo_id)\n    return algo_ids\n```"], [2, "python", "Expression.op_and", "```python\ndef op_and(self, *elements):\n        \"\"\"Update the ``Expression`` by joining the specified additional\n        ``elements`` using an \"AND\" ``Operator``\n\n        Args:\n            *elements (BaseExpression): The ``Expression`` and/or\n                ``Constraint`` elements which the \"AND\" ``Operator`` applies\n                to.\n\n        Returns:\n            Expression: ``self`` or related ``Expression``.\n        \"\"\"\n        expression = self.add_operator(Operator(';'))\n        for element in elements:\n            expression.add_element(element)\n        return expression\n```"], [10, "python", "LinkyClient._get_data", "```python\ndef _get_data(self, p_p_resource_id, start_date=None, end_date=None):\n        \"\"\"Get data.\"\"\"\n\n        data = {\n            '_' + REQ_PART + '_dateDebut': start_date,\n            '_' + REQ_PART + '_dateFin': end_date\n        }\n\n        params = {\n            'p_p_id': REQ_PART,\n            'p_p_lifecycle': 2,\n            'p_p_state': 'normal',\n            'p_p_mode': 'view',\n            'p_p_resource_id': p_p_resource_id,\n            'p_p_cacheability': 'cacheLevelPage',\n            'p_p_col_id': 'column-1',\n            'p_p_col_pos': 1,\n            'p_p_col_count': 3\n        }\n\n        try:\n            raw_res = self._session.post(DATA_URL,\n                                         data=data,\n                                         params=params,\n                                         allow_redirects=False,\n                                         timeout=self._timeout)\n\n            if 300 <= raw_res.status_code < 400:\n                raw_res = self._session.post(DATA_URL,\n                                             data=data,\n                                             params=params,\n                                             allow_redirects=False,\n                                             timeout=self._timeout)\n        except OSError as e:\n            raise PyLinkyError(\"Could not access enedis.fr: \" + str(e))\n\n        if raw_res.text is \"\":\n            raise PyLinkyError(\"No data\")\n\n        if 302 == raw_res.status_code and \"/messages/maintenance.html\" in raw_res.text:\n            raise PyLinkyError(\"Site in maintenance\")\n\n        try:\n            json_output = raw_res.json()\n        except (OSError, json.decoder.JSONDecodeError, simplejson.errors.JSONDecodeError) as e:\n            raise PyLinkyError(\"Impossible to decode response: \" + str(e) + \"\\nResponse was: \" + str(raw_res.text))\n\n        if json_output.get('etat').get('valeur') == 'erreur':\n            raise PyLinkyError(\"Enedis.fr answered with an error: \" + str(json_output))\n\n        return json_output.get('graphe')\n```"], [161, "python", "clinvar", "```python\ndef clinvar(institute_id, case_name, variant_id):\n    \"\"\"Build a clinVar submission form for a variant.\"\"\"\n    data = controllers.clinvar_export(store, institute_id, case_name, variant_id)\n    if request.method == 'GET':\n        return data\n    else: #POST\n        form_dict = request.form.to_dict()\n        submission_objects = set_submission_objects(form_dict) # A tuple of submission objects (variants and casedata objects)\n\n        # Add submission data to an open clinvar submission object,\n        # or create a new if no open submission is found in database\n        open_submission = store.get_open_clinvar_submission(current_user.email, institute_id)\n        updated_submission = store.add_to_submission(open_submission['_id'], submission_objects)\n\n        # Redirect to clinvar submissions handling page, and pass it the updated_submission_object\n        return redirect(url_for('cases.clinvar_submissions', institute_id=institute_id))\n```"], [7, "python", "MozillaClubParser.__get_event_fields", "```python\ndef __get_event_fields(self):\n        \"\"\"Get the events fields (columns) from the cells received.\"\"\"\n\n        event_fields = {}\n        # The cells in the first row are the column names\n        # Check that the columns names are the same we have as template\n        # Create the event template from the data retrieved\n        while self.ncell < len(self.cells):\n            cell = self.cells[self.ncell]\n            row = cell['gs$cell']['row']\n            if int(row) > 1:\n                # When the row number >1 the column row is finished\n                break\n            ncol = int(cell['gs$cell']['col'])\n            name = cell['content']['$t']\n            event_fields[ncol] = name\n            if ncol in EVENT_TEMPLATE:\n                if event_fields[ncol] != EVENT_TEMPLATE[ncol]:\n                    logger.warning(\"Event template changed in spreadsheet %s vs %s\",\n                                   name, EVENT_TEMPLATE[ncol])\n            else:\n                logger.warning(\"Event template changed in spreadsheet. New column: %s\", name)\n\n            self.ncell += 1\n        return event_fields\n```"], [161, "python", "center_eigenvalue_diff", "```python\ndef center_eigenvalue_diff(mat):\n    \"\"\"Compute the eigvals of mat and then find the center eigval difference.\"\"\"\n    N = len(mat)\n    evals = np.sort(la.eigvals(mat))\n    diff = np.abs(evals[N/2] - evals[N/2-1])\n    return diff\n```"], [3, "python", "MongoHook.delete_one", "```python\ndef delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)\n```"], [1, "python", "Mesh.set_fields", "```python\ndef set_fields(self, fields = None, **kwargs):\n    \"\"\"\n    Sets the fields.\n    \"\"\"\n    self.fields = []\n    if fields != None:\n      for field in fields: \n        self.fields.append(field)\n```"], [119, "python", "ResourceManager.addFromTex", "```python\ndef addFromTex(self,name,img,category):\n        \"\"\"\n        Adds a new texture from the given image.\n        \n        ``img`` may be any object that supports Pyglet-style copying in form of the ``blit_to_texture()`` method.\n        \n        This can be used to add textures that come from non-file sources, e.g. Render-to-texture.\n        \"\"\"\n        texreg = self.categoriesTexBin[category].add(img)\n        #texreg = texreg.get_transform(True,True) # Mirrors the image due to how pyglets coordinate system works\n        # Strange behaviour, sometimes needed and sometimes not\n        \n        self.categories[category][name]=texreg\n        target = texreg.target\n        texid = texreg.id\n        texcoords = texreg.tex_coords\n        \n        # Prevents texture bleeding with texture sizes that are powers of 2, else weird lines may appear at certain angles.\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST_MIPMAP_LINEAR)\n        glGenerateMipmap(GL_TEXTURE_2D)\n        \n        out = target,texid,texcoords\n        self.categoriesTexCache[category][name]=out\n        return out\n```"], [1, "python", "InstallRequirement.populate_link", "```python\ndef populate_link(self, finder, upgrade):\n        \"\"\"Ensure that if a link can be found for this, that it is found.\n\n        Note that self.link may still be None - if Upgrade is False and the\n        requirement is already installed.\n        \"\"\"\n        if self.link is None:\n            self.link = finder.find_requirement(self, upgrade)\n```"], [227, "python", "MultivariateNormalTriL.params_size", "```python\ndef params_size(event_size, name=None):\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    with tf.compat.v1.name_scope(name, 'MultivariateNormalTriL_params_size',\n                                 [event_size]):\n      return event_size + event_size * (event_size + 1) // 2\n```"], [8, "python", "ConnectionHandler.use_music_service", "```python\ndef use_music_service(self, service_name, api_key):\n        \"\"\"\n        Sets the current music service to service_name.\n\n        :param str service_name: Name of the music service\n        :param str api_key: Optional API key if necessary\n        \"\"\"\n\n        try:\n            self.current_music = self.music_services[service_name]\n        except KeyError:\n            if service_name == 'youtube':\n                self.music_services['youtube'] = Youtube()\n                self.current_music = self.music_services['youtube']\n            elif service_name == 'soundcloud':\n                self.music_services['soundcloud'] = Soundcloud(api_key=api_key)\n                self.current_music = self.music_services['soundcloud']\n            else:\n                log.error('Music service name is not recognized.')\n```"], [89, "python", "Striplog.invert", "```python\ndef invert(self, copy=False):\n        \"\"\"\n        Inverts the striplog, changing its order and the order of its contents.\n\n        Operates in place by default.\n\n        Args:\n            copy (bool): Whether to operate in place or make a copy.\n\n        Returns:\n            None if operating in-place, or an inverted copy of the striplog\n                if not.\n        \"\"\"\n        if copy:\n            return Striplog([i.invert(copy=True) for i in self])\n        else:\n            for i in self:\n                i.invert()\n            self.__sort()\n            o = self.order\n            self.order = {'depth': 'elevation', 'elevation': 'depth'}[o]\n            return\n```"], [199, "python", "VCGPrinter.edge", "```python\ndef edge(self, from_node, to_node, edge_type=\"\", **args):\n        \"\"\"draw an edge from a node to another.\n        \"\"\"\n        self._stream.write(\n            '%s%sedge: {sourcename:\"%s\" targetname:\"%s\"'\n            % (self._indent, edge_type, from_node, to_node)\n        )\n        self._write_attributes(EDGE_ATTRS, **args)\n        self._stream.write(\"}\\n\")\n```"], [86, "python", "pdf_case_report", "```python\ndef pdf_case_report(institute_id, case_name):\n    \"\"\"Download a pdf report for a case\"\"\"\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.case_report_content(store, institute_obj, case_obj)\n\n    # add coverage report on the bottom of this report\n    if current_app.config.get('SQLALCHEMY_DATABASE_URI'):\n        data['coverage_report'] = controllers.coverage_report_contents(store, institute_obj, case_obj, request.url_root)\n\n    # workaround to be able to print the case pedigree to pdf\n    if case_obj.get('madeline_info') is not None:\n        with open(os.path.join(cases_bp.static_folder, 'madeline.svg'), 'w') as temp_madeline:\n            temp_madeline.write(case_obj['madeline_info'])\n\n    html_report = render_template('cases/case_report.html', institute=institute_obj, case=case_obj, format='pdf', **data)\n    return render_pdf(HTML(string=html_report), download_filename=case_obj['display_name']+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')\n```"], [36, "python", "TaskInstance.are_dependencies_met", "```python\ndef are_dependencies_met(\n            self,\n            dep_context=None,\n            session=None,\n            verbose=False):\n        \"\"\"\n        Returns whether or not all the conditions are met for this task instance to be run\n        given the context for the dependencies (e.g. a task instance being force run from\n        the UI will ignore some dependencies).\n\n        :param dep_context: The execution context that determines the dependencies that\n            should be evaluated.\n        :type dep_context: DepContext\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param verbose: whether log details on failed dependencies on\n            info or debug log level\n        :type verbose: bool\n        \"\"\"\n        dep_context = dep_context or DepContext()\n        failed = False\n        verbose_aware_logger = self.log.info if verbose else self.log.debug\n        for dep_status in self.get_failed_dep_statuses(\n                dep_context=dep_context,\n                session=session):\n            failed = True\n\n            verbose_aware_logger(\n                \"Dependencies not met for %s, dependency '%s' FAILED: %s\",\n                self, dep_status.dep_name, dep_status.reason\n            )\n\n        if failed:\n            return False\n\n        verbose_aware_logger(\"Dependencies all met for %s\", self)\n        return True\n```"], [24, "python", "map_method", "```python\ndef map_method(method,object_list,*argseq,**kw):\n    \"\"\"map_method(method,object_list,*args,**kw) -> list\n\n    Return a list of the results of applying the methods to the items of the\n    argument sequence(s).  If more than one sequence is given, the method is\n    called with an argument list consisting of the corresponding item of each\n    sequence. All sequences must be of the same length.\n\n    Keyword arguments are passed verbatim to all objects called.\n\n    This is Python code, so it's not nearly as fast as the builtin map().\"\"\"\n\n    out_list = []\n    idx = 0\n    for object in object_list:\n        try:\n            handler = getattr(object, method)\n        except AttributeError:\n            out_list.append(None)\n        else:\n            if argseq:\n                args = map(lambda lst:lst[idx],argseq)\n                #print 'ob',object,'hand',handler,'ar',args # dbg\n                out_list.append(handler(args,**kw))\n            else:\n                out_list.append(handler(**kw))\n        idx += 1\n    return out_list\n```"], [13, "python", "Gerrit.parse_reviews", "```python\ndef parse_reviews(raw_data):\n        \"\"\"Parse a Gerrit reviews list.\"\"\"\n\n        # Join isolated reviews in JSON in array for parsing\n        items_raw = \"[\" + raw_data.replace(\"\\n\", \",\") + \"]\"\n        items_raw = items_raw.replace(\",]\", \"]\")\n        items = json.loads(items_raw)\n        reviews = []\n\n        for item in items:\n            if 'project' in item.keys():\n                reviews.append(item)\n\n        return reviews\n```"], [408, "python", "start", "```python\ndef start(dashboards, once, secrets):\n    \"\"\"Display a dashboard from the dashboard file(s) provided in the DASHBOARDS\n       Paths and/or URLs for dashboards (URLs must secrets with http or https)\n    \"\"\"\n\n    if secrets is None:\n        secrets = os.path.join(os.path.expanduser(\"~\"), \"/.doodledashboard/secrets\")\n\n    try:\n        loaded_secrets = try_read_secrets_file(secrets)\n    except InvalidSecretsException as err:\n        click.echo(get_error_message(err, default=\"Secrets file is invalid\"), err=True)\n        raise click.Abort()\n\n    read_configs = [\"\"\"\n    dashboard:\n      display:\n        type: console\n    \"\"\"]\n    for dashboard_file in dashboards:\n        read_configs.append(read_file(dashboard_file))\n\n    dashboard_config = DashboardConfigReader(initialise_component_loader(), loaded_secrets)\n\n    try:\n        dashboard = read_dashboard_from_config(dashboard_config, read_configs)\n    except YAMLError as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    try:\n        DashboardValidator().validate(dashboard)\n    except ValidationException as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    explain_dashboard(dashboard)\n\n    click.echo(\"Dashboard running...\")\n\n    while True:\n        try:\n            DashboardRunner(dashboard).cycle()\n        except SecretNotFound as err:\n            click.echo(get_error_message(err, default=\"Datafeed didn't have required secret\"), err=True)\n            raise click.Abort()\n\n        if once:\n            break\n```"], [1, "python", "OAuth2RequestValidator.authenticate_client", "```python\ndef authenticate_client(self, request, *args, **kwargs):\n        \"\"\"Authenticate itself in other means.\n\n        Other means means is described in `Section 3.2.1`_.\n\n        .. _`Section 3.2.1`: http://tools.ietf.org/html/rfc6749#section-3.2.1\n        \"\"\"\n        client_id, client_secret = self._get_client_creds_from_request(request)\n        log.debug('Authenticate client %r', client_id)\n\n        client = self._clientgetter(client_id)\n        if not client:\n            log.debug('Authenticate client failed, client not found.')\n            return False\n\n        request.client = client\n\n        # http://tools.ietf.org/html/rfc6749#section-2\n        # The client MAY omit the parameter if the client secret is an empty string.\n        if hasattr(client, 'client_secret') and client.client_secret != client_secret:\n            log.debug('Authenticate client failed, secret not match.')\n            return False\n\n        log.debug('Authenticate client success.')\n        return True\n```"], [27, "python", "Trajectory.f_get_parameters", "```python\ndef f_get_parameters(self, fast_access=False, copy=True):\n        \"\"\" Returns a dictionary containing the full parameter names as keys and the parameters\n         or the parameter data items as values.\n\n\n        :param fast_access:\n\n            Determines whether the parameter objects or their values are returned\n            in the dictionary.\n\n        :param copy:\n\n            Whether the original dictionary or a shallow copy is returned.\n            If you want the real dictionary please do not modify it at all!\n            Not Copying and fast access do not work at the same time! Raises ValueError\n            if fast access is true and copy false.\n\n        :return: Dictionary containing the parameters.\n\n        :raises: ValueError\n\n        \"\"\"\n        return self._return_item_dictionary(self._parameters, fast_access, copy)\n```"], [4, "python", "load_config", "```python\ndef load_config(under_test=False, custom=None):  # pragma: no cover\n    \"\"\"\n    Load the configuration.\n\n    :param under_test:\n        Tell us if we only have to load the configuration file (True)\n        or load the configuration file and initate the output directory\n        if it does not exist (False).\n    :type under_test: bool\n\n    :param custom:\n        A dict with the configuration index (from .PyFunceble.yaml) to update.\n    :type custom: dict\n\n    .. warning::\n        If :code:`custom` is given, the given :code:`dict` overwrite\n        the last value of the given configuration indexes.\n    \"\"\"\n\n    if \"config_loaded\" not in INTERN:\n        # The configuration was not already loaded.\n\n        # We load and download the different configuration file if they are non\n        # existant.\n        Load(CURRENT_DIRECTORY)\n\n        if not under_test:\n            # If we are not under test which means that we want to save informations,\n            # we initiate the directory structure.\n            DirectoryStructure()\n\n        # We save that the configuration was loaded.\n        INTERN.update({\"config_loaded\": True})\n\n        if custom and isinstance(custom, dict):\n            # The given configuration is not None or empty.\n            # and\n            # It is a dict.\n\n            # We update the configuration index.\n            CONFIGURATION.update(custom)\n```"], [1, "python", "FourDirectionalMoveController.registerEventHandlers", "```python\ndef registerEventHandlers(self):\n        \"\"\"\n        Registers needed keybinds and schedules the :py:meth:`update` Method.\n        \n        You can control what keybinds are used via the :confval:`controls.controls.forward` etc. Configuration Values.\n        \"\"\"\n        # Forward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.forward\"],\"peng3d:actor.%s.player.controls.forward\"%self.actor.uuid,self.on_fwd_down,False)\n        # Backward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.backward\"],\"peng3d:actor.%s.player.controls.backward\"%self.actor.uuid,self.on_bwd_down,False)\n        # Strafe Left\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.strafeleft\"],\"peng3d:actor.%s.player.controls.strafeleft\"%self.actor.uuid,self.on_left_down,False)\n        # Strafe Right\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.straferight\"],\"peng3d:actor.%s.player.controls.straferight\"%self.actor.uuid,self.on_right_down,False)\n        pyglet.clock.schedule_interval(self.update,1.0/60)\n```"], [2, "python", "FrameIdentifierVisitor.visit_Name", "```python\ndef visit_Name(self, node):\n        \"\"\"All assignments to names go through this function.\"\"\"\n        if node.ctx == 'store':\n            self.identifiers.declared_locally.add(node.name)\n        elif node.ctx == 'param':\n            self.identifiers.declared_parameter.add(node.name)\n        elif node.ctx == 'load' and not \\\n             self.identifiers.is_declared(node.name):\n            self.identifiers.undeclared.add(node.name)\n```"], [24, "python", "Client._call", "```python\ndef _call(self, method, params=None, request_id=None):\n        \"\"\" Calls the JSON-RPC endpoint. \"\"\"\n        params = params or []\n\n        # Determines which 'id' value to use and increment the counter associated with the current\n        # client instance if applicable.\n        rid = request_id or self._id_counter\n        if request_id is None:\n            self._id_counter += 1\n\n        # Prepares the payload and the headers that will be used to forge the request.\n        payload = {'jsonrpc': '2.0', 'method': method, 'params': params, 'id': rid}\n        headers = {'Content-Type': 'application/json'}\n        scheme = 'https' if self.tls else 'http'\n        url = '{}://{}:{}'.format(scheme, self.host, self.port)\n\n        # Calls the JSON-RPC endpoint!\n        try:\n            response = self.session.post(url, headers=headers, data=json.dumps(payload))\n            response.raise_for_status()\n        except HTTPError:\n            raise TransportError(\n                'Got unsuccessful response from server (status code: {})'.format(\n                    response.status_code),\n                response=response)\n\n        # Ensures the response body can be deserialized to JSON.\n        try:\n            response_data = response.json()\n        except ValueError as e:\n            raise ProtocolError(\n                'Unable to deserialize response body: {}'.format(e), response=response)\n\n        # Properly handles potential errors.\n        if response_data.get('error'):\n            code = response_data['error'].get('code', '')\n            message = response_data['error'].get('message', '')\n            raise ProtocolError(\n                'Error[{}] {}'.format(code, message), response=response, data=response_data)\n        elif 'result' not in response_data:\n            raise ProtocolError(\n                'Response is empty (result field is missing)', response=response,\n                data=response_data)\n\n        return response_data['result']\n```"], [67, "python", "Client._update_secrets", "```python\ndef _update_secrets(self):\n        '''update secrets will update metadata needed for pull and search\n        '''\n        self.token = self._required_get_and_update('SREGISTRY_GITLAB_TOKEN')\n        self.headers[\"Private-Token\"] = self.token\n```"]]}, "_runtime": 534.4983806610107, "_timestamp": 1571435555.6244793, "_step": 31}
{"Examples-Validation": {"_type": "table", "columns": ["Rank", "Language", "Query", "Code"], "data": [[476, "python", "describe", "```python\ndef describe(lcdict, returndesc=False, offsetwith=None):\n    '''This describes the light curve object and columns present.\n\n    Parameters\n    ----------\n\n    lcdict : dict\n        The input lcdict to parse for column and metadata info.\n\n    returndesc : bool\n        If True, returns the description string as an str instead of just\n        printing it to stdout.\n\n    offsetwith : str\n        This is a character to offset the output description lines by. This is\n        useful to add comment characters like '#' to the output description\n        lines.\n\n    Returns\n    -------\n\n    str or None\n        If returndesc is True, returns the description lines as a str, otherwise\n        returns nothing.\n\n    '''\n\n    # transparently read LCC CSV format description\n    if 'lcformat' in lcdict and 'lcc-csv' in lcdict['lcformat'].lower():\n        return describe_lcc_csv(lcdict, returndesc=returndesc)\n\n\n    # figure out the columndefs part of the header string\n    columndefs = []\n\n    for colind, column in enumerate(lcdict['columns']):\n\n        if '_' in column:\n            colkey, colap = column.split('_')\n            coldesc = COLUMNDEFS[colkey][0] % colap\n        else:\n            coldesc = COLUMNDEFS[column][0]\n\n        columndefstr = '%03i - %s - %s' % (colind,\n                                           column,\n                                           coldesc)\n        columndefs.append(columndefstr)\n\n    columndefs = '\\n'.join(columndefs)\n\n    # figure out the filterdefs\n    filterdefs = []\n\n    for row in lcdict['filters']:\n\n        filterid, filtername, filterdesc = row\n        filterdefstr = '%s - %s - %s' % (filterid,\n                                         filtername,\n                                         filterdesc)\n        filterdefs.append(filterdefstr)\n\n    filterdefs = '\\n'.join(filterdefs)\n\n\n    # figure out the apertures\n    aperturedefs = []\n    for key in sorted(lcdict['lcapertures'].keys()):\n        aperturedefstr = '%s - %.2f px' % (key, lcdict['lcapertures'][key])\n        aperturedefs.append(aperturedefstr)\n\n    aperturedefs = '\\n'.join(aperturedefs)\n\n    # now fill in the description\n    description = DESCTEMPLATE.format(\n        objectid=lcdict['objectid'],\n        hatid=lcdict['objectinfo']['hatid'],\n        twomassid=lcdict['objectinfo']['twomassid'].strip(),\n        ra=lcdict['objectinfo']['ra'],\n        decl=lcdict['objectinfo']['decl'],\n        pmra=lcdict['objectinfo']['pmra'],\n        pmra_err=lcdict['objectinfo']['pmra_err'],\n        pmdecl=lcdict['objectinfo']['pmdecl'],\n        pmdecl_err=lcdict['objectinfo']['pmdecl_err'],\n        jmag=lcdict['objectinfo']['jmag'],\n        hmag=lcdict['objectinfo']['hmag'],\n        kmag=lcdict['objectinfo']['kmag'],\n        bmag=lcdict['objectinfo']['bmag'],\n        vmag=lcdict['objectinfo']['vmag'],\n        sdssg=lcdict['objectinfo']['sdssg'],\n        sdssr=lcdict['objectinfo']['sdssr'],\n        sdssi=lcdict['objectinfo']['sdssi'],\n        ndet=lcdict['objectinfo']['ndet'],\n        lcsortcol=lcdict['lcsortcol'],\n        lcbestaperture=json.dumps(lcdict['lcbestaperture'],ensure_ascii=True),\n        network=lcdict['objectinfo']['network'],\n        stations=lcdict['objectinfo']['stations'],\n        lastupdated=lcdict['lastupdated'],\n        datarelease=lcdict['datarelease'],\n        lcversion=lcdict['lcversion'],\n        lcserver=lcdict['lcserver'],\n        comment=lcdict['comment'],\n        lcfiltersql=(lcdict['lcfiltersql'] if 'lcfiltersql' in lcdict else ''),\n        lcnormcols=(lcdict['lcnormcols'] if 'lcnormcols' in lcdict else ''),\n        filterdefs=filterdefs,\n        columndefs=columndefs,\n        aperturedefs=aperturedefs\n    )\n\n    if offsetwith is not None:\n        description = textwrap.indent(\n            description,\n            '%s ' % offsetwith,\n            lambda line: True\n        )\n        print(description)\n    else:\n        print(description)\n\n    if returndesc:\n        return description\n```"], [8, "python", "Client.append_stream", "```python\ndef append_stream(self, destination, *, offset=0):\n        \"\"\"\n        Create stream for append (write) data to `destination` file.\n\n        :param destination: destination path of file on server side\n        :type destination: :py:class:`str` or :py:class:`pathlib.PurePosixPath`\n\n        :param offset: byte offset for stream start position\n        :type offset: :py:class:`int`\n\n        :rtype: :py:class:`aioftp.DataConnectionThrottleStreamIO`\n        \"\"\"\n        return self.get_stream(\n            \"APPE \" + str(destination),\n            \"1xx\",\n            offset=offset,\n        )\n```"], [1, "python", "Manager.get_droplet_snapshots", "```python\ndef get_droplet_snapshots(self):\n        \"\"\"\n            This method returns a list of all Snapshots based on Droplets.\n        \"\"\"\n        data = self.get_data(\"snapshots?resource_type=droplet\")\n        return [\n            Snapshot(token=self.token, **snapshot)\n            for snapshot in data['snapshots']\n        ]\n```"], [168, "python", "prepare_upload_bundle", "```python\ndef prepare_upload_bundle(name, data):\n    \"\"\"GeoServer's REST API uses ZIP archives as containers for file formats such\n    as Shapefile and WorldImage which include several 'boxcar' files alongside\n    the main data.  In such archives, GeoServer assumes that all of the relevant\n    files will have the same base name and appropriate extensions, and live in\n    the root of the ZIP archive.  This method produces a zip file that matches\n    these expectations, based on a basename, and a dict of extensions to paths or\n    file-like objects. The client code is responsible for deleting the zip\n    archive when it's done.\"\"\"\n    fd, path = mkstemp()\n    zip_file = ZipFile(path, 'w')\n    for ext, stream in data.items():\n        fname = \"%s.%s\" % (name, ext)\n        if (isinstance(stream, basestring)):\n            zip_file.write(stream, fname)\n        else:\n            zip_file.writestr(fname, stream.read())\n    zip_file.close()\n    os.close(fd)\n    return path\n```"], [203, "python", "IndexBuilder.freeze", "```python\ndef freeze(self):\n        \"\"\"Create a usable data structure for serializing.\"\"\"\n        data = super(IndexBuilder, self).freeze()\n        try:\n            # Sphinx >= 1.5 format\n            # Due to changes from github.com/sphinx-doc/sphinx/pull/2454\n            base_file_names = data['docnames']\n        except KeyError:\n            # Sphinx < 1.5 format\n            base_file_names = data['filenames']\n\n        store = {}\n        c = itertools.count()\n        for prefix, items in iteritems(data['objects']):\n            for name, (index, typeindex, _, shortanchor) in iteritems(items):\n                objtype = data['objtypes'][typeindex]\n                if objtype.startswith('cpp:'):\n                    split =  name.rsplit('::', 1)\n                    if len(split) != 2:\n                        warnings.warn(\"What's up with %s?\" % str((prefix, name, objtype)))\n                        continue\n                    prefix, name = split\n                    last_prefix = prefix.split('::')[-1]\n                else:\n                    last_prefix = prefix.split('.')[-1]\n\n                store[next(c)] = {\n                    'filename': base_file_names[index],\n                    'objtype': objtype,\n                    'prefix': prefix,\n                    'last_prefix': last_prefix,\n                    'name': name,\n                    'shortanchor': shortanchor,\n                }\n\n        data.update({'store': store})\n        return data\n```"], [86, "python", "Draw.bezier", "```python\ndef bezier(self, points):\n        \"\"\"Draw a Bezier-curve.\n\n        :param points: ex.) ((5, 5), (6, 6), (7, 7))\n        :type points: list\n        \"\"\"\n        coordinates = pgmagick.CoordinateList()\n        for point in points:\n            x, y = float(point[0]), float(point[1])\n            coordinates.append(pgmagick.Coordinate(x, y))\n        self.drawer.append(pgmagick.DrawableBezier(coordinates))\n```"], [1, "python", "DeleteFile", "```python\ndef DeleteFile(target_filename):\n    '''\n    Deletes the given local filename.\n\n    .. note:: If file doesn't exist this method has no effect.\n\n    :param unicode target_filename:\n        A local filename\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a non-local path\n\n    :raises FileOnlyActionError:\n        Raised when filename refers to a directory.\n    '''\n    _AssertIsLocal(target_filename)\n\n    try:\n        if IsLink(target_filename):\n            DeleteLink(target_filename)\n        elif IsFile(target_filename):\n            os.remove(target_filename)\n        elif IsDir(target_filename):\n            from ._exceptions import FileOnlyActionError\n            raise FileOnlyActionError(target_filename)\n    except Exception as e:\n        reraise(e, 'While executing filesystem.DeleteFile(%s)' % (target_filename))\n```"], [64, "python", "WechatExt.get_group_list", "```python\ndef get_group_list(self):\n        \"\"\"\n        \u83b7\u53d6\u5206\u7ec4\u5217\u8868\n\n        \u8fd4\u56deJSON\u793a\u4f8b::\n\n            {\n                \"groups\": [\n                    {\n                        \"cnt\": 8,\n                        \"id\": 0,\n                        \"name\": \"\u672a\u5206\u7ec4\"\n                    },\n                    {\n                        \"cnt\": 0,\n                        \"id\": 1,\n                        \"name\": \"\u9ed1\u540d\u5355\"\n                    },\n                    {\n                        \"cnt\": 0,\n                        \"id\": 2,\n                        \"name\": \"\u661f\u6807\u7ec4\"\n                    }\n                ]\n            }\n\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\n        :raises NeedLoginError: \u64cd\u4f5c\u672a\u6267\u884c\u6210\u529f, \u9700\u8981\u518d\u6b21\u5c1d\u8bd5\u767b\u5f55, \u5f02\u5e38\u5185\u5bb9\u4e3a\u670d\u52a1\u5668\u8fd4\u56de\u7684\u9519\u8bef\u6570\u636e\n        \"\"\"\n        url = 'https://mp.weixin.qq.com/cgi-bin/contactmanage?t=user/index&pagesize=10&pageidx=0&type=0&groupid=0&lang=zh_CN&f=json&token={token}'.format(\n            token=self.__token,\n        )\n        headers = {\n            'x-requested-with': 'XMLHttpRequest',\n            'referer': 'https://mp.weixin.qq.com/cgi-bin/contactmanage?t=user/index&pagesize=10&pageidx=0&type=0&groupid=0&lang=zh_CN&token='.format(\n                token=self.__token,\n            ),\n            'cookie': self.__cookies,\n        }\n        r = requests.get(url, headers=headers)\n\n        try:\n            message = json.loads(r.text)['group_list']\n        except (KeyError, ValueError):\n            raise NeedLoginError(r.text)\n\n        return message\n```"], [7, "python", "activate", "```python\ndef activate(paths, skip_local, skip_shared):\n    '''Activate an environment'''\n\n\n    if not paths:\n        ctx = click.get_current_context()\n        if cpenv.get_active_env():\n            ctx.invoke(info)\n            return\n\n        click.echo(ctx.get_help())\n        examples = (\n            '\\nExamples: \\n'\n            '    cpenv activate my_env\\n'\n            '    cpenv activate ./relative/path/to/my_env\\n'\n            '    cpenv activate my_env my_module\\n'\n        )\n        click.echo(examples)\n        return\n\n    if skip_local:\n        cpenv.module_resolvers.remove(cpenv.resolver.module_resolver)\n        cpenv.module_resolvers.remove(cpenv.resolver.active_env_module_resolver)\n\n    if skip_shared:\n        cpenv.module_resolvers.remove(cpenv.resolver.modules_path_resolver)\n\n    try:\n        r = cpenv.resolve(*paths)\n    except cpenv.ResolveError as e:\n        click.echo('\\n' + str(e))\n        return\n\n    resolved = set(r.resolved)\n    active_modules = set()\n    env = cpenv.get_active_env()\n    if env:\n        active_modules.add(env)\n    active_modules.update(cpenv.get_active_modules())\n\n    new_modules = resolved - active_modules\n    old_modules = active_modules & resolved\n\n    if old_modules and not new_modules:\n        click.echo(\n            '\\nModules already active: '\n            + bold(' '.join([obj.name for obj in old_modules]))\n        )\n        return\n\n    if env and contains_env(new_modules):\n        click.echo('\\nUse bold(exit) to leave your active environment first.')\n        return\n\n    click.echo('\\nResolved the following modules...')\n    click.echo(format_objects(r.resolved))\n    r.activate()\n    click.echo(blue('\\nLaunching subshell...'))\n\n    modules = sorted(resolved | active_modules, key=_type_and_name)\n    prompt = ':'.join([obj.name for obj in modules])\n    shell.launch(prompt)\n```"], [48, "python", "create", "```python\ndef create(name_or_path, config):\n    '''Create a new environment.'''\n\n    if not name_or_path:\n        ctx = click.get_current_context()\n        click.echo(ctx.get_help())\n        examples = (\n            '\\nExamples:\\n'\n            '    cpenv create my_env\\n'\n            '    cpenv create ./relative/path/to/my_env\\n'\n            '    cpenv create my_env --config ./relative/path/to/config\\n'\n            '    cpenv create my_env --config git@github.com:user/config.git\\n'\n        )\n        click.echo(examples)\n        return\n\n    click.echo(\n        blue('Creating a new virtual environment ' + name_or_path)\n    )\n    try:\n        env = cpenv.create(name_or_path, config)\n    except Exception as e:\n        click.echo(bold_red('FAILED TO CREATE ENVIRONMENT!'))\n        click.echo(e)\n    else:\n        click.echo(bold_green('Successfully created environment!'))\n    click.echo(blue('Launching subshell'))\n\n    cpenv.activate(env)\n    shell.launch(env.name)\n```"], [199, "python", "Parser.p_pkg_down_value_1", "```python\ndef p_pkg_down_value_1(self, p):\n        \"\"\"pkg_down_value : LINE \"\"\"\n        if six.PY2:\n            p[0] = p[1].decode(encoding='utf-8')\n        else:\n            p[0] = p[1]\n```"], [149, "python", "get_type", "```python\nasync def get_type(media, path=None):\n    \"\"\"\n    Parameters\n    ----------\n    media : file object\n        A file object of the image\n    path : str, optional\n        The path to the file\n\n    Returns\n    -------\n    str\n        The mimetype of the media\n    str\n        The category of the media on Twitter\n    \"\"\"\n    if magic:\n        if not media:\n            raise TypeError(\"Media data is empty\")\n\n        _logger.debug(\"guessing mimetype using magic\")\n        media_type = mime.from_buffer(media[:1024])\n    else:\n        media_type = None\n        if path:\n            _logger.debug(\"guessing mimetype using built-in module\")\n            media_type = mime.guess_type(path)[0]\n\n        if media_type is None:\n            msg = (\"Could not guess the mimetype of the media.\\n\"\n                   \"Please consider installing python-magic\\n\"\n                   \"(pip3 install peony-twitter[magic])\")\n            raise RuntimeError(msg)\n\n    return media_type\n```"], [73, "python", "AlignmentPropertyMatrix.bundle", "```python\ndef bundle(self, reset=False, shallow=False): # Copies the original matrix (Use lots of memory)\r\n        \"\"\"\r\n        Returns ``AlignmentPropertyMatrix`` object in which loci are bundled using grouping information.\r\n\r\n        :param reset: whether to reset the values at the loci\r\n        :param shallow: whether to copy all the meta data\r\n        \"\"\"\r\n        if self.finalized:\r\n            # if self.num_groups > 0:\r\n            if self.groups is not None and self.gname is not None:\r\n                grp_conv_mat = lil_matrix((self.num_loci, self.num_groups))\r\n                for i in xrange(self.num_groups):\r\n                    grp_conv_mat[self.groups[i], i] = 1.0\r\n                grp_align = Sparse3DMatrix.__mul__(self, grp_conv_mat) # The core of the bundling\r\n                grp_align.num_loci = self.num_groups\r\n                grp_align.num_haplotypes = self.num_haplotypes\r\n                grp_align.num_reads = self.num_reads\r\n                grp_align.shape = (grp_align.num_loci, grp_align.num_haplotypes, grp_align.num_reads)\r\n                if not shallow:\r\n                    grp_align.lname = copy.copy(self.gname)\r\n                    grp_align.hname = self.hname\r\n                    grp_align.rname = copy.copy(self.rname)\r\n                    grp_align.lid   = dict(zip(grp_align.lname, np.arange(grp_align.num_loci)))\r\n                    grp_align.rid   = copy.copy(self.rid)\r\n                if reset:\r\n                    grp_align.reset()\r\n                return grp_align\r\n            else:\r\n                raise RuntimeError('No group information is available for bundling.')\r\n        else:\r\n            raise RuntimeError('The matrix is not finalized.')\n```"], [106, "python", "Townsend_Hales", "```python\ndef Townsend_Hales(T, Tc, Vc, omega):\n    r'''Calculates saturation liquid density, using the Townsend and Hales\n    CSP method as modified from the original Riedel equation. Uses\n    chemical critical volume and temperature, as well as acentric factor\n\n    The density of a liquid is given by:\n\n    .. math::\n        Vs = V_c/\\left(1+0.85(1-T_r)+(1.692+0.986\\omega)(1-T_r)^{1/3}\\right)\n\n    Parameters\n    ----------\n    T : float\n        Temperature of fluid [K]\n    Tc : float\n        Critical temperature of fluid [K]\n    Vc : float\n        Critical volume of fluid [m^3/mol]\n    omega : float\n        Acentric factor for fluid, [-]\n\n    Returns\n    -------\n    Vs : float\n        Saturation liquid volume, [m^3/mol]\n\n    Notes\n    -----\n    The requirement for critical volume and acentric factor requires all data.\n\n    Examples\n    --------\n    >>> Townsend_Hales(300, 647.14, 55.95E-6, 0.3449)\n    1.8007361992619923e-05\n\n    References\n    ----------\n    .. [1] Hales, J. L, and R Townsend. \"Liquid Densities from 293 to 490 K of\n       Nine Aromatic Hydrocarbons.\" The Journal of Chemical Thermodynamics\n       4, no. 5 (1972): 763-72. doi:10.1016/0021-9614(72)90050-X\n    '''\n    Tr = T/Tc\n    return Vc/(1 + 0.85*(1-Tr) + (1.692 + 0.986*omega)*(1-Tr)**(1/3.))\n```"], [6, "python", "_nbytes", "```python\ndef _nbytes(buf):\n    \"\"\"Return byte-size of a memoryview or buffer.\"\"\"\n    if isinstance(buf, memoryview):\n        if PY3:\n            # py3 introduces nbytes attribute\n            return buf.nbytes\n        else:\n            # compute nbytes on py2\n            size = buf.itemsize\n            for dim in buf.shape:\n                size *= dim\n            return size\n    else:\n        # not a memoryview, raw bytes/ py2 buffer\n        return len(buf)\n```"], [22, "python", "update", "```python\ndef update(x, **entries):\n    \"\"\"Update a dict, or an object with slots, according to `entries` dict.\n\n    >>> update({'a': 1}, a=10, b=20)\n    {'a': 10, 'b': 20}\n    >>> update(Struct(a=1), a=10, b=20)\n    Struct(a=10, b=20)\n    \"\"\"\n    if isinstance(x, dict):\n        x.update(entries)\n    else:\n        x.__dict__.update(entries)\n    return x\n```"], [2, "python", "Encoder.getDecoderOutputFieldTypes", "```python\ndef getDecoderOutputFieldTypes(self):\n    \"\"\"\n    Returns a sequence of field types corresponding to the elements in the\n    decoded output field array.  The types are defined by\n    :class:`~nupic.data.field_meta.FieldMetaType`.\n\n    :return: list of :class:`~nupic.data.field_meta.FieldMetaType` objects\n    \"\"\"\n    if hasattr(self, '_flattenedFieldTypeList') and \\\n          self._flattenedFieldTypeList is not None:\n      return self._flattenedFieldTypeList\n\n    fieldTypes = []\n\n    # NOTE: we take care of the composites, but leaf encoders must override\n    #       this method and return a list of one field_meta.FieldMetaType.XXXX\n    #       element corresponding to the encoder's decoder output field type\n    for (name, encoder, offset) in self.encoders:\n      subTypes = encoder.getDecoderOutputFieldTypes()\n      fieldTypes.extend(subTypes)\n\n    self._flattenedFieldTypeList = fieldTypes\n    return fieldTypes\n```"], [10, "python", "Instruction.parse_operand", "```python\ndef parse_operand(self, buf):\n        \"\"\" Parses an operand from buf\n\n            :param buf: a buffer\n            :type buf: iterator/generator/string\n        \"\"\"\n        buf = iter(buf)\n        try:\n            operand = 0\n            for _ in range(self.operand_size):\n                operand <<= 8\n                operand |= next(buf)\n            self._operand = operand\n        except StopIteration:\n            raise ParseError(\"Not enough data for decoding\")\n```"], [356, "python", "_BaseFile._updateType", "```python\ndef _updateType(self):\n        \"\"\"Make sure that the class behaves like the data structure that it\n        is, so that we don't get a ListFile trying to represent a dict.\"\"\"\n        data = self._data()\n        # Change type if needed\n        if isinstance(data, dict) and isinstance(self, ListFile):\n            self.__class__ = DictFile\n        elif isinstance(data, list) and isinstance(self, DictFile):\n            self.__class__ = ListFile\n```"], [1, "python", "LsstLatexDoc._parse_documentclass", "```python\ndef _parse_documentclass(self):\n        \"\"\"Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute.\n        \"\"\"\n        command = LatexCommand(\n            'documentclass',\n            {'name': 'options', 'required': False, 'bracket': '['},\n            {'name': 'class_name', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no documentclass')\n            self._document_options = []\n\n        try:\n            content = parsed['options']\n            self._document_options = [opt.strip()\n                                      for opt in content.split(',')]\n        except KeyError:\n            self._logger.warning('lsstdoc has no documentclass options')\n            self._document_options = []\n```"], [1, "python", "reshape_for_broadcasting", "```python\ndef reshape_for_broadcasting(source, target):\n    \"\"\"Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI.\n    \"\"\"\n    dim = len(target.get_shape())\n    shape = ([1] * (dim - 1)) + [-1]\n    return tf.reshape(tf.cast(source, target.dtype), shape)\n```"], [1, "python", "PathFilters.select_by_pattern_in_abspath", "```python\ndef select_by_pattern_in_abspath(self,\n                                     pattern,\n                                     recursive=True,\n                                     case_sensitive=False):\n        \"\"\"\n        Select file path by text pattern in absolute path.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u9009\u62e9\u7edd\u5bf9\u8def\u5f84\u4e2d\u5305\u542b\u6307\u5b9a\u5b50\u5b57\u7b26\u4e32\u7684\u6587\u4ef6\u3002\n        \"\"\"\n        if case_sensitive:\n            def filters(p):\n                return pattern in p.abspath\n        else:\n            pattern = pattern.lower()\n\n            def filters(p):\n                return pattern in p.abspath.lower()\n\n        return self.select_file(filters, recursive)\n```"], [282, "python", "Reaction._update_awareness", "```python\ndef _update_awareness(self):\n        \"\"\"Make sure all metabolites and genes that are associated with\n        this reaction are aware of it.\n\n        \"\"\"\n        for x in self._metabolites:\n            x._reaction.add(self)\n        for x in self._genes:\n            x._reaction.add(self)\n```"], [151, "python", "REST.conference_list", "```python\ndef conference_list(self, call_params):\n        \"\"\"REST Conference List Helper\n        \"\"\"\n        path = '/' + self.api_version + '/ConferenceList/'\n        method = 'POST'\n        return self.request(path, method, call_params)\n```"], [1, "python", "Timer.stop", "```python\ndef stop(self) -> float:\n        \"\"\"\n        Stop the timer\n\n        Returns:\n            The time the timer was stopped\n        \"\"\"\n        self.stop_time = time.time()\n        return self.stop_time - self.start_time - self.offset\n```"], [930, "python", "X86Cpu.ADD", "```python\ndef ADD(cpu, dest, src):\n        \"\"\"\n        Add.\n\n        Adds the first operand (destination operand) and the second operand (source operand)\n        and stores the result in the destination operand. When an immediate value is used as\n        an operand, it is sign-extended to the length of the destination operand format.\n        The ADD instruction does not distinguish between signed or unsigned operands. Instead,\n        the processor evaluates the result for both data types and sets the OF and CF flags to\n        indicate a carry in the signed or unsigned result, respectively. The SF flag indicates\n        the sign of the signed result::\n\n                DEST  =  DEST + SRC;\n\n        :param cpu: current CPU.\n        :param dest: destination operand.\n        :param src: source operand.\n        \"\"\"\n        cpu._ADD(dest, src, carry=False)\n```"], [266, "python", "DebugSatchel.list_server_specs", "```python\ndef list_server_specs(self, cpu=1, memory=1, hdd=1):\n        \"\"\"\n        Displays a list of common servers characteristics, like number\n        of CPU cores, amount of memory and hard drive capacity.\n        \"\"\"\n        r = self.local_renderer\n\n        cpu = int(cpu)\n        memory = int(memory)\n        hdd = int(hdd)\n\n        # CPU\n        if cpu:\n            cmd = 'cat /proc/cpuinfo | grep -i \"model name\"'\n            ret = r.run(cmd)\n            matches = map(str.strip, re.findall(r'model name\\s+:\\s*([^\\n]+)', ret, re.DOTALL|re.I))\n            cores = {}\n            for match in matches:\n                cores.setdefault(match, 0)\n                cores[match] += 1\n\n        # Memory\n        if memory:\n            cmd = 'dmidecode --type 17'\n            ret = r.sudo(cmd)\n            #print repr(ret)\n            matches = re.findall(r'Memory\\s+Device\\r\\n(.*?)(?:\\r\\n\\r\\n|$)', ret, flags=re.DOTALL|re.I)\n            #print len(matches)\n            #print matches[0]\n            memory_slot_dicts = []\n            for match in matches:\n                attrs = dict([(_a.strip(), _b.strip()) for _a, _b in re.findall(r'^([^:]+):\\s+(.*)$', match, flags=re.MULTILINE)])\n                #print attrs\n                memory_slot_dicts.append(attrs)\n            total_memory_gb = 0\n            total_slots_filled = 0\n            total_slots = len(memory_slot_dicts)\n            memory_types = set()\n            memory_forms = set()\n            memory_speeds = set()\n            for memory_dict in memory_slot_dicts:\n                try:\n                    size = int(round(float(re.findall(r'([0-9]+)\\s+MB', memory_dict['Size'])[0])/1024.))\n                    #print size\n                    total_memory_gb += size\n                    total_slots_filled += 1\n                except IndexError:\n                    pass\n                _v = memory_dict['Type']\n                if _v != 'Unknown':\n                    memory_types.add(_v)\n                _v = memory_dict['Form Factor']\n                if _v != 'Unknown':\n                    memory_forms.add(_v)\n                #_v = memory_dict['Speed']\n                #if _v != 'Unknown':\n                    #memory_speeds.add(_v)\n\n        # Storage\n        if hdd:\n            #cmd = 'ls /dev/*d* | grep \"/dev/[a-z]+d[a-z]$\"'\n            cmd = 'find /dev -maxdepth 1 | grep -E \"/dev/[a-z]+d[a-z]$\"'\n            devices = map(str.strip, r.run(cmd).split('\\n'))\n            total_drives = len(devices)\n            total_physical_storage_gb = 0\n            total_logical_storage_gb = 0\n            drive_transports = set()\n            for device in devices:\n                #cmd = 'udisks --show-info %s |grep -i \"  size:\"' % (device)\n                cmd = 'udisksctl info -b %s |grep -i \"  size:\"' % (device)\n                ret = r.run(cmd)\n                size_bytes = float(re.findall(r'size:\\s*([0-9]+)', ret, flags=re.I)[0].strip())\n                size_gb = int(round(size_bytes/1024/1024/1024))\n                #print device, size_gb\n                total_physical_storage_gb += size_gb\n\n                with self.settings(warn_only=True):\n                    cmd = 'hdparm -I %s|grep -i \"Transport:\"' % device\n                    ret = self.sudo(cmd)\n                    if ret and not ret.return_code:\n                        drive_transports.add(ret.split('Transport:')[-1].strip())\n\n            cmd = \"df | grep '^/dev/[mhs]d*' | awk '{{s+=$2}} END {{print s/1048576}}'\"\n            ret = r.run(cmd)\n            total_logical_storage_gb = float(ret)\n\n        if cpu:\n            print('-'*80)\n            print('CPU')\n            print('-'*80)\n            type_str = ', '.join(['%s x %i' % (_type, _count) for _type, _count in cores.items()])\n            print('Cores: %i' % sum(cores.values()))\n            print('Types: %s' % type_str)\n\n        if memory:\n            print('-'*80)\n            print('MEMORY')\n            print('-'*80)\n            print('Total: %s GB' % total_memory_gb)\n            print('Type: %s' % list_to_str_or_unknown(memory_types))\n            print('Form: %s' % list_to_str_or_unknown(memory_forms))\n            print('Speed: %s' % list_to_str_or_unknown(memory_speeds))\n            print('Slots: %i (%i filled, %i empty)' % (total_slots, total_slots_filled, total_slots - total_slots_filled))\n\n        if hdd:\n            print('-'*80)\n            print('STORAGE')\n            print('-'*80)\n            print('Total physical drives: %i' % total_drives)\n            print('Total physical storage: %s GB' % total_physical_storage_gb)\n            print('Total logical storage: %s GB' % total_logical_storage_gb)\n            print('Types: %s' % list_to_str_or_unknown(drive_transports))\n```"], [90, "python", "Controller.start", "```python\ndef start(self):\n        \"\"\"Start the controller.\"\"\"\n\n        if self.mode == \"manual\":\n            return\n\n        if self.ipython_dir != '~/.ipython':\n            self.ipython_dir = os.path.abspath(os.path.expanduser(self.ipython_dir))\n\n        if self.log:\n            stdout = open(os.path.join(self.ipython_dir, \"{0}.controller.out\".format(self.profile)), 'w')\n            stderr = open(os.path.join(self.ipython_dir, \"{0}.controller.err\".format(self.profile)), 'w')\n        else:\n            stdout = open(os.devnull, 'w')\n            stderr = open(os.devnull, 'w')\n\n        try:\n            opts = [\n                'ipcontroller',\n                '' if self.ipython_dir == '~/.ipython' else '--ipython-dir={}'.format(self.ipython_dir),\n                self.interfaces if self.interfaces is not None else '--ip=*',\n                '' if self.profile == 'default' else '--profile={0}'.format(self.profile),\n                '--reuse' if self.reuse else '',\n                '--location={}'.format(self.public_ip) if self.public_ip else '',\n                '--port={}'.format(self.port) if self.port is not None else ''\n            ]\n            if self.port_range is not None:\n                opts += [\n                    '--HubFactory.hb={0},{1}'.format(self.hb_ping, self.hb_pong),\n                    '--HubFactory.control={0},{1}'.format(self.control_client, self.control_engine),\n                    '--HubFactory.mux={0},{1}'.format(self.mux_client, self.mux_engine),\n                    '--HubFactory.task={0},{1}'.format(self.task_client, self.task_engine)\n                ]\n            logger.debug(\"Starting ipcontroller with '{}'\".format(' '.join([str(x) for x in opts])))\n            self.proc = subprocess.Popen(opts, stdout=stdout, stderr=stderr, preexec_fn=os.setsid)\n        except FileNotFoundError:\n            msg = \"Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env\"\n            logger.error(msg)\n            raise ControllerError(msg)\n        except Exception as e:\n            msg = \"IPPController failed to start: {0}\".format(e)\n            logger.error(msg)\n            raise ControllerError(msg)\n```"], [3, "python", "Resource.launch", "```python\ndef launch(self, workflow_job_template=None, monitor=False, wait=False,\n               timeout=None, extra_vars=None, **kwargs):\n        \"\"\"Launch a new workflow job based on a workflow job template.\n\n        Creates a new workflow job in Ansible Tower, starts it, and\n        returns back an ID in order for its status to be monitored.\n\n        =====API DOCS=====\n        Launch a new workflow job based on a workflow job template.\n\n        :param workflow_job_template: Primary key or name of the workflow job template to launch new job.\n        :type workflow_job_template: str\n        :param monitor: Flag that if set, immediately calls ``monitor`` on the newly launched workflow job rather\n                        than exiting with a success.\n        :type monitor: bool\n        :param wait: Flag that if set, monitor the status of the workflow job, but do not print while job is\n                     in progress.\n        :type wait: bool\n        :param timeout: If provided with ``monitor`` flag set, this attempt will time out after the given number\n                        of seconds.\n        :type timeout: int\n        :param extra_vars: yaml formatted texts that contains extra variables to pass on.\n        :type extra_vars: array of strings\n        :param `**kwargs`: Fields needed to create and launch a workflow job.\n        :returns: Result of subsequent ``monitor`` call if ``monitor`` flag is on; Result of subsequent ``wait``\n                  call if ``wait`` flag is on; loaded JSON output of the job launch if none of the two flags are on.\n        :rtype: dict\n\n        =====API DOCS=====\n        \"\"\"\n        if extra_vars is not None and len(extra_vars) > 0:\n            kwargs['extra_vars'] = parser.process_extra_vars(extra_vars)\n\n        debug.log('Launching the workflow job.', header='details')\n        self._pop_none(kwargs)\n        post_response = client.post('workflow_job_templates/{0}/launch/'.format(\n            workflow_job_template), data=kwargs).json()\n\n        workflow_job_id = post_response['id']\n        post_response['changed'] = True\n\n        if monitor:\n            return self.monitor(workflow_job_id, timeout=timeout)\n        elif wait:\n            return self.wait(workflow_job_id, timeout=timeout)\n\n        return post_response\n```"], [3, "python", "color_scale", "```python\ndef color_scale(color, level):\n    \"\"\"\n    Scale RGB tuple by level, 0 - 256\n    \"\"\"\n    return tuple([int(i * level) >> 8 for i in list(color)])\n```"], [115, "python", "EVM.CALLDATALOAD", "```python\ndef CALLDATALOAD(self, offset):\n        \"\"\"Get input data of current environment\"\"\"\n\n        if issymbolic(offset):\n            if solver.can_be_true(self._constraints, offset == self._used_calldata_size):\n                self.constraints.add(offset == self._used_calldata_size)\n            raise ConcretizeArgument(1, policy='SAMPLED')\n\n        self._use_calldata(offset, 32)\n\n        data_length = len(self.data)\n\n        bytes = []\n        for i in range(32):\n            try:\n                c = Operators.ITEBV(8, offset + i < data_length, self.data[offset + i], 0)\n            except IndexError:\n                # offset + i is concrete and outside data\n                c = 0\n\n            bytes.append(c)\n        return Operators.CONCAT(256, *bytes)\n```"], [2, "python", "cornice_enable_openapi_view", "```python\ndef cornice_enable_openapi_view(\n        config,\n        api_path='/api-explorer/swagger.json',\n        permission=NO_PERMISSION_REQUIRED,\n        route_factory=None, **kwargs):\n    \"\"\"\n    :param config:\n        Pyramid configurator object\n    :param api_path:\n        where to expose swagger JSON definition view\n    :param permission:\n        pyramid permission for those views\n    :param route_factory:\n        factory for context object for those routes\n    :param kwargs:\n        kwargs that will be passed to CorniceSwagger's `generate()`\n\n    This registers and configures the view that serves api definitions\n    \"\"\"\n    config.registry.settings['cornice_swagger.spec_kwargs'] = kwargs\n    config.add_route('cornice_swagger.open_api_path', api_path,\n                     factory=route_factory)\n    config.add_view('cornice_swagger.views.open_api_json_view',\n                    renderer='json', permission=permission,\n                    route_name='cornice_swagger.open_api_path')\n```"], [123, "python", "tui.shorthelp", "```python\ndef shorthelp(self, width=0):\n        \"\"\"Return brief help containing Title and usage instructions.\n        ARGS:\n        width = 0 <int>:\n            Maximum allowed page width. 0 means use default from\n            self.iMaxHelpWidth.\n\n        \"\"\"\n        out = []\n        out.append(self._wrap(self.docs['title'], width=width))\n        if self.docs['description']:\n            out.append(self._wrap(self.docs['description'], indent=2, width=width))\n        out.append('')\n        out.append(self._wrapusage(width=width))\n        out.append('')\n        return '\\n'.join(out)\n```"], [148, "python", "RecurringScheduleComponent.daily_periods", "```python\ndef daily_periods(self, range_start=datetime.date.min, range_end=datetime.date.max, exclude_dates=tuple()):\n        \"\"\"Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end.\"\"\"\n        tz = self.timezone\n        period = self.period\n        weekdays = self.weekdays\n\n        current_date = max(range_start, self.start_date)\n        end_date = range_end\n        if self.end_date:\n            end_date = min(end_date, self.end_date)\n\n        while current_date <= end_date:\n            if current_date.weekday() in weekdays and current_date not in exclude_dates:\n                yield Period(\n                    tz.localize(datetime.datetime.combine(current_date, period.start)),\n                    tz.localize(datetime.datetime.combine(current_date, period.end))\n                )\n            current_date += datetime.timedelta(days=1)\n```"], [1, "python", "Connection.get_headers", "```python\ndef get_headers(self):\n        \"\"\" Get headers.\n\n        Returns:\n            tuple: Headers\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"kFlame 1.0\"\n        }\n\n        password_url = self._get_password_url()\n        if password_url and password_url in self._settings[\"authorizations\"]:\n            headers[\"Authorization\"] = self._settings[\"authorizations\"][password_url]\n\n        return headers\n```"], [530, "python", "CQHttp.set_group_whole_ban", "```python\ndef set_group_whole_ban(self, *, group_id, enable=True):\n        \"\"\"\n        \u7fa4\u7ec4\u5168\u5458\u7981\u8a00\n\n        ------------\n\n        :param int group_id: \u7fa4\u53f7\n        :param bool enable: \u662f\u5426\u7981\u8a00\n        :return: None\n        :rtype: None\n        \"\"\"\n        return super().__getattr__('set_group_whole_ban') \\\n            (group_id=group_id, enable=enable)\n```"], [307, "python", "MemSizeLRUCache.delete", "```python\ndef delete(self, key):\n        \"\"\"\n        >>> c = MemSizeLRUCache()\n        >>> c.put(1, 1)\n        >>> c.mem()\n        24\n        >>> c.delete(1)\n        >>> c.mem()\n        0\n        \"\"\"\n        (_value, mem) = LRUCache.get(self, key)\n        self._mem -= mem\n        LRUCache.delete(self, key)\n```"], [114, "python", "phase_identification_parameter_phase", "```python\ndef phase_identification_parameter_phase(d2P_dVdT, V=None, dP_dT=None, dP_dV=None, d2P_dV2=None):\n    r'''Uses the Phase Identification Parameter concept developed in [1]_ and \n    [2]_ to determine if a chemical is a solid, liquid, or vapor given the \n    appropriate thermodynamic conditions.\n\n    The criteria for liquid is PIP > 1; for vapor, PIP <= 1.\n\n    For solids, PIP(solid) is defined to be d2P_dVdT. If it is larger than 0, \n    the species is a solid. It is less than 0 for all liquids and gases.\n\n    Parameters\n    ----------\n    d2P_dVdT : float\n        Second derivative of `P` with respect to both `V` and `T`, [Pa*mol/m^3/K]\n    V : float, optional\n        Molar volume at `T` and `P`, [m^3/mol]\n    dP_dT : float, optional\n        Derivative of `P` with respect to `T`, [Pa/K]\n    dP_dV : float, optional\n        Derivative of `P` with respect to `V`, [Pa*mol/m^3]\n    d2P_dV2 : float, optionsl\n        Second derivative of `P` with respect to `V`, [Pa*mol^2/m^6]\n\n    Returns\n    -------\n    phase : str\n        Either 's', 'l' or 'g'\n    \n    Notes\n    -----\n    The criteria for being a solid phase is checked first, which only\n    requires d2P_dVdT. All other inputs are optional for this reason.\n    However, an exception will be raised if the other inputs become \n    needed to determine if a species is a liquid or a gas.\n        \n    Examples\n    --------\n    Calculated for hexane from the PR EOS at 299 K and 1 MPa (liquid):\n    \n    >>> phase_identification_parameter_phase(-20518995218.2, 0.000130229900874, \n    ... 582169.397484, -3.66431747236e+12, 4.48067893805e+17)\n    'l'\n\n    References\n    ----------\n    .. [1] Venkatarathnam, G., and L. R. Oellrich. \"Identification of the Phase\n       of a Fluid Using Partial Derivatives of Pressure, Volume, and \n       Temperature without Reference to Saturation Properties: Applications in \n       Phase Equilibria Calculations.\" Fluid Phase Equilibria 301, no. 2 \n       (February 25, 2011): 225-33. doi:10.1016/j.fluid.2010.12.001.\n    .. [2] Jayanti, Pranava Chaitanya, and G. Venkatarathnam. \"Identification\n       of the Phase of a Substance from the Derivatives of Pressure, Volume and\n       Temperature, without Prior Knowledge of Saturation Properties: Extension\n       to Solid Phase.\" Fluid Phase Equilibria 425 (October 15, 2016): 269-277.\n       doi:10.1016/j.fluid.2016.06.001.\n    '''\n    if d2P_dVdT > 0:\n        return 's'\n    else:\n        PIP = phase_identification_parameter(V=V, dP_dT=dP_dT, dP_dV=dP_dV, \n                                             d2P_dV2=d2P_dV2, d2P_dVdT=d2P_dVdT)\n        return 'l' if PIP > 1 else 'g'\n```"], [306, "python", "profileit", "```python\ndef profileit(field='cumulative'):\n    \"\"\"\n    \u6d4b\u8bd5\u51fd\u6570\u8fd0\u884c\u6d88\u8017\u60c5\u51b5\n\n    :param field: \u8f93\u51fa\u5185\u5bb9\u6392\u5e8f\u65b9\u5f0f\u3002\n        \u53ef\u9009\u53c2\u6570\u4e3a \"stdname\", \"calls\", \"time\", \"cumulative\"\n    \"\"\"\n    def wrapper(func):\n        @wraps(func)\n        def inner(*args, **kwargs):\n            pro = Profile()\n            pro.runcall(func, *args, **kwargs)\n            stats = Stats(pro)\n            stats.strip_dirs()\n            stats.sort_stats(field)\n            print(\"Profile for {}()\".format(func.__name__))\n            stats.print_stats()\n            stats.print_callers()\n        return inner\n    return wrapper\n```"], [51, "python", "LoadBalancer.create", "```python\ndef create(self, *args, **kwargs):\n        \"\"\"\n        Creates a new LoadBalancer.\n\n        Note: Every argument and parameter given to this method will be\n        assigned to the object.\n\n        Args:\n            name (str): The Load Balancer's name\n            region (str): The slug identifier for a DigitalOcean region\n            algorithm (str, optional): The load balancing algorithm to be\n                used. Currently, it must be either \"round_robin\" or\n                \"least_connections\"\n            forwarding_rules (obj:`list`): A list of `ForwrdingRules` objects\n            health_check (obj, optional): A `HealthCheck` object\n            sticky_sessions (obj, optional): A `StickySessions` object\n            redirect_http_to_https (bool, optional): A boolean indicating\n                whether HTTP requests to the Load Balancer should be\n                redirected to HTTPS\n            droplet_ids (obj:`list` of `int`): A list of IDs representing\n                Droplets to be added to the Load Balancer (mutually\n                exclusive with 'tag')\n            tag (str): A string representing a DigitalOcean Droplet tag\n                (mutually exclusive with 'droplet_ids')\n        \"\"\"\n        rules_dict = [rule.__dict__ for rule in self.forwarding_rules]\n\n        params = {'name': self.name, 'region': self.region,\n                  'forwarding_rules': rules_dict,\n                  'redirect_http_to_https': self.redirect_http_to_https}\n\n        if self.droplet_ids and self.tag:\n            raise ValueError('droplet_ids and tag are mutually exclusive args')\n        elif self.tag:\n            params['tag'] = self.tag\n        else:\n            params['droplet_ids'] = self.droplet_ids\n\n        if self.algorithm:\n            params['algorithm'] = self.algorithm\n        if self.health_check:\n            params['health_check'] = self.health_check.__dict__\n        if self.sticky_sessions:\n            params['sticky_sessions'] = self.sticky_sessions.__dict__\n\n        data = self.get_data('load_balancers/', type=POST, params=params)\n\n        if data:\n            self.id = data['load_balancer']['id']\n            self.ip = data['load_balancer']['ip']\n            self.algorithm = data['load_balancer']['algorithm']\n            self.health_check = HealthCheck(\n                **data['load_balancer']['health_check'])\n            self.sticky_sessions = StickySesions(\n                **data['load_balancer']['sticky_sessions'])\n            self.droplet_ids = data['load_balancer']['droplet_ids']\n            self.status = data['load_balancer']['status']\n            self.created_at = data['load_balancer']['created_at']\n\n        return self\n```"], [39, "python", "PostgresDB.rename", "```python\ndef rename(self, from_name, to_name):\n        \"\"\"Renames an existing database.\"\"\"\n        log.info('renaming database from %s to %s' % (from_name, to_name))\n        self._run_stmt('alter database %s rename to %s' % (from_name, to_name))\n```"], [76, "python", "XmlElement.siblings", "```python\ndef siblings(self, name=None):\n        \"\"\"\n        Yields all siblings of this node (not including the node itself).\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"\n        if self.parent and self.index:\n            for c in self.parent._children:\n                if c.index != self.index and (name is None or name == c.tagname):\n                    yield c\n```"], [99, "python", "KNNAnomalyClassifierRegion._recomputeRecordFromKNN", "```python\ndef _recomputeRecordFromKNN(self, record):\n    \"\"\"\n    returns the classified labeling of record\n    \"\"\"\n    inputs = {\n      \"categoryIn\": [None],\n      \"bottomUpIn\": self._getStateAnomalyVector(record),\n    }\n\n    outputs = {\"categoriesOut\": numpy.zeros((1,)),\n               \"bestPrototypeIndices\":numpy.zeros((1,)),\n               \"categoryProbabilitiesOut\":numpy.zeros((1,))}\n\n    # Only use points before record to classify and after the wait period.\n    classifier_indexes = numpy.array(\n        self._knnclassifier.getParameter('categoryRecencyList'))\n    valid_idx = numpy.where(\n        (classifier_indexes >= self.getParameter('trainRecords')) &\n        (classifier_indexes < record.ROWID)\n      )[0].tolist()\n\n    if len(valid_idx) == 0:\n      return None\n\n    self._knnclassifier.setParameter('inferenceMode', None, True)\n    self._knnclassifier.setParameter('learningMode', None, False)\n    self._knnclassifier.compute(inputs, outputs)\n    self._knnclassifier.setParameter('learningMode', None, True)\n\n    classifier_distances = self._knnclassifier.getLatestDistances()\n    valid_distances = classifier_distances[valid_idx]\n    if valid_distances.min() <= self._classificationMaxDist:\n      classifier_indexes_prev = classifier_indexes[valid_idx]\n      rowID = classifier_indexes_prev[valid_distances.argmin()]\n      indexID = numpy.where(classifier_indexes == rowID)[0][0]\n      category = self._knnclassifier.getCategoryList()[indexID]\n      return category\n    return None\n```"], [368, "python", "timestamp", "```python\ndef timestamp(t = None, forfilename=False):\n    \"\"\"Returns a human-readable timestamp given a Unix timestamp 't' or\n    for the current time. The Unix timestamp is the number of seconds since\n    start of epoch (1970-01-01 00:00:00).\n    When forfilename is True, then spaces and semicolons are replace with\n    hyphens. The returned string is usable as a (part of a) filename. \"\"\"\n\n    datetimesep = ' '\n    timesep     = ':'\n    if forfilename:\n        datetimesep = '-'\n        timesep     = '-'\n\n    return time.strftime('%Y-%m-%d' + datetimesep +\n                         '%H' + timesep + '%M' + timesep + '%S',\n                         time.localtime(t))\n```"], [122, "python", "DeploySatchel.lock", "```python\ndef lock(self):\n        \"\"\"\n        Marks the remote server as currently being deployed to.\n        \"\"\"\n        self.init()\n        r = self.local_renderer\n        if self.file_exists(r.env.lockfile_path):\n            raise exceptions.AbortDeployment('Lock file %s exists. Perhaps another deployment is currently underway?' % r.env.lockfile_path)\n        else:\n            self.vprint('Locking %s.' % r.env.lockfile_path)\n            r.env.hostname = socket.gethostname()\n            r.run_or_local('echo \"{hostname}\" > {lockfile_path}')\n```"], [192, "python", "md_dimension_info", "```python\ndef md_dimension_info(name, node):\n    \"\"\"Extract metadata Dimension Info from an xml node\"\"\"\n    def _get_value(child_name):\n        return getattr(node.find(child_name), 'text', None)\n\n    resolution = _get_value('resolution')\n    defaultValue = node.find(\"defaultValue\")\n    strategy = defaultValue.find(\"strategy\") if defaultValue is not None else None\n    strategy = strategy.text if strategy is not None else None\n    return DimensionInfo(\n        name,\n        _get_value('enabled') == 'true',\n        _get_value('presentation'),\n        int(resolution) if resolution else None,\n        _get_value('units'),\n        _get_value('unitSymbol'),\n        strategy,\n        _get_value('attribute'),\n        _get_value('endAttribute'),\n        _get_value('referenceValue'),\n        _get_value('nearestMatchEnabled')\n    )\n```"], [35, "python", "TicTacToe.k_in_row", "```python\ndef k_in_row(self, board, move, player, (delta_x, delta_y)):\n        \"Return true if there is a line through move on board for player.\"\n        x, y = move\n        n = 0 # n is number of moves in row\n        while board.get((x, y)) == player:\n            n += 1\n            x, y = x + delta_x, y + delta_y\n        x, y = move\n        while board.get((x, y)) == player:\n            n += 1\n            x, y = x - delta_x, y - delta_y\n        n -= 1 # Because we counted move itself twice\n        return n >= self.k\n```"], [2, "python", "keypoint_random_resize", "```python\ndef keypoint_random_resize(image, annos, mask=None, zoom_range=(0.8, 1.2)):\n    \"\"\"Randomly resize an image and corresponding keypoints.\n    The height and width of image will be changed independently, so the scale will be changed.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    zoom_range : tuple of two floats\n        The minimum and maximum factor to zoom in or out, e.g (0.5, 1) means zoom out 1~2 times.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    \"\"\"\n    height = image.shape[0]\n    width = image.shape[1]\n    _min, _max = zoom_range\n    scalew = np.random.uniform(_min, _max)\n    scaleh = np.random.uniform(_min, _max)\n\n    neww = int(width * scalew)\n    newh = int(height * scaleh)\n\n    dst = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n    if mask is not None:\n        mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA)\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in annos:  # TODO : speed up with affine transform\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n    if mask is not None:\n        return dst, adjust_joint_list, mask\n    else:\n        return dst, adjust_joint_list, None\n```"], [1, "python", "numpy_array_2d_to_fits", "```python\ndef numpy_array_2d_to_fits(array_2d, file_path, overwrite=False):\n    \"\"\"Write a 2D NumPy array to a .fits file.\n\n    Before outputting a NumPy array, the array is flipped upside-down using np.flipud. This is so that the arrays \\\n    appear the same orientation as .fits files loaded in DS9.\n\n    Parameters\n    ----------\n    array_2d : ndarray\n        The 2D array that is written to fits.\n    file_path : str\n        The full path of the file that is output, including the file name and '.fits' extension.\n    overwrite : bool\n        If True and a file already exists with the input file_path the .fits file is overwritten. If False, an error \\\n        will be raised.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    array_2d = np.ones((5,5))\n    numpy_array_to_fits(array=array_2d, file_path='/path/to/file/filename.fits', overwrite=True)\n    \"\"\"\n    if overwrite and os.path.exists(file_path):\n        os.remove(file_path)\n\n    new_hdr = fits.Header()\n    hdu = fits.PrimaryHDU(np.flipud(array_2d), new_hdr)\n    hdu.writeto(file_path)\n```"], [5, "python", "StreamSASLHandler._process_sasl_challenge", "```python\ndef _process_sasl_challenge(self, stream, element):\n        \"\"\"Process incoming <sasl:challenge/> element.\n\n        [initiating entity only]\n        \"\"\"\n        if not self.authenticator:\n            logger.debug(\"Unexpected SASL challenge\")\n            return False\n\n        content = element.text.encode(\"us-ascii\")\n        ret = self.authenticator.challenge(a2b_base64(content))\n        if isinstance(ret, sasl.Response):\n            element = ElementTree.Element(RESPONSE_TAG)\n            element.text = ret.encode()\n        else:\n            element = ElementTree.Element(ABORT_TAG)\n\n        stream.write_element(element)\n\n        if isinstance(ret, sasl.Failure):\n            stream.disconnect()\n            raise SASLAuthenticationFailed(\"SASL authentication failed\")\n\n        return True\n```"]]}, "_runtime": 560.426290512085, "_timestamp": 1571435581.5523899, "_step": 32}
{"Examples-Test-python": {"_type": "table", "columns": ["Rank", "Language", "Query", "Code"], "data": [[68, "python", "unpack_4to8", "```python\ndef unpack_4to8(data):\n    \"\"\" Promote 2-bit unisgned data into 8-bit unsigned data.\n\n    Args:\n        data: Numpy array with dtype == uint8\n\n    Notes:\n        # The process is this:\n        # ABCDEFGH [Bits of one 4+4-bit value]\n        # 00000000ABCDEFGH [astype(uint16)]\n        # 0000ABCDEFGH0000 [<< 4]\n        # 0000ABCDXXXXEFGH [bitwise 'or' of previous two lines]\n        # 0000111100001111 [0x0F0F]\n        # 0000ABCD0000EFGH [bitwise 'and' of previous two lines]\n        # ABCD0000EFGH0000 [<< 4]\n        # which effectively pads the two 4-bit values with zeros on the right\n        # Note: This technique assumes LSB-first ordering\n    \"\"\"\n\n    tmpdata = data.astype(np.int16)  # np.empty(upshape, dtype=np.int16)\n    tmpdata = (tmpdata | (tmpdata << 4)) & 0x0F0F\n    # tmpdata = tmpdata << 4 # Shift into high bits to avoid needing to sign extend\n    updata = tmpdata.byteswap()\n    return updata.view(data.dtype)\n```"], [73, "python", "parse_mim2gene", "```python\ndef parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry\n```"], [211, "python", "CreditNoteController.cancellation_fee", "```python\ndef cancellation_fee(self, percentage):\n        ''' Generates an invoice with a cancellation fee, and applies\n        credit to the invoice.\n\n        percentage (Decimal): The percentage of the credit note to turn into\n        a cancellation fee. Must be 0 <= percentage <= 100.\n        '''\n\n        # Local import to fix import cycles. Can we do better?\n        from .invoice import InvoiceController\n\n        assert(percentage >= 0 and percentage <= 100)\n\n        cancellation_fee = self.credit_note.value * percentage / 100\n        due = datetime.timedelta(days=1)\n        item = [(\"Cancellation fee\", cancellation_fee)]\n        invoice = InvoiceController.manual_invoice(\n            self.credit_note.invoice.user, due, item\n        )\n\n        if not invoice.is_paid:\n            self.apply_to_invoice(invoice)\n\n        return InvoiceController(invoice)\n```"], [28, "python", "update_panel", "```python\ndef update_panel(store, panel_name, csv_lines, option):\n    \"\"\"Update an existing gene panel with genes.\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        panel_name(str)\n        csv_lines(iterable(str)): Stream with genes\n        option(str): 'add' or 'replace'\n\n    Returns:\n        panel_obj(dict)\n    \"\"\"\n    new_genes= []\n    panel_obj = store.gene_panel(panel_name)\n    if panel_obj is None:\n        return None\n    try:\n        new_genes = parse_genes(csv_lines) # a list of gene dictionaries containing gene info\n    except SyntaxError as error:\n        flash(error.args[0], 'danger')\n        return None\n\n    # if existing genes are to be replaced by those in csv_lines\n    if option == 'replace':\n        # all existing genes should be deleted\n        for gene in panel_obj['genes']:\n            #create extra key to use in pending actions:\n            gene['hgnc_symbol'] = gene['symbol']\n            store.add_pending(panel_obj, gene, action='delete', info=None)\n\n    for new_gene in new_genes:\n        if not new_gene['hgnc_id']:\n            flash(\"gene missing hgnc id: {}\".format(new_gene['hgnc_symbol']),'danger')\n            continue\n        gene_obj = store.hgnc_gene(new_gene['hgnc_id'])\n        if gene_obj is None:\n            flash(\"gene not found: {} - {}\".format(new_gene['hgnc_id'], new_gene['hgnc_symbol']),'danger')\n            continue\n        if new_gene['hgnc_symbol'] and gene_obj['hgnc_symbol'] != new_gene['hgnc_symbol']:\n            flash(\"symbol mis-match: {0} | {1}\".format(\n                gene_obj['hgnc_symbol'], new_gene['hgnc_symbol']), 'warning')\n\n        info_data = {\n            'disease_associated_transcripts': new_gene['transcripts'],\n            'reduced_penetrance': new_gene['reduced_penetrance'],\n            'mosaicism': new_gene['mosaicism'],\n            'inheritance_models': new_gene['inheritance_models'],\n            'database_entry_version': new_gene['database_entry_version'],\n        }\n        if option == 'replace': # there will be no existing genes for sure, because we're replacing them all\n            action = 'add'\n        else: # add option. Add if genes is not existing. otherwise edit it\n            existing_genes = {gene['hgnc_id'] for gene in panel_obj['genes']}\n            action = 'edit' if gene_obj['hgnc_id'] in existing_genes else 'add'\n        store.add_pending(panel_obj, gene_obj, action=action, info=info_data)\n\n    return panel_obj\n```"], [26, "python", "MinHashLSHForest.index", "```python\ndef index(self):\n        '''\n        Index all the keys added so far and make them searchable.\n        '''\n        for i, hashtable in enumerate(self.hashtables):\n            self.sorted_hashtables[i] = [H for H in hashtable.keys()]\n            self.sorted_hashtables[i].sort()\n```"], [3, "python", "Eaf.generate_ts_id", "```python\ndef generate_ts_id(self, time=None):\n        \"\"\"Generate the next timeslot id, this function is mainly used\n        internally\n\n        :param int time: Initial time to assign to the timeslot.\n        :raises ValueError: If the time is negative.\n        \"\"\"\n        if time and time < 0:\n            raise ValueError('Time is negative...')\n        if not self.maxts:\n            valid_ts = [int(''.join(filter(str.isdigit, a)))\n                        for a in self.timeslots]\n            self.maxts = max(valid_ts + [1])+1\n        else:\n            self.maxts += 1\n        ts = 'ts{:d}'.format(self.maxts)\n        self.timeslots[ts] = time\n        return ts\n```"], [10, "python", "BandwidthLimitedStream.read", "```python\ndef read(self, amount):\n        \"\"\"Read a specified amount\n\n        Reads will only be throttled if bandwidth limiting is enabled.\n        \"\"\"\n        if not self._bandwidth_limiting_enabled:\n            return self._fileobj.read(amount)\n\n        # We do not want to be calling consume on every read as the read\n        # amounts can be small causing the lock of the leaky bucket to\n        # introduce noticeable overhead. So instead we keep track of\n        # how many bytes we have seen and only call consume once we pass a\n        # certain threshold.\n        self._bytes_seen += amount\n        if self._bytes_seen < self._bytes_threshold:\n            return self._fileobj.read(amount)\n\n        self._consume_through_leaky_bucket()\n        return self._fileobj.read(amount)\n```"], [33, "python", "Meter.extractMonthTariff", "```python\ndef extractMonthTariff(self, month):\n        \"\"\" Extract the tariff for a single month from the meter object buffer.\n\n        Args:\n            month (int):  A :class:`~ekmmeters.Months` value or range(Extents.Months).\n\n        Returns:\n            tuple: The eight tariff period totals for month. The return tuple breaks out as follows:\n\n            ================= ======================================\n            kWh_Tariff_1      kWh for tariff period 1 over month.\n            kWh_Tariff_2      kWh for tariff period 2 over month\n            kWh_Tariff_3      kWh for tariff period 3 over month\n            kWh_Tariff_4      kWh for tariff period 4 over month\n            kWh_Tot           Total kWh over requested month\n            Rev_kWh_Tariff_1  Rev kWh for tariff period 1 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 2 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 3 over month\n            Rev_kWh_Tariff_4  Rev kWh for tariff period 4 over month\n            Rev_kWh_Tot       Total Rev kWh over requested month\n            ================= ======================================\n\n        \"\"\"\n        ret = namedtuple(\"ret\", [\"Month\", Field.kWh_Tariff_1, Field.kWh_Tariff_2, Field.kWh_Tariff_3,\n                         Field.kWh_Tariff_4, Field.kWh_Tot, Field.Rev_kWh_Tariff_1,\n                         Field.Rev_kWh_Tariff_2, Field.Rev_kWh_Tariff_3,\n                         Field.Rev_kWh_Tariff_4, Field.Rev_kWh_Tot])\n        month += 1\n        ret.Month = str(month)\n        if (month < 1) or (month > Extents.Months):\n            ret.kWh_Tariff_1 = ret.kWh_Tariff_2 = ret.kWh_Tariff_3 = ret.kWh_Tariff_4 = str(0)\n            ret.Rev_kWh_Tariff_1 = ret.Rev_kWh_Tariff_2 = ret.Rev_kWh_Tariff_3 = ret.Rev_kWh_Tariff_4 = str(0)\n            ret.kWh_Tot = ret.Rev_kWh_Tot = str(0)\n            ekm_log(\"Out of range(Extents.Months) month = \" + str(month))\n            return ret\n\n        base_str = \"Month_\" + str(month) + \"_\"\n        ret.kWh_Tariff_1 = self.m_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.kWh_Tariff_2 = self.m_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.kWh_Tariff_3 = self.m_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.kWh_Tariff_4 = self.m_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.kWh_Tot = self.m_mons[base_str + \"Tot\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_1 = self.m_rev_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_2 = self.m_rev_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_3 = self.m_rev_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_4 = self.m_rev_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.Rev_kWh_Tot = self.m_rev_mons[base_str + \"Tot\"][MeterData.StringValue]\n        return ret\n```"], [9, "python", "BigQueryCursor.executemany", "```python\ndef executemany(self, operation, seq_of_parameters):\n        \"\"\"\n        Execute a BigQuery query multiple times with different parameters.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param seq_of_parameters: List of dictionary parameters to substitute into the\n            query.\n        :type seq_of_parameters: list\n        \"\"\"\n        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)\n```"], [10, "python", "TypeSystem.convert_from_binary", "```python\ndef convert_from_binary(self, binvalue, type, **kwargs):\n        \"\"\"\n        Convert binary data to type 'type'.\n\n        'type' must have a convert_binary function.  If 'type'\n        supports size checking, the size function is called to ensure\n        that binvalue is the correct size for deserialization\n        \"\"\"\n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)\n```"], [181, "python", "read_data", "```python\ndef read_data(data_file, dataformat, name_mode):\n    \"\"\"\n    Load data_file described by a dataformat dict.\n\n    Parameters\n    ----------\n    data_file : str\n        Path to data file, including extension.\n    dataformat : dict\n        A dataformat dict, see example below.\n    name_mode : str\n        How to identyfy sample names. If 'file_names' uses the\n        input name of the file, stripped of the extension. If\n        'metadata_names' uses the 'name' attribute of the 'meta'\n        sub-dictionary in dataformat. If any other str, uses this\n        str as the sample name.\n\n    Example\n    -------\n    >>>\n    {'genfromtext_args': {'delimiter': ',',\n                          'skip_header': 4},  # passed directly to np.genfromtxt\n     'column_id': {'name_row': 3,  # which row contains the column names\n                   'delimiter': ',',  # delimeter between column names\n                   'timecolumn': 0,  # which column contains the 'time' variable\n                   'pattern': '([A-z]{1,2}[0-9]{1,3})'},  # a regex pattern which captures the column names\n     'meta_regex': {  # a dict of (line_no: ([descriptors], [regexs])) pairs\n                    0: (['path'], '(.*)'),\n                    2: (['date', 'method'],  # MUST include date\n                     '([A-Z][a-z]+ [0-9]+ [0-9]{4}[ ]+[0-9:]+ [amp]+).* ([A-z0-9]+\\.m)')\n                   }\n    }\n\n    Returns\n    -------\n    sample, analytes, data, meta : tuple\n    \"\"\"\n    with open(data_file) as f:\n        lines = f.readlines()\n\n    if 'meta_regex' in dataformat.keys():\n        meta = Bunch()\n        for k, v in dataformat['meta_regex'].items():\n            try:\n                out = re.search(v[-1], lines[int(k)]).groups()\n            except:\n                raise ValueError('Failed reading metadata when applying:\\n  regex: {}\\nto\\n  line: {}'.format(v[-1], lines[int(k)]))\n            for i in np.arange(len(v[0])):\n                meta[v[0][i]] = out[i]\n    else:\n        meta = {}\n\n    # sample name\n    if name_mode == 'file_names':\n        sample = os.path.basename(data_file).split('.')[0]\n    elif name_mode == 'metadata_names':\n        sample = meta['name']\n    else:\n        sample = name_mode\n\n    # column and analyte names\n    columns = np.array(lines[dataformat['column_id']['name_row']].strip().split(\n        dataformat['column_id']['delimiter']))\n    if 'pattern' in dataformat['column_id'].keys():\n        pr = re.compile(dataformat['column_id']['pattern'])\n        analytes = [pr.match(c).groups()[0] for c in columns if pr.match(c)]\n\n    # do any required pre-formatting\n    if 'preformat_replace' in dataformat.keys():\n        with open(data_file) as f:\n            fbuffer = f.read()\n        for k, v in dataformat['preformat_replace'].items():\n            fbuffer = re.sub(k, v, fbuffer)\n        # dead data\n        read_data = np.genfromtxt(BytesIO(fbuffer.encode()),\n                                  **dataformat['genfromtext_args']).T\n    else:\n        # read data\n        read_data = np.genfromtxt(data_file,\n                                  **dataformat['genfromtext_args']).T\n\n    # data dict\n    dind = np.zeros(read_data.shape[0], dtype=bool)\n    for a in analytes:\n        dind[columns == a] = True\n\n    data = Bunch()\n    data['Time'] = read_data[dataformat['column_id']['timecolumn']]\n\n    # deal with time units\n    if 'time_unit' in dataformat['column_id']:\n        if isinstance(dataformat['column_id']['time_unit'], (float, int)):\n            time_mult = dataformat['column_id']['time_unit']\n        elif isinstance(dataformat['column_id']['time_unit'], str):\n            unit_multipliers = {'ms': 1/1000,\n                                'min': 60/1,\n                                's': 1}\n            try:\n                time_mult = unit_multipliers[dataformat['column_id']['time_unit']]\n            except:\n                raise ValueError(\"In dataformat: time_unit must be a number, 'ms', 'min' or 's'\")\n        data['Time'] *= time_mult\n        \n    # convert raw data into counts\n    # TODO: Is this correct? Should actually be per-analyte dwell?\n    # if 'unit' in dataformat:\n    #     if dataformat['unit'] == 'cps':\n    #         tstep = data['Time'][1] - data['Time'][0]\n    #         read_data[dind] *= tstep\n    #     else:\n    #         pass\n    data['rawdata'] = Bunch(zip(analytes, read_data[dind]))\n    data['total_counts'] = np.nansum(read_data[dind], 0)\n\n    return sample, analytes, data, meta\n```"], [11, "python", "DrawElement.top", "```python\ndef top(self):\n        \"\"\" Constructs the top line of the element\"\"\"\n        ret = self.top_format % self.top_connect.center(\n            self.width, self.top_pad)\n        if self.right_fill:\n            ret = ret.ljust(self.right_fill, self.top_pad)\n        if self.left_fill:\n            ret = ret.rjust(self.left_fill, self.top_pad)\n        ret = ret.center(self.layer_width, self.top_bck)\n        return ret\n```"], [733, "python", "GoogleCloudBucketHelper.google_cloud_to_local", "```python\ndef google_cloud_to_local(self, file_name):\n        \"\"\"\n        Checks whether the file specified by file_name is stored in Google Cloud\n        Storage (GCS), if so, downloads the file and saves it locally. The full\n        path of the saved file will be returned. Otherwise the local file_name\n        will be returned immediately.\n\n        :param file_name: The full path of input file.\n        :type file_name: str\n        :return: The full path of local file.\n        :rtype: str\n        \"\"\"\n        if not file_name.startswith('gs://'):\n            return file_name\n\n        # Extracts bucket_id and object_id by first removing 'gs://' prefix and\n        # then split the remaining by path delimiter '/'.\n        path_components = file_name[self.GCS_PREFIX_LENGTH:].split('/')\n        if len(path_components) < 2:\n            raise Exception(\n                'Invalid Google Cloud Storage (GCS) object path: {}'\n                .format(file_name))\n\n        bucket_id = path_components[0]\n        object_id = '/'.join(path_components[1:])\n        local_file = '/tmp/dataflow{}-{}'.format(str(uuid.uuid4())[:8],\n                                                 path_components[-1])\n        self._gcs_hook.download(bucket_id, object_id, local_file)\n\n        if os.stat(local_file).st_size > 0:\n            return local_file\n        raise Exception(\n            'Failed to download Google Cloud Storage (GCS) object: {}'\n            .format(file_name))\n```"], [64, "python", "get_single_list_nodes_data", "```python\ndef get_single_list_nodes_data(li, meta_data):\n    \"\"\"\n    Find consecutive li tags that have content that have the same list id.\n    \"\"\"\n    yield li\n    w_namespace = get_namespace(li, 'w')\n    current_numId = get_numId(li, w_namespace)\n    starting_ilvl = get_ilvl(li, w_namespace)\n    el = li\n    while True:\n        el = el.getnext()\n        if el is None:\n            break\n        # If the tag has no content ignore it.\n        if not has_text(el):\n            continue\n\n        # Stop the lists if you come across a list item that should be a\n        # heading.\n        if _is_top_level_upper_roman(el, meta_data):\n            break\n\n        if (\n                is_li(el, meta_data) and\n                (starting_ilvl > get_ilvl(el, w_namespace))):\n            break\n\n        new_numId = get_numId(el, w_namespace)\n        if new_numId is None or new_numId == -1:\n            # Not a p tag or a list item\n            yield el\n            continue\n        # If the list id of the next tag is different that the previous that\n        # means a new list being made (not nested)\n        if current_numId != new_numId:\n            # Not a subsequent list.\n            break\n        if is_last_li(el, meta_data, current_numId):\n            yield el\n            break\n        yield el\n```"], [161, "python", "Node.select", "```python\ndef select(self, selector):\n        \"\"\"\n        Like :meth:`find_all`, but takes a CSS selector string as input.\n        \"\"\"\n        op = operator.methodcaller('select', selector)\n        return self._wrap_multi(op)\n```"], [2, "python", "SqlDatabaseManagementService.list_quotas", "```python\ndef list_quotas(self, server_name):\n        '''\n        Gets quotas for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_quotas_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServerQuota)\n```"], [88, "python", "SolveBioAuth.logout", "```python\ndef logout(self):\n        \"\"\"Revoke the token and remove the cookie.\"\"\"\n        if self._oauth_client_secret:\n            try:\n                oauth_token = flask.request.cookies[self.TOKEN_COOKIE_NAME]\n                # Revoke the token\n                requests.post(\n                    urljoin(self._api_host, self.OAUTH2_REVOKE_TOKEN_PATH),\n                    data={\n                        'client_id': self._oauth_client_id,\n                        'client_secret': self._oauth_client_secret,\n                        'token': oauth_token\n                    })\n            except:\n                pass\n\n        response = flask.redirect('/')\n        self.clear_cookies(response)\n        return response\n```"], [517, "python", "Context.integrity_negotiated", "```python\ndef integrity_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if integrity protection (signing) has been negotiated in this context, False\n        otherwise. If this property is True, you can use :meth:`get_mic` to sign messages with a\n        message integrity code (MIC), which the peer application can verify.\n        \"\"\"\n        return (\n            self.flags & C.GSS_C_INTEG_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )\n```"], [6, "python", "QasmSimulatorPy._get_statevector", "```python\ndef _get_statevector(self):\n        \"\"\"Return the current statevector in JSON Result spec format\"\"\"\n        vec = np.reshape(self._statevector, 2 ** self._number_of_qubits)\n        # Expand complex numbers\n        vec = np.stack([vec.real, vec.imag], axis=1)\n        # Truncate small values\n        vec[abs(vec) < self._chop_threshold] = 0.0\n        return vec\n```"], [3, "python", "OpenIdMixin.authenticate_redirect", "```python\ndef authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))\n```"], [2, "python", "NotebookManager.save_new_notebook", "```python\ndef save_new_notebook(self, data, name=None, format=u'json'):\n        \"\"\"Save a new notebook and return its notebook_id.\n\n        If a name is passed in, it overrides any values in the notebook data\n        and the value in the data is updated to use that value.\n        \"\"\"\n        if format not in self.allowed_formats:\n            raise web.HTTPError(415, u'Invalid notebook format: %s' % format)\n\n        try:\n            nb = current.reads(data.decode('utf-8'), format)\n        except:\n            raise web.HTTPError(400, u'Invalid JSON data')\n\n        if name is None:\n            try:\n                name = nb.metadata.name\n            except AttributeError:\n                raise web.HTTPError(400, u'Missing notebook name')\n        nb.metadata.name = name\n\n        notebook_id = self.new_notebook_id(name)\n        self.save_notebook_object(notebook_id, nb)\n        return notebook_id\n```"], [323, "python", "PassManager.passes", "```python\ndef passes(self):\n        \"\"\"\n        Returns a list structure of the appended passes and its options.\n\n        Returns (list): The appended passes.\n        \"\"\"\n        ret = []\n        for pass_ in self.working_list:\n            ret.append(pass_.dump_passes())\n        return ret\n```"], [3, "python", "DatabaseConnection.read", "```python\ndef read(self, path, params=None):\n        \"\"\"Read the result at the given path (GET) from the CRUD API, using the optional params dictionary\n        as url parameters.\"\"\"\n        return self.handleresult(self.r.get(urljoin(self.url + CRUD_PATH,\n                                                    path),\n                                            params=params))\n```"], [11, "python", "Session.evaluate_script", "```python\ndef evaluate_script(self, script, *args):\n        \"\"\"\n        Evaluate the given JavaScript and return the result. Be careful when using this with\n        scripts that return complex objects, such as jQuery statements. :meth:`execute_script`\n        might be a better alternative.\n\n        Args:\n            script (str): A string of JavaScript to evaluate.\n            *args: Variable length argument list to pass to the executed JavaScript string.\n\n        Returns:\n            object: The result of the evaluated JavaScript (may be driver specific).\n        \"\"\"\n\n        args = [arg.base if isinstance(arg, Base) else arg for arg in args]\n        result = self.driver.evaluate_script(script, *args)\n        return self._wrap_element_script_result(result)\n```"], [279, "python", "get_all_boundary_algorithms", "```python\ndef get_all_boundary_algorithms():\n    \"\"\"Gets all the possible boundary algorithms in MSAF.\n\n    Returns\n    -------\n    algo_ids : list\n        List of all the IDs of boundary algorithms (strings).\n    \"\"\"\n    algo_ids = []\n    for name in msaf.algorithms.__all__:\n        module = eval(msaf.algorithms.__name__ + \".\" + name)\n        if module.is_boundary_type:\n            algo_ids.append(module.algo_id)\n    return algo_ids\n```"], [3, "python", "Expression.op_and", "```python\ndef op_and(self, *elements):\n        \"\"\"Update the ``Expression`` by joining the specified additional\n        ``elements`` using an \"AND\" ``Operator``\n\n        Args:\n            *elements (BaseExpression): The ``Expression`` and/or\n                ``Constraint`` elements which the \"AND\" ``Operator`` applies\n                to.\n\n        Returns:\n            Expression: ``self`` or related ``Expression``.\n        \"\"\"\n        expression = self.add_operator(Operator(';'))\n        for element in elements:\n            expression.add_element(element)\n        return expression\n```"], [47, "python", "LinkyClient._get_data", "```python\ndef _get_data(self, p_p_resource_id, start_date=None, end_date=None):\n        \"\"\"Get data.\"\"\"\n\n        data = {\n            '_' + REQ_PART + '_dateDebut': start_date,\n            '_' + REQ_PART + '_dateFin': end_date\n        }\n\n        params = {\n            'p_p_id': REQ_PART,\n            'p_p_lifecycle': 2,\n            'p_p_state': 'normal',\n            'p_p_mode': 'view',\n            'p_p_resource_id': p_p_resource_id,\n            'p_p_cacheability': 'cacheLevelPage',\n            'p_p_col_id': 'column-1',\n            'p_p_col_pos': 1,\n            'p_p_col_count': 3\n        }\n\n        try:\n            raw_res = self._session.post(DATA_URL,\n                                         data=data,\n                                         params=params,\n                                         allow_redirects=False,\n                                         timeout=self._timeout)\n\n            if 300 <= raw_res.status_code < 400:\n                raw_res = self._session.post(DATA_URL,\n                                             data=data,\n                                             params=params,\n                                             allow_redirects=False,\n                                             timeout=self._timeout)\n        except OSError as e:\n            raise PyLinkyError(\"Could not access enedis.fr: \" + str(e))\n\n        if raw_res.text is \"\":\n            raise PyLinkyError(\"No data\")\n\n        if 302 == raw_res.status_code and \"/messages/maintenance.html\" in raw_res.text:\n            raise PyLinkyError(\"Site in maintenance\")\n\n        try:\n            json_output = raw_res.json()\n        except (OSError, json.decoder.JSONDecodeError, simplejson.errors.JSONDecodeError) as e:\n            raise PyLinkyError(\"Impossible to decode response: \" + str(e) + \"\\nResponse was: \" + str(raw_res.text))\n\n        if json_output.get('etat').get('valeur') == 'erreur':\n            raise PyLinkyError(\"Enedis.fr answered with an error: \" + str(json_output))\n\n        return json_output.get('graphe')\n```"], [182, "python", "clinvar", "```python\ndef clinvar(institute_id, case_name, variant_id):\n    \"\"\"Build a clinVar submission form for a variant.\"\"\"\n    data = controllers.clinvar_export(store, institute_id, case_name, variant_id)\n    if request.method == 'GET':\n        return data\n    else: #POST\n        form_dict = request.form.to_dict()\n        submission_objects = set_submission_objects(form_dict) # A tuple of submission objects (variants and casedata objects)\n\n        # Add submission data to an open clinvar submission object,\n        # or create a new if no open submission is found in database\n        open_submission = store.get_open_clinvar_submission(current_user.email, institute_id)\n        updated_submission = store.add_to_submission(open_submission['_id'], submission_objects)\n\n        # Redirect to clinvar submissions handling page, and pass it the updated_submission_object\n        return redirect(url_for('cases.clinvar_submissions', institute_id=institute_id))\n```"], [2, "python", "MozillaClubParser.__get_event_fields", "```python\ndef __get_event_fields(self):\n        \"\"\"Get the events fields (columns) from the cells received.\"\"\"\n\n        event_fields = {}\n        # The cells in the first row are the column names\n        # Check that the columns names are the same we have as template\n        # Create the event template from the data retrieved\n        while self.ncell < len(self.cells):\n            cell = self.cells[self.ncell]\n            row = cell['gs$cell']['row']\n            if int(row) > 1:\n                # When the row number >1 the column row is finished\n                break\n            ncol = int(cell['gs$cell']['col'])\n            name = cell['content']['$t']\n            event_fields[ncol] = name\n            if ncol in EVENT_TEMPLATE:\n                if event_fields[ncol] != EVENT_TEMPLATE[ncol]:\n                    logger.warning(\"Event template changed in spreadsheet %s vs %s\",\n                                   name, EVENT_TEMPLATE[ncol])\n            else:\n                logger.warning(\"Event template changed in spreadsheet. New column: %s\", name)\n\n            self.ncell += 1\n        return event_fields\n```"], [11, "python", "center_eigenvalue_diff", "```python\ndef center_eigenvalue_diff(mat):\n    \"\"\"Compute the eigvals of mat and then find the center eigval difference.\"\"\"\n    N = len(mat)\n    evals = np.sort(la.eigvals(mat))\n    diff = np.abs(evals[N/2] - evals[N/2-1])\n    return diff\n```"], [9, "python", "MongoHook.delete_one", "```python\ndef delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)\n```"], [3, "python", "Mesh.set_fields", "```python\ndef set_fields(self, fields = None, **kwargs):\n    \"\"\"\n    Sets the fields.\n    \"\"\"\n    self.fields = []\n    if fields != None:\n      for field in fields: \n        self.fields.append(field)\n```"], [2, "python", "ResourceManager.addFromTex", "```python\ndef addFromTex(self,name,img,category):\n        \"\"\"\n        Adds a new texture from the given image.\n        \n        ``img`` may be any object that supports Pyglet-style copying in form of the ``blit_to_texture()`` method.\n        \n        This can be used to add textures that come from non-file sources, e.g. Render-to-texture.\n        \"\"\"\n        texreg = self.categoriesTexBin[category].add(img)\n        #texreg = texreg.get_transform(True,True) # Mirrors the image due to how pyglets coordinate system works\n        # Strange behaviour, sometimes needed and sometimes not\n        \n        self.categories[category][name]=texreg\n        target = texreg.target\n        texid = texreg.id\n        texcoords = texreg.tex_coords\n        \n        # Prevents texture bleeding with texture sizes that are powers of 2, else weird lines may appear at certain angles.\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST_MIPMAP_LINEAR)\n        glGenerateMipmap(GL_TEXTURE_2D)\n        \n        out = target,texid,texcoords\n        self.categoriesTexCache[category][name]=out\n        return out\n```"], [1, "python", "InstallRequirement.populate_link", "```python\ndef populate_link(self, finder, upgrade):\n        \"\"\"Ensure that if a link can be found for this, that it is found.\n\n        Note that self.link may still be None - if Upgrade is False and the\n        requirement is already installed.\n        \"\"\"\n        if self.link is None:\n            self.link = finder.find_requirement(self, upgrade)\n```"], [19, "python", "MultivariateNormalTriL.params_size", "```python\ndef params_size(event_size, name=None):\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    with tf.compat.v1.name_scope(name, 'MultivariateNormalTriL_params_size',\n                                 [event_size]):\n      return event_size + event_size * (event_size + 1) // 2\n```"], [7, "python", "ConnectionHandler.use_music_service", "```python\ndef use_music_service(self, service_name, api_key):\n        \"\"\"\n        Sets the current music service to service_name.\n\n        :param str service_name: Name of the music service\n        :param str api_key: Optional API key if necessary\n        \"\"\"\n\n        try:\n            self.current_music = self.music_services[service_name]\n        except KeyError:\n            if service_name == 'youtube':\n                self.music_services['youtube'] = Youtube()\n                self.current_music = self.music_services['youtube']\n            elif service_name == 'soundcloud':\n                self.music_services['soundcloud'] = Soundcloud(api_key=api_key)\n                self.current_music = self.music_services['soundcloud']\n            else:\n                log.error('Music service name is not recognized.')\n```"], [86, "python", "Striplog.invert", "```python\ndef invert(self, copy=False):\n        \"\"\"\n        Inverts the striplog, changing its order and the order of its contents.\n\n        Operates in place by default.\n\n        Args:\n            copy (bool): Whether to operate in place or make a copy.\n\n        Returns:\n            None if operating in-place, or an inverted copy of the striplog\n                if not.\n        \"\"\"\n        if copy:\n            return Striplog([i.invert(copy=True) for i in self])\n        else:\n            for i in self:\n                i.invert()\n            self.__sort()\n            o = self.order\n            self.order = {'depth': 'elevation', 'elevation': 'depth'}[o]\n            return\n```"], [352, "python", "VCGPrinter.edge", "```python\ndef edge(self, from_node, to_node, edge_type=\"\", **args):\n        \"\"\"draw an edge from a node to another.\n        \"\"\"\n        self._stream.write(\n            '%s%sedge: {sourcename:\"%s\" targetname:\"%s\"'\n            % (self._indent, edge_type, from_node, to_node)\n        )\n        self._write_attributes(EDGE_ATTRS, **args)\n        self._stream.write(\"}\\n\")\n```"], [226, "python", "pdf_case_report", "```python\ndef pdf_case_report(institute_id, case_name):\n    \"\"\"Download a pdf report for a case\"\"\"\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.case_report_content(store, institute_obj, case_obj)\n\n    # add coverage report on the bottom of this report\n    if current_app.config.get('SQLALCHEMY_DATABASE_URI'):\n        data['coverage_report'] = controllers.coverage_report_contents(store, institute_obj, case_obj, request.url_root)\n\n    # workaround to be able to print the case pedigree to pdf\n    if case_obj.get('madeline_info') is not None:\n        with open(os.path.join(cases_bp.static_folder, 'madeline.svg'), 'w') as temp_madeline:\n            temp_madeline.write(case_obj['madeline_info'])\n\n    html_report = render_template('cases/case_report.html', institute=institute_obj, case=case_obj, format='pdf', **data)\n    return render_pdf(HTML(string=html_report), download_filename=case_obj['display_name']+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')\n```"], [11, "python", "TaskInstance.are_dependencies_met", "```python\ndef are_dependencies_met(\n            self,\n            dep_context=None,\n            session=None,\n            verbose=False):\n        \"\"\"\n        Returns whether or not all the conditions are met for this task instance to be run\n        given the context for the dependencies (e.g. a task instance being force run from\n        the UI will ignore some dependencies).\n\n        :param dep_context: The execution context that determines the dependencies that\n            should be evaluated.\n        :type dep_context: DepContext\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param verbose: whether log details on failed dependencies on\n            info or debug log level\n        :type verbose: bool\n        \"\"\"\n        dep_context = dep_context or DepContext()\n        failed = False\n        verbose_aware_logger = self.log.info if verbose else self.log.debug\n        for dep_status in self.get_failed_dep_statuses(\n                dep_context=dep_context,\n                session=session):\n            failed = True\n\n            verbose_aware_logger(\n                \"Dependencies not met for %s, dependency '%s' FAILED: %s\",\n                self, dep_status.dep_name, dep_status.reason\n            )\n\n        if failed:\n            return False\n\n        verbose_aware_logger(\"Dependencies all met for %s\", self)\n        return True\n```"], [11, "python", "map_method", "```python\ndef map_method(method,object_list,*argseq,**kw):\n    \"\"\"map_method(method,object_list,*args,**kw) -> list\n\n    Return a list of the results of applying the methods to the items of the\n    argument sequence(s).  If more than one sequence is given, the method is\n    called with an argument list consisting of the corresponding item of each\n    sequence. All sequences must be of the same length.\n\n    Keyword arguments are passed verbatim to all objects called.\n\n    This is Python code, so it's not nearly as fast as the builtin map().\"\"\"\n\n    out_list = []\n    idx = 0\n    for object in object_list:\n        try:\n            handler = getattr(object, method)\n        except AttributeError:\n            out_list.append(None)\n        else:\n            if argseq:\n                args = map(lambda lst:lst[idx],argseq)\n                #print 'ob',object,'hand',handler,'ar',args # dbg\n                out_list.append(handler(args,**kw))\n            else:\n                out_list.append(handler(**kw))\n        idx += 1\n    return out_list\n```"], [40, "python", "Gerrit.parse_reviews", "```python\ndef parse_reviews(raw_data):\n        \"\"\"Parse a Gerrit reviews list.\"\"\"\n\n        # Join isolated reviews in JSON in array for parsing\n        items_raw = \"[\" + raw_data.replace(\"\\n\", \",\") + \"]\"\n        items_raw = items_raw.replace(\",]\", \"]\")\n        items = json.loads(items_raw)\n        reviews = []\n\n        for item in items:\n            if 'project' in item.keys():\n                reviews.append(item)\n\n        return reviews\n```"], [10, "python", "start", "```python\ndef start(dashboards, once, secrets):\n    \"\"\"Display a dashboard from the dashboard file(s) provided in the DASHBOARDS\n       Paths and/or URLs for dashboards (URLs must secrets with http or https)\n    \"\"\"\n\n    if secrets is None:\n        secrets = os.path.join(os.path.expanduser(\"~\"), \"/.doodledashboard/secrets\")\n\n    try:\n        loaded_secrets = try_read_secrets_file(secrets)\n    except InvalidSecretsException as err:\n        click.echo(get_error_message(err, default=\"Secrets file is invalid\"), err=True)\n        raise click.Abort()\n\n    read_configs = [\"\"\"\n    dashboard:\n      display:\n        type: console\n    \"\"\"]\n    for dashboard_file in dashboards:\n        read_configs.append(read_file(dashboard_file))\n\n    dashboard_config = DashboardConfigReader(initialise_component_loader(), loaded_secrets)\n\n    try:\n        dashboard = read_dashboard_from_config(dashboard_config, read_configs)\n    except YAMLError as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    try:\n        DashboardValidator().validate(dashboard)\n    except ValidationException as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    explain_dashboard(dashboard)\n\n    click.echo(\"Dashboard running...\")\n\n    while True:\n        try:\n            DashboardRunner(dashboard).cycle()\n        except SecretNotFound as err:\n            click.echo(get_error_message(err, default=\"Datafeed didn't have required secret\"), err=True)\n            raise click.Abort()\n\n        if once:\n            break\n```"], [20, "python", "OAuth2RequestValidator.authenticate_client", "```python\ndef authenticate_client(self, request, *args, **kwargs):\n        \"\"\"Authenticate itself in other means.\n\n        Other means means is described in `Section 3.2.1`_.\n\n        .. _`Section 3.2.1`: http://tools.ietf.org/html/rfc6749#section-3.2.1\n        \"\"\"\n        client_id, client_secret = self._get_client_creds_from_request(request)\n        log.debug('Authenticate client %r', client_id)\n\n        client = self._clientgetter(client_id)\n        if not client:\n            log.debug('Authenticate client failed, client not found.')\n            return False\n\n        request.client = client\n\n        # http://tools.ietf.org/html/rfc6749#section-2\n        # The client MAY omit the parameter if the client secret is an empty string.\n        if hasattr(client, 'client_secret') and client.client_secret != client_secret:\n            log.debug('Authenticate client failed, secret not match.')\n            return False\n\n        log.debug('Authenticate client success.')\n        return True\n```"], [1, "python", "Trajectory.f_get_parameters", "```python\ndef f_get_parameters(self, fast_access=False, copy=True):\n        \"\"\" Returns a dictionary containing the full parameter names as keys and the parameters\n         or the parameter data items as values.\n\n\n        :param fast_access:\n\n            Determines whether the parameter objects or their values are returned\n            in the dictionary.\n\n        :param copy:\n\n            Whether the original dictionary or a shallow copy is returned.\n            If you want the real dictionary please do not modify it at all!\n            Not Copying and fast access do not work at the same time! Raises ValueError\n            if fast access is true and copy false.\n\n        :return: Dictionary containing the parameters.\n\n        :raises: ValueError\n\n        \"\"\"\n        return self._return_item_dictionary(self._parameters, fast_access, copy)\n```"], [6, "python", "load_config", "```python\ndef load_config(under_test=False, custom=None):  # pragma: no cover\n    \"\"\"\n    Load the configuration.\n\n    :param under_test:\n        Tell us if we only have to load the configuration file (True)\n        or load the configuration file and initate the output directory\n        if it does not exist (False).\n    :type under_test: bool\n\n    :param custom:\n        A dict with the configuration index (from .PyFunceble.yaml) to update.\n    :type custom: dict\n\n    .. warning::\n        If :code:`custom` is given, the given :code:`dict` overwrite\n        the last value of the given configuration indexes.\n    \"\"\"\n\n    if \"config_loaded\" not in INTERN:\n        # The configuration was not already loaded.\n\n        # We load and download the different configuration file if they are non\n        # existant.\n        Load(CURRENT_DIRECTORY)\n\n        if not under_test:\n            # If we are not under test which means that we want to save informations,\n            # we initiate the directory structure.\n            DirectoryStructure()\n\n        # We save that the configuration was loaded.\n        INTERN.update({\"config_loaded\": True})\n\n        if custom and isinstance(custom, dict):\n            # The given configuration is not None or empty.\n            # and\n            # It is a dict.\n\n            # We update the configuration index.\n            CONFIGURATION.update(custom)\n```"], [81, "python", "FourDirectionalMoveController.registerEventHandlers", "```python\ndef registerEventHandlers(self):\n        \"\"\"\n        Registers needed keybinds and schedules the :py:meth:`update` Method.\n        \n        You can control what keybinds are used via the :confval:`controls.controls.forward` etc. Configuration Values.\n        \"\"\"\n        # Forward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.forward\"],\"peng3d:actor.%s.player.controls.forward\"%self.actor.uuid,self.on_fwd_down,False)\n        # Backward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.backward\"],\"peng3d:actor.%s.player.controls.backward\"%self.actor.uuid,self.on_bwd_down,False)\n        # Strafe Left\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.strafeleft\"],\"peng3d:actor.%s.player.controls.strafeleft\"%self.actor.uuid,self.on_left_down,False)\n        # Strafe Right\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.straferight\"],\"peng3d:actor.%s.player.controls.straferight\"%self.actor.uuid,self.on_right_down,False)\n        pyglet.clock.schedule_interval(self.update,1.0/60)\n```"], [1, "python", "FrameIdentifierVisitor.visit_Name", "```python\ndef visit_Name(self, node):\n        \"\"\"All assignments to names go through this function.\"\"\"\n        if node.ctx == 'store':\n            self.identifiers.declared_locally.add(node.name)\n        elif node.ctx == 'param':\n            self.identifiers.declared_parameter.add(node.name)\n        elif node.ctx == 'load' and not \\\n             self.identifiers.is_declared(node.name):\n            self.identifiers.undeclared.add(node.name)\n```"], [182, "python", "Client._call", "```python\ndef _call(self, method, params=None, request_id=None):\n        \"\"\" Calls the JSON-RPC endpoint. \"\"\"\n        params = params or []\n\n        # Determines which 'id' value to use and increment the counter associated with the current\n        # client instance if applicable.\n        rid = request_id or self._id_counter\n        if request_id is None:\n            self._id_counter += 1\n\n        # Prepares the payload and the headers that will be used to forge the request.\n        payload = {'jsonrpc': '2.0', 'method': method, 'params': params, 'id': rid}\n        headers = {'Content-Type': 'application/json'}\n        scheme = 'https' if self.tls else 'http'\n        url = '{}://{}:{}'.format(scheme, self.host, self.port)\n\n        # Calls the JSON-RPC endpoint!\n        try:\n            response = self.session.post(url, headers=headers, data=json.dumps(payload))\n            response.raise_for_status()\n        except HTTPError:\n            raise TransportError(\n                'Got unsuccessful response from server (status code: {})'.format(\n                    response.status_code),\n                response=response)\n\n        # Ensures the response body can be deserialized to JSON.\n        try:\n            response_data = response.json()\n        except ValueError as e:\n            raise ProtocolError(\n                'Unable to deserialize response body: {}'.format(e), response=response)\n\n        # Properly handles potential errors.\n        if response_data.get('error'):\n            code = response_data['error'].get('code', '')\n            message = response_data['error'].get('message', '')\n            raise ProtocolError(\n                'Error[{}] {}'.format(code, message), response=response, data=response_data)\n        elif 'result' not in response_data:\n            raise ProtocolError(\n                'Response is empty (result field is missing)', response=response,\n                data=response_data)\n\n        return response_data['result']\n```"], [38, "python", "Client._update_secrets", "```python\ndef _update_secrets(self):\n        '''update secrets will update metadata needed for pull and search\n        '''\n        self.token = self._required_get_and_update('SREGISTRY_GITLAB_TOKEN')\n        self.headers[\"Private-Token\"] = self.token\n```"]]}, "_runtime": 582.3312878608704, "_timestamp": 1571435603.4573867, "_step": 33}
{"Examples-FuncNameTest-python": {"_type": "table", "columns": ["Rank", "Language", "Query", "Code"], "data": [[152, "python", "unpack_4to8", "```python\ndef unpack_4to8(data):\n    \"\"\" Promote 2-bit unisgned data into 8-bit unsigned data.\n\n    Args:\n        data: Numpy array with dtype == uint8\n\n    Notes:\n        # The process is this:\n        # ABCDEFGH [Bits of one 4+4-bit value]\n        # 00000000ABCDEFGH [astype(uint16)]\n        # 0000ABCDEFGH0000 [<< 4]\n        # 0000ABCDXXXXEFGH [bitwise 'or' of previous two lines]\n        # 0000111100001111 [0x0F0F]\n        # 0000ABCD0000EFGH [bitwise 'and' of previous two lines]\n        # ABCD0000EFGH0000 [<< 4]\n        # which effectively pads the two 4-bit values with zeros on the right\n        # Note: This technique assumes LSB-first ordering\n    \"\"\"\n\n    tmpdata = data.astype(np.int16)  # np.empty(upshape, dtype=np.int16)\n    tmpdata = (tmpdata | (tmpdata << 4)) & 0x0F0F\n    # tmpdata = tmpdata << 4 # Shift into high bits to avoid needing to sign extend\n    updata = tmpdata.byteswap()\n    return updata.view(data.dtype)\n```"], [78, "python", "parse_mim2gene", "```python\ndef parse_mim2gene(lines):\n    \"\"\"Parse the file called mim2gene\n    \n    This file describes what type(s) the different mim numbers have.\n    The different entry types are: 'gene', 'gene/phenotype', 'moved/removed',\n    'phenotype', 'predominantly phenotypes'\n    Where:\n        gene: Is a gene entry\n        gene/phenotype: This entry describes both a phenotype and a gene\n        moved/removed: No explanation needed\n        phenotype: Describes a phenotype\n        predominantly phenotype: Not clearly established (probably phenotype)\n    \n    Args:\n        lines(iterable(str)): The mim2gene lines\n    \n    Yields:\n        parsed_entry(dict)\n    \n        {\n            \"mim_number\": int, \n            \"entry_type\": str, \n            \"entrez_gene_id\": int, \n            \"hgnc_symbol\": str, \n            \"ensembl_gene_id\": str,\n            \"ensembl_transcript_id\": str,\n        }\n    \n    \"\"\"\n    LOG.info(\"Parsing mim2gene\")\n    header = [\"mim_number\", \"entry_type\", \"entrez_gene_id\", \"hgnc_symbol\", \"ensembl_gene_id\"]\n    for i, line in enumerate(lines):\n        if line.startswith('#'):\n            continue\n        \n        if not len(line) > 0:\n            continue\n\n        line = line.rstrip()\n        parsed_entry = parse_omim_line(line, header)\n        parsed_entry['mim_number'] = int(parsed_entry['mim_number'])\n        parsed_entry['raw'] = line\n        \n        if 'hgnc_symbol' in parsed_entry:\n            parsed_entry['hgnc_symbol'] = parsed_entry['hgnc_symbol']\n        \n        if parsed_entry.get('entrez_gene_id'):\n            parsed_entry['entrez_gene_id'] = int(parsed_entry['entrez_gene_id'])\n        \n        if parsed_entry.get('ensembl_gene_id'):\n            ensembl_info = parsed_entry['ensembl_gene_id'].split(',')\n            parsed_entry['ensembl_gene_id'] = ensembl_info[0].strip()\n            if len(ensembl_info) > 1:\n                parsed_entry['ensembl_transcript_id'] = ensembl_info[1].strip()\n        \n        yield parsed_entry\n```"], [97, "python", "CreditNoteController.cancellation_fee", "```python\ndef cancellation_fee(self, percentage):\n        ''' Generates an invoice with a cancellation fee, and applies\n        credit to the invoice.\n\n        percentage (Decimal): The percentage of the credit note to turn into\n        a cancellation fee. Must be 0 <= percentage <= 100.\n        '''\n\n        # Local import to fix import cycles. Can we do better?\n        from .invoice import InvoiceController\n\n        assert(percentage >= 0 and percentage <= 100)\n\n        cancellation_fee = self.credit_note.value * percentage / 100\n        due = datetime.timedelta(days=1)\n        item = [(\"Cancellation fee\", cancellation_fee)]\n        invoice = InvoiceController.manual_invoice(\n            self.credit_note.invoice.user, due, item\n        )\n\n        if not invoice.is_paid:\n            self.apply_to_invoice(invoice)\n\n        return InvoiceController(invoice)\n```"], [33, "python", "update_panel", "```python\ndef update_panel(store, panel_name, csv_lines, option):\n    \"\"\"Update an existing gene panel with genes.\n\n    Args:\n        store(scout.adapter.MongoAdapter)\n        panel_name(str)\n        csv_lines(iterable(str)): Stream with genes\n        option(str): 'add' or 'replace'\n\n    Returns:\n        panel_obj(dict)\n    \"\"\"\n    new_genes= []\n    panel_obj = store.gene_panel(panel_name)\n    if panel_obj is None:\n        return None\n    try:\n        new_genes = parse_genes(csv_lines) # a list of gene dictionaries containing gene info\n    except SyntaxError as error:\n        flash(error.args[0], 'danger')\n        return None\n\n    # if existing genes are to be replaced by those in csv_lines\n    if option == 'replace':\n        # all existing genes should be deleted\n        for gene in panel_obj['genes']:\n            #create extra key to use in pending actions:\n            gene['hgnc_symbol'] = gene['symbol']\n            store.add_pending(panel_obj, gene, action='delete', info=None)\n\n    for new_gene in new_genes:\n        if not new_gene['hgnc_id']:\n            flash(\"gene missing hgnc id: {}\".format(new_gene['hgnc_symbol']),'danger')\n            continue\n        gene_obj = store.hgnc_gene(new_gene['hgnc_id'])\n        if gene_obj is None:\n            flash(\"gene not found: {} - {}\".format(new_gene['hgnc_id'], new_gene['hgnc_symbol']),'danger')\n            continue\n        if new_gene['hgnc_symbol'] and gene_obj['hgnc_symbol'] != new_gene['hgnc_symbol']:\n            flash(\"symbol mis-match: {0} | {1}\".format(\n                gene_obj['hgnc_symbol'], new_gene['hgnc_symbol']), 'warning')\n\n        info_data = {\n            'disease_associated_transcripts': new_gene['transcripts'],\n            'reduced_penetrance': new_gene['reduced_penetrance'],\n            'mosaicism': new_gene['mosaicism'],\n            'inheritance_models': new_gene['inheritance_models'],\n            'database_entry_version': new_gene['database_entry_version'],\n        }\n        if option == 'replace': # there will be no existing genes for sure, because we're replacing them all\n            action = 'add'\n        else: # add option. Add if genes is not existing. otherwise edit it\n            existing_genes = {gene['hgnc_id'] for gene in panel_obj['genes']}\n            action = 'edit' if gene_obj['hgnc_id'] in existing_genes else 'add'\n        store.add_pending(panel_obj, gene_obj, action=action, info=info_data)\n\n    return panel_obj\n```"], [19, "python", "MinHashLSHForest.index", "```python\ndef index(self):\n        '''\n        Index all the keys added so far and make them searchable.\n        '''\n        for i, hashtable in enumerate(self.hashtables):\n            self.sorted_hashtables[i] = [H for H in hashtable.keys()]\n            self.sorted_hashtables[i].sort()\n```"], [92, "python", "Eaf.generate_ts_id", "```python\ndef generate_ts_id(self, time=None):\n        \"\"\"Generate the next timeslot id, this function is mainly used\n        internally\n\n        :param int time: Initial time to assign to the timeslot.\n        :raises ValueError: If the time is negative.\n        \"\"\"\n        if time and time < 0:\n            raise ValueError('Time is negative...')\n        if not self.maxts:\n            valid_ts = [int(''.join(filter(str.isdigit, a)))\n                        for a in self.timeslots]\n            self.maxts = max(valid_ts + [1])+1\n        else:\n            self.maxts += 1\n        ts = 'ts{:d}'.format(self.maxts)\n        self.timeslots[ts] = time\n        return ts\n```"], [4, "python", "BandwidthLimitedStream.read", "```python\ndef read(self, amount):\n        \"\"\"Read a specified amount\n\n        Reads will only be throttled if bandwidth limiting is enabled.\n        \"\"\"\n        if not self._bandwidth_limiting_enabled:\n            return self._fileobj.read(amount)\n\n        # We do not want to be calling consume on every read as the read\n        # amounts can be small causing the lock of the leaky bucket to\n        # introduce noticeable overhead. So instead we keep track of\n        # how many bytes we have seen and only call consume once we pass a\n        # certain threshold.\n        self._bytes_seen += amount\n        if self._bytes_seen < self._bytes_threshold:\n            return self._fileobj.read(amount)\n\n        self._consume_through_leaky_bucket()\n        return self._fileobj.read(amount)\n```"], [89, "python", "Meter.extractMonthTariff", "```python\ndef extractMonthTariff(self, month):\n        \"\"\" Extract the tariff for a single month from the meter object buffer.\n\n        Args:\n            month (int):  A :class:`~ekmmeters.Months` value or range(Extents.Months).\n\n        Returns:\n            tuple: The eight tariff period totals for month. The return tuple breaks out as follows:\n\n            ================= ======================================\n            kWh_Tariff_1      kWh for tariff period 1 over month.\n            kWh_Tariff_2      kWh for tariff period 2 over month\n            kWh_Tariff_3      kWh for tariff period 3 over month\n            kWh_Tariff_4      kWh for tariff period 4 over month\n            kWh_Tot           Total kWh over requested month\n            Rev_kWh_Tariff_1  Rev kWh for tariff period 1 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 2 over month\n            Rev_kWh_Tariff_3  Rev kWh for tariff period 3 over month\n            Rev_kWh_Tariff_4  Rev kWh for tariff period 4 over month\n            Rev_kWh_Tot       Total Rev kWh over requested month\n            ================= ======================================\n\n        \"\"\"\n        ret = namedtuple(\"ret\", [\"Month\", Field.kWh_Tariff_1, Field.kWh_Tariff_2, Field.kWh_Tariff_3,\n                         Field.kWh_Tariff_4, Field.kWh_Tot, Field.Rev_kWh_Tariff_1,\n                         Field.Rev_kWh_Tariff_2, Field.Rev_kWh_Tariff_3,\n                         Field.Rev_kWh_Tariff_4, Field.Rev_kWh_Tot])\n        month += 1\n        ret.Month = str(month)\n        if (month < 1) or (month > Extents.Months):\n            ret.kWh_Tariff_1 = ret.kWh_Tariff_2 = ret.kWh_Tariff_3 = ret.kWh_Tariff_4 = str(0)\n            ret.Rev_kWh_Tariff_1 = ret.Rev_kWh_Tariff_2 = ret.Rev_kWh_Tariff_3 = ret.Rev_kWh_Tariff_4 = str(0)\n            ret.kWh_Tot = ret.Rev_kWh_Tot = str(0)\n            ekm_log(\"Out of range(Extents.Months) month = \" + str(month))\n            return ret\n\n        base_str = \"Month_\" + str(month) + \"_\"\n        ret.kWh_Tariff_1 = self.m_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.kWh_Tariff_2 = self.m_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.kWh_Tariff_3 = self.m_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.kWh_Tariff_4 = self.m_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.kWh_Tot = self.m_mons[base_str + \"Tot\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_1 = self.m_rev_mons[base_str + \"Tariff_1\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_2 = self.m_rev_mons[base_str + \"Tariff_2\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_3 = self.m_rev_mons[base_str + \"Tariff_3\"][MeterData.StringValue]\n        ret.Rev_kWh_Tariff_4 = self.m_rev_mons[base_str + \"Tariff_4\"][MeterData.StringValue]\n        ret.Rev_kWh_Tot = self.m_rev_mons[base_str + \"Tot\"][MeterData.StringValue]\n        return ret\n```"], [38, "python", "BigQueryCursor.executemany", "```python\ndef executemany(self, operation, seq_of_parameters):\n        \"\"\"\n        Execute a BigQuery query multiple times with different parameters.\n\n        :param operation: The query to execute.\n        :type operation: str\n        :param seq_of_parameters: List of dictionary parameters to substitute into the\n            query.\n        :type seq_of_parameters: list\n        \"\"\"\n        for parameters in seq_of_parameters:\n            self.execute(operation, parameters)\n```"], [2, "python", "TypeSystem.convert_from_binary", "```python\ndef convert_from_binary(self, binvalue, type, **kwargs):\n        \"\"\"\n        Convert binary data to type 'type'.\n\n        'type' must have a convert_binary function.  If 'type'\n        supports size checking, the size function is called to ensure\n        that binvalue is the correct size for deserialization\n        \"\"\"\n\n        size = self.get_type_size(type)\n        if size > 0 and len(binvalue) != size:\n            raise ArgumentError(\"Could not convert type from binary since the data was not the correct size\", required_size=size, actual_size=len(binvalue), type=type)\n\n        typeobj = self.get_type(type)\n\n        if not hasattr(typeobj, 'convert_binary'):\n            raise ArgumentError(\"Type does not support conversion from binary\", type=type)\n\n        return typeobj.convert_binary(binvalue, **kwargs)\n```"], [33, "python", "read_data", "```python\ndef read_data(data_file, dataformat, name_mode):\n    \"\"\"\n    Load data_file described by a dataformat dict.\n\n    Parameters\n    ----------\n    data_file : str\n        Path to data file, including extension.\n    dataformat : dict\n        A dataformat dict, see example below.\n    name_mode : str\n        How to identyfy sample names. If 'file_names' uses the\n        input name of the file, stripped of the extension. If\n        'metadata_names' uses the 'name' attribute of the 'meta'\n        sub-dictionary in dataformat. If any other str, uses this\n        str as the sample name.\n\n    Example\n    -------\n    >>>\n    {'genfromtext_args': {'delimiter': ',',\n                          'skip_header': 4},  # passed directly to np.genfromtxt\n     'column_id': {'name_row': 3,  # which row contains the column names\n                   'delimiter': ',',  # delimeter between column names\n                   'timecolumn': 0,  # which column contains the 'time' variable\n                   'pattern': '([A-z]{1,2}[0-9]{1,3})'},  # a regex pattern which captures the column names\n     'meta_regex': {  # a dict of (line_no: ([descriptors], [regexs])) pairs\n                    0: (['path'], '(.*)'),\n                    2: (['date', 'method'],  # MUST include date\n                     '([A-Z][a-z]+ [0-9]+ [0-9]{4}[ ]+[0-9:]+ [amp]+).* ([A-z0-9]+\\.m)')\n                   }\n    }\n\n    Returns\n    -------\n    sample, analytes, data, meta : tuple\n    \"\"\"\n    with open(data_file) as f:\n        lines = f.readlines()\n\n    if 'meta_regex' in dataformat.keys():\n        meta = Bunch()\n        for k, v in dataformat['meta_regex'].items():\n            try:\n                out = re.search(v[-1], lines[int(k)]).groups()\n            except:\n                raise ValueError('Failed reading metadata when applying:\\n  regex: {}\\nto\\n  line: {}'.format(v[-1], lines[int(k)]))\n            for i in np.arange(len(v[0])):\n                meta[v[0][i]] = out[i]\n    else:\n        meta = {}\n\n    # sample name\n    if name_mode == 'file_names':\n        sample = os.path.basename(data_file).split('.')[0]\n    elif name_mode == 'metadata_names':\n        sample = meta['name']\n    else:\n        sample = name_mode\n\n    # column and analyte names\n    columns = np.array(lines[dataformat['column_id']['name_row']].strip().split(\n        dataformat['column_id']['delimiter']))\n    if 'pattern' in dataformat['column_id'].keys():\n        pr = re.compile(dataformat['column_id']['pattern'])\n        analytes = [pr.match(c).groups()[0] for c in columns if pr.match(c)]\n\n    # do any required pre-formatting\n    if 'preformat_replace' in dataformat.keys():\n        with open(data_file) as f:\n            fbuffer = f.read()\n        for k, v in dataformat['preformat_replace'].items():\n            fbuffer = re.sub(k, v, fbuffer)\n        # dead data\n        read_data = np.genfromtxt(BytesIO(fbuffer.encode()),\n                                  **dataformat['genfromtext_args']).T\n    else:\n        # read data\n        read_data = np.genfromtxt(data_file,\n                                  **dataformat['genfromtext_args']).T\n\n    # data dict\n    dind = np.zeros(read_data.shape[0], dtype=bool)\n    for a in analytes:\n        dind[columns == a] = True\n\n    data = Bunch()\n    data['Time'] = read_data[dataformat['column_id']['timecolumn']]\n\n    # deal with time units\n    if 'time_unit' in dataformat['column_id']:\n        if isinstance(dataformat['column_id']['time_unit'], (float, int)):\n            time_mult = dataformat['column_id']['time_unit']\n        elif isinstance(dataformat['column_id']['time_unit'], str):\n            unit_multipliers = {'ms': 1/1000,\n                                'min': 60/1,\n                                's': 1}\n            try:\n                time_mult = unit_multipliers[dataformat['column_id']['time_unit']]\n            except:\n                raise ValueError(\"In dataformat: time_unit must be a number, 'ms', 'min' or 's'\")\n        data['Time'] *= time_mult\n        \n    # convert raw data into counts\n    # TODO: Is this correct? Should actually be per-analyte dwell?\n    # if 'unit' in dataformat:\n    #     if dataformat['unit'] == 'cps':\n    #         tstep = data['Time'][1] - data['Time'][0]\n    #         read_data[dind] *= tstep\n    #     else:\n    #         pass\n    data['rawdata'] = Bunch(zip(analytes, read_data[dind]))\n    data['total_counts'] = np.nansum(read_data[dind], 0)\n\n    return sample, analytes, data, meta\n```"], [15, "python", "DrawElement.top", "```python\ndef top(self):\n        \"\"\" Constructs the top line of the element\"\"\"\n        ret = self.top_format % self.top_connect.center(\n            self.width, self.top_pad)\n        if self.right_fill:\n            ret = ret.ljust(self.right_fill, self.top_pad)\n        if self.left_fill:\n            ret = ret.rjust(self.left_fill, self.top_pad)\n        ret = ret.center(self.layer_width, self.top_bck)\n        return ret\n```"], [2, "python", "GoogleCloudBucketHelper.google_cloud_to_local", "```python\ndef google_cloud_to_local(self, file_name):\n        \"\"\"\n        Checks whether the file specified by file_name is stored in Google Cloud\n        Storage (GCS), if so, downloads the file and saves it locally. The full\n        path of the saved file will be returned. Otherwise the local file_name\n        will be returned immediately.\n\n        :param file_name: The full path of input file.\n        :type file_name: str\n        :return: The full path of local file.\n        :rtype: str\n        \"\"\"\n        if not file_name.startswith('gs://'):\n            return file_name\n\n        # Extracts bucket_id and object_id by first removing 'gs://' prefix and\n        # then split the remaining by path delimiter '/'.\n        path_components = file_name[self.GCS_PREFIX_LENGTH:].split('/')\n        if len(path_components) < 2:\n            raise Exception(\n                'Invalid Google Cloud Storage (GCS) object path: {}'\n                .format(file_name))\n\n        bucket_id = path_components[0]\n        object_id = '/'.join(path_components[1:])\n        local_file = '/tmp/dataflow{}-{}'.format(str(uuid.uuid4())[:8],\n                                                 path_components[-1])\n        self._gcs_hook.download(bucket_id, object_id, local_file)\n\n        if os.stat(local_file).st_size > 0:\n            return local_file\n        raise Exception(\n            'Failed to download Google Cloud Storage (GCS) object: {}'\n            .format(file_name))\n```"], [260, "python", "get_single_list_nodes_data", "```python\ndef get_single_list_nodes_data(li, meta_data):\n    \"\"\"\n    Find consecutive li tags that have content that have the same list id.\n    \"\"\"\n    yield li\n    w_namespace = get_namespace(li, 'w')\n    current_numId = get_numId(li, w_namespace)\n    starting_ilvl = get_ilvl(li, w_namespace)\n    el = li\n    while True:\n        el = el.getnext()\n        if el is None:\n            break\n        # If the tag has no content ignore it.\n        if not has_text(el):\n            continue\n\n        # Stop the lists if you come across a list item that should be a\n        # heading.\n        if _is_top_level_upper_roman(el, meta_data):\n            break\n\n        if (\n                is_li(el, meta_data) and\n                (starting_ilvl > get_ilvl(el, w_namespace))):\n            break\n\n        new_numId = get_numId(el, w_namespace)\n        if new_numId is None or new_numId == -1:\n            # Not a p tag or a list item\n            yield el\n            continue\n        # If the list id of the next tag is different that the previous that\n        # means a new list being made (not nested)\n        if current_numId != new_numId:\n            # Not a subsequent list.\n            break\n        if is_last_li(el, meta_data, current_numId):\n            yield el\n            break\n        yield el\n```"], [13, "python", "Node.select", "```python\ndef select(self, selector):\n        \"\"\"\n        Like :meth:`find_all`, but takes a CSS selector string as input.\n        \"\"\"\n        op = operator.methodcaller('select', selector)\n        return self._wrap_multi(op)\n```"], [23, "python", "SqlDatabaseManagementService.list_quotas", "```python\ndef list_quotas(self, server_name):\n        '''\n        Gets quotas for an Azure SQL Database Server.\n\n        server_name:\n            Name of the server.\n        '''\n        _validate_not_none('server_name', server_name)\n        response = self._perform_get(self._get_quotas_path(server_name),\n                                     None)\n        return _MinidomXmlToObject.parse_service_resources_response(\n            response, ServerQuota)\n```"], [292, "python", "SolveBioAuth.logout", "```python\ndef logout(self):\n        \"\"\"Revoke the token and remove the cookie.\"\"\"\n        if self._oauth_client_secret:\n            try:\n                oauth_token = flask.request.cookies[self.TOKEN_COOKIE_NAME]\n                # Revoke the token\n                requests.post(\n                    urljoin(self._api_host, self.OAUTH2_REVOKE_TOKEN_PATH),\n                    data={\n                        'client_id': self._oauth_client_id,\n                        'client_secret': self._oauth_client_secret,\n                        'token': oauth_token\n                    })\n            except:\n                pass\n\n        response = flask.redirect('/')\n        self.clear_cookies(response)\n        return response\n```"], [5, "python", "Context.integrity_negotiated", "```python\ndef integrity_negotiated(self):\n        \"\"\"\n        After :meth:`step` has been called, this property will be set to\n        True if integrity protection (signing) has been negotiated in this context, False\n        otherwise. If this property is True, you can use :meth:`get_mic` to sign messages with a\n        message integrity code (MIC), which the peer application can verify.\n        \"\"\"\n        return (\n            self.flags & C.GSS_C_INTEG_FLAG\n        ) and (\n            self.established or (self.flags & C.GSS_C_PROT_READY_FLAG)\n        )\n```"], [16, "python", "QasmSimulatorPy._get_statevector", "```python\ndef _get_statevector(self):\n        \"\"\"Return the current statevector in JSON Result spec format\"\"\"\n        vec = np.reshape(self._statevector, 2 ** self._number_of_qubits)\n        # Expand complex numbers\n        vec = np.stack([vec.real, vec.imag], axis=1)\n        # Truncate small values\n        vec[abs(vec) < self._chop_threshold] = 0.0\n        return vec\n```"], [1, "python", "OpenIdMixin.authenticate_redirect", "```python\ndef authenticate_redirect(\n        self, callback_uri=None, ax_attrs=[\"name\", \"email\", \"language\",\n                                           \"username\"]):\n\n        \"\"\"Returns the authentication URL for this service.\n\n        After authentication, the service will redirect back to the given\n        callback URI.\n\n        We request the given attributes for the authenticated user by\n        default (name, email, language, and username). If you don't need\n        all those attributes for your app, you can request fewer with\n        the ax_attrs keyword argument.\n        \"\"\"\n        callback_uri = callback_uri or self.request.uri\n        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)\n        self.redirect(self._OPENID_ENDPOINT + \"?\" + urllib.urlencode(args))\n```"], [1, "python", "NotebookManager.save_new_notebook", "```python\ndef save_new_notebook(self, data, name=None, format=u'json'):\n        \"\"\"Save a new notebook and return its notebook_id.\n\n        If a name is passed in, it overrides any values in the notebook data\n        and the value in the data is updated to use that value.\n        \"\"\"\n        if format not in self.allowed_formats:\n            raise web.HTTPError(415, u'Invalid notebook format: %s' % format)\n\n        try:\n            nb = current.reads(data.decode('utf-8'), format)\n        except:\n            raise web.HTTPError(400, u'Invalid JSON data')\n\n        if name is None:\n            try:\n                name = nb.metadata.name\n            except AttributeError:\n                raise web.HTTPError(400, u'Missing notebook name')\n        nb.metadata.name = name\n\n        notebook_id = self.new_notebook_id(name)\n        self.save_notebook_object(notebook_id, nb)\n        return notebook_id\n```"], [616, "python", "PassManager.passes", "```python\ndef passes(self):\n        \"\"\"\n        Returns a list structure of the appended passes and its options.\n\n        Returns (list): The appended passes.\n        \"\"\"\n        ret = []\n        for pass_ in self.working_list:\n            ret.append(pass_.dump_passes())\n        return ret\n```"], [38, "python", "DatabaseConnection.read", "```python\ndef read(self, path, params=None):\n        \"\"\"Read the result at the given path (GET) from the CRUD API, using the optional params dictionary\n        as url parameters.\"\"\"\n        return self.handleresult(self.r.get(urljoin(self.url + CRUD_PATH,\n                                                    path),\n                                            params=params))\n```"], [1, "python", "Session.evaluate_script", "```python\ndef evaluate_script(self, script, *args):\n        \"\"\"\n        Evaluate the given JavaScript and return the result. Be careful when using this with\n        scripts that return complex objects, such as jQuery statements. :meth:`execute_script`\n        might be a better alternative.\n\n        Args:\n            script (str): A string of JavaScript to evaluate.\n            *args: Variable length argument list to pass to the executed JavaScript string.\n\n        Returns:\n            object: The result of the evaluated JavaScript (may be driver specific).\n        \"\"\"\n\n        args = [arg.base if isinstance(arg, Base) else arg for arg in args]\n        result = self.driver.evaluate_script(script, *args)\n        return self._wrap_element_script_result(result)\n```"], [31, "python", "get_all_boundary_algorithms", "```python\ndef get_all_boundary_algorithms():\n    \"\"\"Gets all the possible boundary algorithms in MSAF.\n\n    Returns\n    -------\n    algo_ids : list\n        List of all the IDs of boundary algorithms (strings).\n    \"\"\"\n    algo_ids = []\n    for name in msaf.algorithms.__all__:\n        module = eval(msaf.algorithms.__name__ + \".\" + name)\n        if module.is_boundary_type:\n            algo_ids.append(module.algo_id)\n    return algo_ids\n```"], [2, "python", "Expression.op_and", "```python\ndef op_and(self, *elements):\n        \"\"\"Update the ``Expression`` by joining the specified additional\n        ``elements`` using an \"AND\" ``Operator``\n\n        Args:\n            *elements (BaseExpression): The ``Expression`` and/or\n                ``Constraint`` elements which the \"AND\" ``Operator`` applies\n                to.\n\n        Returns:\n            Expression: ``self`` or related ``Expression``.\n        \"\"\"\n        expression = self.add_operator(Operator(';'))\n        for element in elements:\n            expression.add_element(element)\n        return expression\n```"], [10, "python", "LinkyClient._get_data", "```python\ndef _get_data(self, p_p_resource_id, start_date=None, end_date=None):\n        \"\"\"Get data.\"\"\"\n\n        data = {\n            '_' + REQ_PART + '_dateDebut': start_date,\n            '_' + REQ_PART + '_dateFin': end_date\n        }\n\n        params = {\n            'p_p_id': REQ_PART,\n            'p_p_lifecycle': 2,\n            'p_p_state': 'normal',\n            'p_p_mode': 'view',\n            'p_p_resource_id': p_p_resource_id,\n            'p_p_cacheability': 'cacheLevelPage',\n            'p_p_col_id': 'column-1',\n            'p_p_col_pos': 1,\n            'p_p_col_count': 3\n        }\n\n        try:\n            raw_res = self._session.post(DATA_URL,\n                                         data=data,\n                                         params=params,\n                                         allow_redirects=False,\n                                         timeout=self._timeout)\n\n            if 300 <= raw_res.status_code < 400:\n                raw_res = self._session.post(DATA_URL,\n                                             data=data,\n                                             params=params,\n                                             allow_redirects=False,\n                                             timeout=self._timeout)\n        except OSError as e:\n            raise PyLinkyError(\"Could not access enedis.fr: \" + str(e))\n\n        if raw_res.text is \"\":\n            raise PyLinkyError(\"No data\")\n\n        if 302 == raw_res.status_code and \"/messages/maintenance.html\" in raw_res.text:\n            raise PyLinkyError(\"Site in maintenance\")\n\n        try:\n            json_output = raw_res.json()\n        except (OSError, json.decoder.JSONDecodeError, simplejson.errors.JSONDecodeError) as e:\n            raise PyLinkyError(\"Impossible to decode response: \" + str(e) + \"\\nResponse was: \" + str(raw_res.text))\n\n        if json_output.get('etat').get('valeur') == 'erreur':\n            raise PyLinkyError(\"Enedis.fr answered with an error: \" + str(json_output))\n\n        return json_output.get('graphe')\n```"], [161, "python", "clinvar", "```python\ndef clinvar(institute_id, case_name, variant_id):\n    \"\"\"Build a clinVar submission form for a variant.\"\"\"\n    data = controllers.clinvar_export(store, institute_id, case_name, variant_id)\n    if request.method == 'GET':\n        return data\n    else: #POST\n        form_dict = request.form.to_dict()\n        submission_objects = set_submission_objects(form_dict) # A tuple of submission objects (variants and casedata objects)\n\n        # Add submission data to an open clinvar submission object,\n        # or create a new if no open submission is found in database\n        open_submission = store.get_open_clinvar_submission(current_user.email, institute_id)\n        updated_submission = store.add_to_submission(open_submission['_id'], submission_objects)\n\n        # Redirect to clinvar submissions handling page, and pass it the updated_submission_object\n        return redirect(url_for('cases.clinvar_submissions', institute_id=institute_id))\n```"], [7, "python", "MozillaClubParser.__get_event_fields", "```python\ndef __get_event_fields(self):\n        \"\"\"Get the events fields (columns) from the cells received.\"\"\"\n\n        event_fields = {}\n        # The cells in the first row are the column names\n        # Check that the columns names are the same we have as template\n        # Create the event template from the data retrieved\n        while self.ncell < len(self.cells):\n            cell = self.cells[self.ncell]\n            row = cell['gs$cell']['row']\n            if int(row) > 1:\n                # When the row number >1 the column row is finished\n                break\n            ncol = int(cell['gs$cell']['col'])\n            name = cell['content']['$t']\n            event_fields[ncol] = name\n            if ncol in EVENT_TEMPLATE:\n                if event_fields[ncol] != EVENT_TEMPLATE[ncol]:\n                    logger.warning(\"Event template changed in spreadsheet %s vs %s\",\n                                   name, EVENT_TEMPLATE[ncol])\n            else:\n                logger.warning(\"Event template changed in spreadsheet. New column: %s\", name)\n\n            self.ncell += 1\n        return event_fields\n```"], [161, "python", "center_eigenvalue_diff", "```python\ndef center_eigenvalue_diff(mat):\n    \"\"\"Compute the eigvals of mat and then find the center eigval difference.\"\"\"\n    N = len(mat)\n    evals = np.sort(la.eigvals(mat))\n    diff = np.abs(evals[N/2] - evals[N/2-1])\n    return diff\n```"], [3, "python", "MongoHook.delete_one", "```python\ndef delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):\n        \"\"\"\n        Deletes a single document in a mongo collection.\n        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one\n\n        :param mongo_collection: The name of the collection to delete from.\n        :type mongo_collection: str\n        :param filter_doc: A query that matches the document to delete.\n        :type filter_doc: dict\n        :param mongo_db: The name of the database to use.\n            Can be omitted; then the database from the connection string is used.\n        :type mongo_db: str\n\n        \"\"\"\n        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)\n\n        return collection.delete_one(filter_doc, **kwargs)\n```"], [1, "python", "Mesh.set_fields", "```python\ndef set_fields(self, fields = None, **kwargs):\n    \"\"\"\n    Sets the fields.\n    \"\"\"\n    self.fields = []\n    if fields != None:\n      for field in fields: \n        self.fields.append(field)\n```"], [119, "python", "ResourceManager.addFromTex", "```python\ndef addFromTex(self,name,img,category):\n        \"\"\"\n        Adds a new texture from the given image.\n        \n        ``img`` may be any object that supports Pyglet-style copying in form of the ``blit_to_texture()`` method.\n        \n        This can be used to add textures that come from non-file sources, e.g. Render-to-texture.\n        \"\"\"\n        texreg = self.categoriesTexBin[category].add(img)\n        #texreg = texreg.get_transform(True,True) # Mirrors the image due to how pyglets coordinate system works\n        # Strange behaviour, sometimes needed and sometimes not\n        \n        self.categories[category][name]=texreg\n        target = texreg.target\n        texid = texreg.id\n        texcoords = texreg.tex_coords\n        \n        # Prevents texture bleeding with texture sizes that are powers of 2, else weird lines may appear at certain angles.\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST_MIPMAP_LINEAR)\n        glGenerateMipmap(GL_TEXTURE_2D)\n        \n        out = target,texid,texcoords\n        self.categoriesTexCache[category][name]=out\n        return out\n```"], [1, "python", "InstallRequirement.populate_link", "```python\ndef populate_link(self, finder, upgrade):\n        \"\"\"Ensure that if a link can be found for this, that it is found.\n\n        Note that self.link may still be None - if Upgrade is False and the\n        requirement is already installed.\n        \"\"\"\n        if self.link is None:\n            self.link = finder.find_requirement(self, upgrade)\n```"], [227, "python", "MultivariateNormalTriL.params_size", "```python\ndef params_size(event_size, name=None):\n    \"\"\"The number of `params` needed to create a single distribution.\"\"\"\n    with tf.compat.v1.name_scope(name, 'MultivariateNormalTriL_params_size',\n                                 [event_size]):\n      return event_size + event_size * (event_size + 1) // 2\n```"], [8, "python", "ConnectionHandler.use_music_service", "```python\ndef use_music_service(self, service_name, api_key):\n        \"\"\"\n        Sets the current music service to service_name.\n\n        :param str service_name: Name of the music service\n        :param str api_key: Optional API key if necessary\n        \"\"\"\n\n        try:\n            self.current_music = self.music_services[service_name]\n        except KeyError:\n            if service_name == 'youtube':\n                self.music_services['youtube'] = Youtube()\n                self.current_music = self.music_services['youtube']\n            elif service_name == 'soundcloud':\n                self.music_services['soundcloud'] = Soundcloud(api_key=api_key)\n                self.current_music = self.music_services['soundcloud']\n            else:\n                log.error('Music service name is not recognized.')\n```"], [89, "python", "Striplog.invert", "```python\ndef invert(self, copy=False):\n        \"\"\"\n        Inverts the striplog, changing its order and the order of its contents.\n\n        Operates in place by default.\n\n        Args:\n            copy (bool): Whether to operate in place or make a copy.\n\n        Returns:\n            None if operating in-place, or an inverted copy of the striplog\n                if not.\n        \"\"\"\n        if copy:\n            return Striplog([i.invert(copy=True) for i in self])\n        else:\n            for i in self:\n                i.invert()\n            self.__sort()\n            o = self.order\n            self.order = {'depth': 'elevation', 'elevation': 'depth'}[o]\n            return\n```"], [199, "python", "VCGPrinter.edge", "```python\ndef edge(self, from_node, to_node, edge_type=\"\", **args):\n        \"\"\"draw an edge from a node to another.\n        \"\"\"\n        self._stream.write(\n            '%s%sedge: {sourcename:\"%s\" targetname:\"%s\"'\n            % (self._indent, edge_type, from_node, to_node)\n        )\n        self._write_attributes(EDGE_ATTRS, **args)\n        self._stream.write(\"}\\n\")\n```"], [86, "python", "pdf_case_report", "```python\ndef pdf_case_report(institute_id, case_name):\n    \"\"\"Download a pdf report for a case\"\"\"\n\n    institute_obj, case_obj = institute_and_case(store, institute_id, case_name)\n    data = controllers.case_report_content(store, institute_obj, case_obj)\n\n    # add coverage report on the bottom of this report\n    if current_app.config.get('SQLALCHEMY_DATABASE_URI'):\n        data['coverage_report'] = controllers.coverage_report_contents(store, institute_obj, case_obj, request.url_root)\n\n    # workaround to be able to print the case pedigree to pdf\n    if case_obj.get('madeline_info') is not None:\n        with open(os.path.join(cases_bp.static_folder, 'madeline.svg'), 'w') as temp_madeline:\n            temp_madeline.write(case_obj['madeline_info'])\n\n    html_report = render_template('cases/case_report.html', institute=institute_obj, case=case_obj, format='pdf', **data)\n    return render_pdf(HTML(string=html_report), download_filename=case_obj['display_name']+'_'+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'_scout.pdf')\n```"], [36, "python", "TaskInstance.are_dependencies_met", "```python\ndef are_dependencies_met(\n            self,\n            dep_context=None,\n            session=None,\n            verbose=False):\n        \"\"\"\n        Returns whether or not all the conditions are met for this task instance to be run\n        given the context for the dependencies (e.g. a task instance being force run from\n        the UI will ignore some dependencies).\n\n        :param dep_context: The execution context that determines the dependencies that\n            should be evaluated.\n        :type dep_context: DepContext\n        :param session: database session\n        :type session: sqlalchemy.orm.session.Session\n        :param verbose: whether log details on failed dependencies on\n            info or debug log level\n        :type verbose: bool\n        \"\"\"\n        dep_context = dep_context or DepContext()\n        failed = False\n        verbose_aware_logger = self.log.info if verbose else self.log.debug\n        for dep_status in self.get_failed_dep_statuses(\n                dep_context=dep_context,\n                session=session):\n            failed = True\n\n            verbose_aware_logger(\n                \"Dependencies not met for %s, dependency '%s' FAILED: %s\",\n                self, dep_status.dep_name, dep_status.reason\n            )\n\n        if failed:\n            return False\n\n        verbose_aware_logger(\"Dependencies all met for %s\", self)\n        return True\n```"], [24, "python", "map_method", "```python\ndef map_method(method,object_list,*argseq,**kw):\n    \"\"\"map_method(method,object_list,*args,**kw) -> list\n\n    Return a list of the results of applying the methods to the items of the\n    argument sequence(s).  If more than one sequence is given, the method is\n    called with an argument list consisting of the corresponding item of each\n    sequence. All sequences must be of the same length.\n\n    Keyword arguments are passed verbatim to all objects called.\n\n    This is Python code, so it's not nearly as fast as the builtin map().\"\"\"\n\n    out_list = []\n    idx = 0\n    for object in object_list:\n        try:\n            handler = getattr(object, method)\n        except AttributeError:\n            out_list.append(None)\n        else:\n            if argseq:\n                args = map(lambda lst:lst[idx],argseq)\n                #print 'ob',object,'hand',handler,'ar',args # dbg\n                out_list.append(handler(args,**kw))\n            else:\n                out_list.append(handler(**kw))\n        idx += 1\n    return out_list\n```"], [13, "python", "Gerrit.parse_reviews", "```python\ndef parse_reviews(raw_data):\n        \"\"\"Parse a Gerrit reviews list.\"\"\"\n\n        # Join isolated reviews in JSON in array for parsing\n        items_raw = \"[\" + raw_data.replace(\"\\n\", \",\") + \"]\"\n        items_raw = items_raw.replace(\",]\", \"]\")\n        items = json.loads(items_raw)\n        reviews = []\n\n        for item in items:\n            if 'project' in item.keys():\n                reviews.append(item)\n\n        return reviews\n```"], [408, "python", "start", "```python\ndef start(dashboards, once, secrets):\n    \"\"\"Display a dashboard from the dashboard file(s) provided in the DASHBOARDS\n       Paths and/or URLs for dashboards (URLs must secrets with http or https)\n    \"\"\"\n\n    if secrets is None:\n        secrets = os.path.join(os.path.expanduser(\"~\"), \"/.doodledashboard/secrets\")\n\n    try:\n        loaded_secrets = try_read_secrets_file(secrets)\n    except InvalidSecretsException as err:\n        click.echo(get_error_message(err, default=\"Secrets file is invalid\"), err=True)\n        raise click.Abort()\n\n    read_configs = [\"\"\"\n    dashboard:\n      display:\n        type: console\n    \"\"\"]\n    for dashboard_file in dashboards:\n        read_configs.append(read_file(dashboard_file))\n\n    dashboard_config = DashboardConfigReader(initialise_component_loader(), loaded_secrets)\n\n    try:\n        dashboard = read_dashboard_from_config(dashboard_config, read_configs)\n    except YAMLError as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    try:\n        DashboardValidator().validate(dashboard)\n    except ValidationException as err:\n        click.echo(get_error_message(err, default=\"Dashboard configuration is invalid\"), err=True)\n        raise click.Abort()\n\n    explain_dashboard(dashboard)\n\n    click.echo(\"Dashboard running...\")\n\n    while True:\n        try:\n            DashboardRunner(dashboard).cycle()\n        except SecretNotFound as err:\n            click.echo(get_error_message(err, default=\"Datafeed didn't have required secret\"), err=True)\n            raise click.Abort()\n\n        if once:\n            break\n```"], [1, "python", "OAuth2RequestValidator.authenticate_client", "```python\ndef authenticate_client(self, request, *args, **kwargs):\n        \"\"\"Authenticate itself in other means.\n\n        Other means means is described in `Section 3.2.1`_.\n\n        .. _`Section 3.2.1`: http://tools.ietf.org/html/rfc6749#section-3.2.1\n        \"\"\"\n        client_id, client_secret = self._get_client_creds_from_request(request)\n        log.debug('Authenticate client %r', client_id)\n\n        client = self._clientgetter(client_id)\n        if not client:\n            log.debug('Authenticate client failed, client not found.')\n            return False\n\n        request.client = client\n\n        # http://tools.ietf.org/html/rfc6749#section-2\n        # The client MAY omit the parameter if the client secret is an empty string.\n        if hasattr(client, 'client_secret') and client.client_secret != client_secret:\n            log.debug('Authenticate client failed, secret not match.')\n            return False\n\n        log.debug('Authenticate client success.')\n        return True\n```"], [27, "python", "Trajectory.f_get_parameters", "```python\ndef f_get_parameters(self, fast_access=False, copy=True):\n        \"\"\" Returns a dictionary containing the full parameter names as keys and the parameters\n         or the parameter data items as values.\n\n\n        :param fast_access:\n\n            Determines whether the parameter objects or their values are returned\n            in the dictionary.\n\n        :param copy:\n\n            Whether the original dictionary or a shallow copy is returned.\n            If you want the real dictionary please do not modify it at all!\n            Not Copying and fast access do not work at the same time! Raises ValueError\n            if fast access is true and copy false.\n\n        :return: Dictionary containing the parameters.\n\n        :raises: ValueError\n\n        \"\"\"\n        return self._return_item_dictionary(self._parameters, fast_access, copy)\n```"], [4, "python", "load_config", "```python\ndef load_config(under_test=False, custom=None):  # pragma: no cover\n    \"\"\"\n    Load the configuration.\n\n    :param under_test:\n        Tell us if we only have to load the configuration file (True)\n        or load the configuration file and initate the output directory\n        if it does not exist (False).\n    :type under_test: bool\n\n    :param custom:\n        A dict with the configuration index (from .PyFunceble.yaml) to update.\n    :type custom: dict\n\n    .. warning::\n        If :code:`custom` is given, the given :code:`dict` overwrite\n        the last value of the given configuration indexes.\n    \"\"\"\n\n    if \"config_loaded\" not in INTERN:\n        # The configuration was not already loaded.\n\n        # We load and download the different configuration file if they are non\n        # existant.\n        Load(CURRENT_DIRECTORY)\n\n        if not under_test:\n            # If we are not under test which means that we want to save informations,\n            # we initiate the directory structure.\n            DirectoryStructure()\n\n        # We save that the configuration was loaded.\n        INTERN.update({\"config_loaded\": True})\n\n        if custom and isinstance(custom, dict):\n            # The given configuration is not None or empty.\n            # and\n            # It is a dict.\n\n            # We update the configuration index.\n            CONFIGURATION.update(custom)\n```"], [1, "python", "FourDirectionalMoveController.registerEventHandlers", "```python\ndef registerEventHandlers(self):\n        \"\"\"\n        Registers needed keybinds and schedules the :py:meth:`update` Method.\n        \n        You can control what keybinds are used via the :confval:`controls.controls.forward` etc. Configuration Values.\n        \"\"\"\n        # Forward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.forward\"],\"peng3d:actor.%s.player.controls.forward\"%self.actor.uuid,self.on_fwd_down,False)\n        # Backward\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.backward\"],\"peng3d:actor.%s.player.controls.backward\"%self.actor.uuid,self.on_bwd_down,False)\n        # Strafe Left\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.strafeleft\"],\"peng3d:actor.%s.player.controls.strafeleft\"%self.actor.uuid,self.on_left_down,False)\n        # Strafe Right\n        self.peng.keybinds.add(self.peng.cfg[\"controls.controls.straferight\"],\"peng3d:actor.%s.player.controls.straferight\"%self.actor.uuid,self.on_right_down,False)\n        pyglet.clock.schedule_interval(self.update,1.0/60)\n```"], [2, "python", "FrameIdentifierVisitor.visit_Name", "```python\ndef visit_Name(self, node):\n        \"\"\"All assignments to names go through this function.\"\"\"\n        if node.ctx == 'store':\n            self.identifiers.declared_locally.add(node.name)\n        elif node.ctx == 'param':\n            self.identifiers.declared_parameter.add(node.name)\n        elif node.ctx == 'load' and not \\\n             self.identifiers.is_declared(node.name):\n            self.identifiers.undeclared.add(node.name)\n```"], [24, "python", "Client._call", "```python\ndef _call(self, method, params=None, request_id=None):\n        \"\"\" Calls the JSON-RPC endpoint. \"\"\"\n        params = params or []\n\n        # Determines which 'id' value to use and increment the counter associated with the current\n        # client instance if applicable.\n        rid = request_id or self._id_counter\n        if request_id is None:\n            self._id_counter += 1\n\n        # Prepares the payload and the headers that will be used to forge the request.\n        payload = {'jsonrpc': '2.0', 'method': method, 'params': params, 'id': rid}\n        headers = {'Content-Type': 'application/json'}\n        scheme = 'https' if self.tls else 'http'\n        url = '{}://{}:{}'.format(scheme, self.host, self.port)\n\n        # Calls the JSON-RPC endpoint!\n        try:\n            response = self.session.post(url, headers=headers, data=json.dumps(payload))\n            response.raise_for_status()\n        except HTTPError:\n            raise TransportError(\n                'Got unsuccessful response from server (status code: {})'.format(\n                    response.status_code),\n                response=response)\n\n        # Ensures the response body can be deserialized to JSON.\n        try:\n            response_data = response.json()\n        except ValueError as e:\n            raise ProtocolError(\n                'Unable to deserialize response body: {}'.format(e), response=response)\n\n        # Properly handles potential errors.\n        if response_data.get('error'):\n            code = response_data['error'].get('code', '')\n            message = response_data['error'].get('message', '')\n            raise ProtocolError(\n                'Error[{}] {}'.format(code, message), response=response, data=response_data)\n        elif 'result' not in response_data:\n            raise ProtocolError(\n                'Response is empty (result field is missing)', response=response,\n                data=response_data)\n\n        return response_data['result']\n```"], [67, "python", "Client._update_secrets", "```python\ndef _update_secrets(self):\n        '''update secrets will update metadata needed for pull and search\n        '''\n        self.token = self._required_get_and_update('SREGISTRY_GITLAB_TOKEN')\n        self.headers[\"Private-Token\"] = self.token\n```"]]}, "_runtime": 606.853328704834, "_timestamp": 1571435627.979428, "_step": 34}
{"Examples-Validation-python": {"_type": "table", "columns": ["Rank", "Language", "Query", "Code"], "data": [[476, "python", "describe", "```python\ndef describe(lcdict, returndesc=False, offsetwith=None):\n    '''This describes the light curve object and columns present.\n\n    Parameters\n    ----------\n\n    lcdict : dict\n        The input lcdict to parse for column and metadata info.\n\n    returndesc : bool\n        If True, returns the description string as an str instead of just\n        printing it to stdout.\n\n    offsetwith : str\n        This is a character to offset the output description lines by. This is\n        useful to add comment characters like '#' to the output description\n        lines.\n\n    Returns\n    -------\n\n    str or None\n        If returndesc is True, returns the description lines as a str, otherwise\n        returns nothing.\n\n    '''\n\n    # transparently read LCC CSV format description\n    if 'lcformat' in lcdict and 'lcc-csv' in lcdict['lcformat'].lower():\n        return describe_lcc_csv(lcdict, returndesc=returndesc)\n\n\n    # figure out the columndefs part of the header string\n    columndefs = []\n\n    for colind, column in enumerate(lcdict['columns']):\n\n        if '_' in column:\n            colkey, colap = column.split('_')\n            coldesc = COLUMNDEFS[colkey][0] % colap\n        else:\n            coldesc = COLUMNDEFS[column][0]\n\n        columndefstr = '%03i - %s - %s' % (colind,\n                                           column,\n                                           coldesc)\n        columndefs.append(columndefstr)\n\n    columndefs = '\\n'.join(columndefs)\n\n    # figure out the filterdefs\n    filterdefs = []\n\n    for row in lcdict['filters']:\n\n        filterid, filtername, filterdesc = row\n        filterdefstr = '%s - %s - %s' % (filterid,\n                                         filtername,\n                                         filterdesc)\n        filterdefs.append(filterdefstr)\n\n    filterdefs = '\\n'.join(filterdefs)\n\n\n    # figure out the apertures\n    aperturedefs = []\n    for key in sorted(lcdict['lcapertures'].keys()):\n        aperturedefstr = '%s - %.2f px' % (key, lcdict['lcapertures'][key])\n        aperturedefs.append(aperturedefstr)\n\n    aperturedefs = '\\n'.join(aperturedefs)\n\n    # now fill in the description\n    description = DESCTEMPLATE.format(\n        objectid=lcdict['objectid'],\n        hatid=lcdict['objectinfo']['hatid'],\n        twomassid=lcdict['objectinfo']['twomassid'].strip(),\n        ra=lcdict['objectinfo']['ra'],\n        decl=lcdict['objectinfo']['decl'],\n        pmra=lcdict['objectinfo']['pmra'],\n        pmra_err=lcdict['objectinfo']['pmra_err'],\n        pmdecl=lcdict['objectinfo']['pmdecl'],\n        pmdecl_err=lcdict['objectinfo']['pmdecl_err'],\n        jmag=lcdict['objectinfo']['jmag'],\n        hmag=lcdict['objectinfo']['hmag'],\n        kmag=lcdict['objectinfo']['kmag'],\n        bmag=lcdict['objectinfo']['bmag'],\n        vmag=lcdict['objectinfo']['vmag'],\n        sdssg=lcdict['objectinfo']['sdssg'],\n        sdssr=lcdict['objectinfo']['sdssr'],\n        sdssi=lcdict['objectinfo']['sdssi'],\n        ndet=lcdict['objectinfo']['ndet'],\n        lcsortcol=lcdict['lcsortcol'],\n        lcbestaperture=json.dumps(lcdict['lcbestaperture'],ensure_ascii=True),\n        network=lcdict['objectinfo']['network'],\n        stations=lcdict['objectinfo']['stations'],\n        lastupdated=lcdict['lastupdated'],\n        datarelease=lcdict['datarelease'],\n        lcversion=lcdict['lcversion'],\n        lcserver=lcdict['lcserver'],\n        comment=lcdict['comment'],\n        lcfiltersql=(lcdict['lcfiltersql'] if 'lcfiltersql' in lcdict else ''),\n        lcnormcols=(lcdict['lcnormcols'] if 'lcnormcols' in lcdict else ''),\n        filterdefs=filterdefs,\n        columndefs=columndefs,\n        aperturedefs=aperturedefs\n    )\n\n    if offsetwith is not None:\n        description = textwrap.indent(\n            description,\n            '%s ' % offsetwith,\n            lambda line: True\n        )\n        print(description)\n    else:\n        print(description)\n\n    if returndesc:\n        return description\n```"], [8, "python", "Client.append_stream", "```python\ndef append_stream(self, destination, *, offset=0):\n        \"\"\"\n        Create stream for append (write) data to `destination` file.\n\n        :param destination: destination path of file on server side\n        :type destination: :py:class:`str` or :py:class:`pathlib.PurePosixPath`\n\n        :param offset: byte offset for stream start position\n        :type offset: :py:class:`int`\n\n        :rtype: :py:class:`aioftp.DataConnectionThrottleStreamIO`\n        \"\"\"\n        return self.get_stream(\n            \"APPE \" + str(destination),\n            \"1xx\",\n            offset=offset,\n        )\n```"], [1, "python", "Manager.get_droplet_snapshots", "```python\ndef get_droplet_snapshots(self):\n        \"\"\"\n            This method returns a list of all Snapshots based on Droplets.\n        \"\"\"\n        data = self.get_data(\"snapshots?resource_type=droplet\")\n        return [\n            Snapshot(token=self.token, **snapshot)\n            for snapshot in data['snapshots']\n        ]\n```"], [168, "python", "prepare_upload_bundle", "```python\ndef prepare_upload_bundle(name, data):\n    \"\"\"GeoServer's REST API uses ZIP archives as containers for file formats such\n    as Shapefile and WorldImage which include several 'boxcar' files alongside\n    the main data.  In such archives, GeoServer assumes that all of the relevant\n    files will have the same base name and appropriate extensions, and live in\n    the root of the ZIP archive.  This method produces a zip file that matches\n    these expectations, based on a basename, and a dict of extensions to paths or\n    file-like objects. The client code is responsible for deleting the zip\n    archive when it's done.\"\"\"\n    fd, path = mkstemp()\n    zip_file = ZipFile(path, 'w')\n    for ext, stream in data.items():\n        fname = \"%s.%s\" % (name, ext)\n        if (isinstance(stream, basestring)):\n            zip_file.write(stream, fname)\n        else:\n            zip_file.writestr(fname, stream.read())\n    zip_file.close()\n    os.close(fd)\n    return path\n```"], [203, "python", "IndexBuilder.freeze", "```python\ndef freeze(self):\n        \"\"\"Create a usable data structure for serializing.\"\"\"\n        data = super(IndexBuilder, self).freeze()\n        try:\n            # Sphinx >= 1.5 format\n            # Due to changes from github.com/sphinx-doc/sphinx/pull/2454\n            base_file_names = data['docnames']\n        except KeyError:\n            # Sphinx < 1.5 format\n            base_file_names = data['filenames']\n\n        store = {}\n        c = itertools.count()\n        for prefix, items in iteritems(data['objects']):\n            for name, (index, typeindex, _, shortanchor) in iteritems(items):\n                objtype = data['objtypes'][typeindex]\n                if objtype.startswith('cpp:'):\n                    split =  name.rsplit('::', 1)\n                    if len(split) != 2:\n                        warnings.warn(\"What's up with %s?\" % str((prefix, name, objtype)))\n                        continue\n                    prefix, name = split\n                    last_prefix = prefix.split('::')[-1]\n                else:\n                    last_prefix = prefix.split('.')[-1]\n\n                store[next(c)] = {\n                    'filename': base_file_names[index],\n                    'objtype': objtype,\n                    'prefix': prefix,\n                    'last_prefix': last_prefix,\n                    'name': name,\n                    'shortanchor': shortanchor,\n                }\n\n        data.update({'store': store})\n        return data\n```"], [86, "python", "Draw.bezier", "```python\ndef bezier(self, points):\n        \"\"\"Draw a Bezier-curve.\n\n        :param points: ex.) ((5, 5), (6, 6), (7, 7))\n        :type points: list\n        \"\"\"\n        coordinates = pgmagick.CoordinateList()\n        for point in points:\n            x, y = float(point[0]), float(point[1])\n            coordinates.append(pgmagick.Coordinate(x, y))\n        self.drawer.append(pgmagick.DrawableBezier(coordinates))\n```"], [1, "python", "DeleteFile", "```python\ndef DeleteFile(target_filename):\n    '''\n    Deletes the given local filename.\n\n    .. note:: If file doesn't exist this method has no effect.\n\n    :param unicode target_filename:\n        A local filename\n\n    :raises NotImplementedForRemotePathError:\n        If trying to delete a non-local path\n\n    :raises FileOnlyActionError:\n        Raised when filename refers to a directory.\n    '''\n    _AssertIsLocal(target_filename)\n\n    try:\n        if IsLink(target_filename):\n            DeleteLink(target_filename)\n        elif IsFile(target_filename):\n            os.remove(target_filename)\n        elif IsDir(target_filename):\n            from ._exceptions import FileOnlyActionError\n            raise FileOnlyActionError(target_filename)\n    except Exception as e:\n        reraise(e, 'While executing filesystem.DeleteFile(%s)' % (target_filename))\n```"], [64, "python", "WechatExt.get_group_list", "```python\ndef get_group_list(self):\n        \"\"\"\n        \u83b7\u53d6\u5206\u7ec4\u5217\u8868\n\n        \u8fd4\u56deJSON\u793a\u4f8b::\n\n            {\n                \"groups\": [\n                    {\n                        \"cnt\": 8,\n                        \"id\": 0,\n                        \"name\": \"\u672a\u5206\u7ec4\"\n                    },\n                    {\n                        \"cnt\": 0,\n                        \"id\": 1,\n                        \"name\": \"\u9ed1\u540d\u5355\"\n                    },\n                    {\n                        \"cnt\": 0,\n                        \"id\": 2,\n                        \"name\": \"\u661f\u6807\u7ec4\"\n                    }\n                ]\n            }\n\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\n        :raises NeedLoginError: \u64cd\u4f5c\u672a\u6267\u884c\u6210\u529f, \u9700\u8981\u518d\u6b21\u5c1d\u8bd5\u767b\u5f55, \u5f02\u5e38\u5185\u5bb9\u4e3a\u670d\u52a1\u5668\u8fd4\u56de\u7684\u9519\u8bef\u6570\u636e\n        \"\"\"\n        url = 'https://mp.weixin.qq.com/cgi-bin/contactmanage?t=user/index&pagesize=10&pageidx=0&type=0&groupid=0&lang=zh_CN&f=json&token={token}'.format(\n            token=self.__token,\n        )\n        headers = {\n            'x-requested-with': 'XMLHttpRequest',\n            'referer': 'https://mp.weixin.qq.com/cgi-bin/contactmanage?t=user/index&pagesize=10&pageidx=0&type=0&groupid=0&lang=zh_CN&token='.format(\n                token=self.__token,\n            ),\n            'cookie': self.__cookies,\n        }\n        r = requests.get(url, headers=headers)\n\n        try:\n            message = json.loads(r.text)['group_list']\n        except (KeyError, ValueError):\n            raise NeedLoginError(r.text)\n\n        return message\n```"], [7, "python", "activate", "```python\ndef activate(paths, skip_local, skip_shared):\n    '''Activate an environment'''\n\n\n    if not paths:\n        ctx = click.get_current_context()\n        if cpenv.get_active_env():\n            ctx.invoke(info)\n            return\n\n        click.echo(ctx.get_help())\n        examples = (\n            '\\nExamples: \\n'\n            '    cpenv activate my_env\\n'\n            '    cpenv activate ./relative/path/to/my_env\\n'\n            '    cpenv activate my_env my_module\\n'\n        )\n        click.echo(examples)\n        return\n\n    if skip_local:\n        cpenv.module_resolvers.remove(cpenv.resolver.module_resolver)\n        cpenv.module_resolvers.remove(cpenv.resolver.active_env_module_resolver)\n\n    if skip_shared:\n        cpenv.module_resolvers.remove(cpenv.resolver.modules_path_resolver)\n\n    try:\n        r = cpenv.resolve(*paths)\n    except cpenv.ResolveError as e:\n        click.echo('\\n' + str(e))\n        return\n\n    resolved = set(r.resolved)\n    active_modules = set()\n    env = cpenv.get_active_env()\n    if env:\n        active_modules.add(env)\n    active_modules.update(cpenv.get_active_modules())\n\n    new_modules = resolved - active_modules\n    old_modules = active_modules & resolved\n\n    if old_modules and not new_modules:\n        click.echo(\n            '\\nModules already active: '\n            + bold(' '.join([obj.name for obj in old_modules]))\n        )\n        return\n\n    if env and contains_env(new_modules):\n        click.echo('\\nUse bold(exit) to leave your active environment first.')\n        return\n\n    click.echo('\\nResolved the following modules...')\n    click.echo(format_objects(r.resolved))\n    r.activate()\n    click.echo(blue('\\nLaunching subshell...'))\n\n    modules = sorted(resolved | active_modules, key=_type_and_name)\n    prompt = ':'.join([obj.name for obj in modules])\n    shell.launch(prompt)\n```"], [48, "python", "create", "```python\ndef create(name_or_path, config):\n    '''Create a new environment.'''\n\n    if not name_or_path:\n        ctx = click.get_current_context()\n        click.echo(ctx.get_help())\n        examples = (\n            '\\nExamples:\\n'\n            '    cpenv create my_env\\n'\n            '    cpenv create ./relative/path/to/my_env\\n'\n            '    cpenv create my_env --config ./relative/path/to/config\\n'\n            '    cpenv create my_env --config git@github.com:user/config.git\\n'\n        )\n        click.echo(examples)\n        return\n\n    click.echo(\n        blue('Creating a new virtual environment ' + name_or_path)\n    )\n    try:\n        env = cpenv.create(name_or_path, config)\n    except Exception as e:\n        click.echo(bold_red('FAILED TO CREATE ENVIRONMENT!'))\n        click.echo(e)\n    else:\n        click.echo(bold_green('Successfully created environment!'))\n    click.echo(blue('Launching subshell'))\n\n    cpenv.activate(env)\n    shell.launch(env.name)\n```"], [199, "python", "Parser.p_pkg_down_value_1", "```python\ndef p_pkg_down_value_1(self, p):\n        \"\"\"pkg_down_value : LINE \"\"\"\n        if six.PY2:\n            p[0] = p[1].decode(encoding='utf-8')\n        else:\n            p[0] = p[1]\n```"], [149, "python", "get_type", "```python\nasync def get_type(media, path=None):\n    \"\"\"\n    Parameters\n    ----------\n    media : file object\n        A file object of the image\n    path : str, optional\n        The path to the file\n\n    Returns\n    -------\n    str\n        The mimetype of the media\n    str\n        The category of the media on Twitter\n    \"\"\"\n    if magic:\n        if not media:\n            raise TypeError(\"Media data is empty\")\n\n        _logger.debug(\"guessing mimetype using magic\")\n        media_type = mime.from_buffer(media[:1024])\n    else:\n        media_type = None\n        if path:\n            _logger.debug(\"guessing mimetype using built-in module\")\n            media_type = mime.guess_type(path)[0]\n\n        if media_type is None:\n            msg = (\"Could not guess the mimetype of the media.\\n\"\n                   \"Please consider installing python-magic\\n\"\n                   \"(pip3 install peony-twitter[magic])\")\n            raise RuntimeError(msg)\n\n    return media_type\n```"], [73, "python", "AlignmentPropertyMatrix.bundle", "```python\ndef bundle(self, reset=False, shallow=False): # Copies the original matrix (Use lots of memory)\r\n        \"\"\"\r\n        Returns ``AlignmentPropertyMatrix`` object in which loci are bundled using grouping information.\r\n\r\n        :param reset: whether to reset the values at the loci\r\n        :param shallow: whether to copy all the meta data\r\n        \"\"\"\r\n        if self.finalized:\r\n            # if self.num_groups > 0:\r\n            if self.groups is not None and self.gname is not None:\r\n                grp_conv_mat = lil_matrix((self.num_loci, self.num_groups))\r\n                for i in xrange(self.num_groups):\r\n                    grp_conv_mat[self.groups[i], i] = 1.0\r\n                grp_align = Sparse3DMatrix.__mul__(self, grp_conv_mat) # The core of the bundling\r\n                grp_align.num_loci = self.num_groups\r\n                grp_align.num_haplotypes = self.num_haplotypes\r\n                grp_align.num_reads = self.num_reads\r\n                grp_align.shape = (grp_align.num_loci, grp_align.num_haplotypes, grp_align.num_reads)\r\n                if not shallow:\r\n                    grp_align.lname = copy.copy(self.gname)\r\n                    grp_align.hname = self.hname\r\n                    grp_align.rname = copy.copy(self.rname)\r\n                    grp_align.lid   = dict(zip(grp_align.lname, np.arange(grp_align.num_loci)))\r\n                    grp_align.rid   = copy.copy(self.rid)\r\n                if reset:\r\n                    grp_align.reset()\r\n                return grp_align\r\n            else:\r\n                raise RuntimeError('No group information is available for bundling.')\r\n        else:\r\n            raise RuntimeError('The matrix is not finalized.')\n```"], [106, "python", "Townsend_Hales", "```python\ndef Townsend_Hales(T, Tc, Vc, omega):\n    r'''Calculates saturation liquid density, using the Townsend and Hales\n    CSP method as modified from the original Riedel equation. Uses\n    chemical critical volume and temperature, as well as acentric factor\n\n    The density of a liquid is given by:\n\n    .. math::\n        Vs = V_c/\\left(1+0.85(1-T_r)+(1.692+0.986\\omega)(1-T_r)^{1/3}\\right)\n\n    Parameters\n    ----------\n    T : float\n        Temperature of fluid [K]\n    Tc : float\n        Critical temperature of fluid [K]\n    Vc : float\n        Critical volume of fluid [m^3/mol]\n    omega : float\n        Acentric factor for fluid, [-]\n\n    Returns\n    -------\n    Vs : float\n        Saturation liquid volume, [m^3/mol]\n\n    Notes\n    -----\n    The requirement for critical volume and acentric factor requires all data.\n\n    Examples\n    --------\n    >>> Townsend_Hales(300, 647.14, 55.95E-6, 0.3449)\n    1.8007361992619923e-05\n\n    References\n    ----------\n    .. [1] Hales, J. L, and R Townsend. \"Liquid Densities from 293 to 490 K of\n       Nine Aromatic Hydrocarbons.\" The Journal of Chemical Thermodynamics\n       4, no. 5 (1972): 763-72. doi:10.1016/0021-9614(72)90050-X\n    '''\n    Tr = T/Tc\n    return Vc/(1 + 0.85*(1-Tr) + (1.692 + 0.986*omega)*(1-Tr)**(1/3.))\n```"], [6, "python", "_nbytes", "```python\ndef _nbytes(buf):\n    \"\"\"Return byte-size of a memoryview or buffer.\"\"\"\n    if isinstance(buf, memoryview):\n        if PY3:\n            # py3 introduces nbytes attribute\n            return buf.nbytes\n        else:\n            # compute nbytes on py2\n            size = buf.itemsize\n            for dim in buf.shape:\n                size *= dim\n            return size\n    else:\n        # not a memoryview, raw bytes/ py2 buffer\n        return len(buf)\n```"], [22, "python", "update", "```python\ndef update(x, **entries):\n    \"\"\"Update a dict, or an object with slots, according to `entries` dict.\n\n    >>> update({'a': 1}, a=10, b=20)\n    {'a': 10, 'b': 20}\n    >>> update(Struct(a=1), a=10, b=20)\n    Struct(a=10, b=20)\n    \"\"\"\n    if isinstance(x, dict):\n        x.update(entries)\n    else:\n        x.__dict__.update(entries)\n    return x\n```"], [2, "python", "Encoder.getDecoderOutputFieldTypes", "```python\ndef getDecoderOutputFieldTypes(self):\n    \"\"\"\n    Returns a sequence of field types corresponding to the elements in the\n    decoded output field array.  The types are defined by\n    :class:`~nupic.data.field_meta.FieldMetaType`.\n\n    :return: list of :class:`~nupic.data.field_meta.FieldMetaType` objects\n    \"\"\"\n    if hasattr(self, '_flattenedFieldTypeList') and \\\n          self._flattenedFieldTypeList is not None:\n      return self._flattenedFieldTypeList\n\n    fieldTypes = []\n\n    # NOTE: we take care of the composites, but leaf encoders must override\n    #       this method and return a list of one field_meta.FieldMetaType.XXXX\n    #       element corresponding to the encoder's decoder output field type\n    for (name, encoder, offset) in self.encoders:\n      subTypes = encoder.getDecoderOutputFieldTypes()\n      fieldTypes.extend(subTypes)\n\n    self._flattenedFieldTypeList = fieldTypes\n    return fieldTypes\n```"], [10, "python", "Instruction.parse_operand", "```python\ndef parse_operand(self, buf):\n        \"\"\" Parses an operand from buf\n\n            :param buf: a buffer\n            :type buf: iterator/generator/string\n        \"\"\"\n        buf = iter(buf)\n        try:\n            operand = 0\n            for _ in range(self.operand_size):\n                operand <<= 8\n                operand |= next(buf)\n            self._operand = operand\n        except StopIteration:\n            raise ParseError(\"Not enough data for decoding\")\n```"], [356, "python", "_BaseFile._updateType", "```python\ndef _updateType(self):\n        \"\"\"Make sure that the class behaves like the data structure that it\n        is, so that we don't get a ListFile trying to represent a dict.\"\"\"\n        data = self._data()\n        # Change type if needed\n        if isinstance(data, dict) and isinstance(self, ListFile):\n            self.__class__ = DictFile\n        elif isinstance(data, list) and isinstance(self, DictFile):\n            self.__class__ = ListFile\n```"], [1, "python", "LsstLatexDoc._parse_documentclass", "```python\ndef _parse_documentclass(self):\n        \"\"\"Parse documentclass options.\n\n        Sets the the ``_document_options`` attribute.\n        \"\"\"\n        command = LatexCommand(\n            'documentclass',\n            {'name': 'options', 'required': False, 'bracket': '['},\n            {'name': 'class_name', 'required': True, 'bracket': '{'})\n        try:\n            parsed = next(command.parse(self._tex))\n        except StopIteration:\n            self._logger.warning('lsstdoc has no documentclass')\n            self._document_options = []\n\n        try:\n            content = parsed['options']\n            self._document_options = [opt.strip()\n                                      for opt in content.split(',')]\n        except KeyError:\n            self._logger.warning('lsstdoc has no documentclass options')\n            self._document_options = []\n```"], [1, "python", "reshape_for_broadcasting", "```python\ndef reshape_for_broadcasting(source, target):\n    \"\"\"Reshapes a tensor (source) to have the correct shape and dtype of the target\n    before broadcasting it with MPI.\n    \"\"\"\n    dim = len(target.get_shape())\n    shape = ([1] * (dim - 1)) + [-1]\n    return tf.reshape(tf.cast(source, target.dtype), shape)\n```"], [1, "python", "PathFilters.select_by_pattern_in_abspath", "```python\ndef select_by_pattern_in_abspath(self,\n                                     pattern,\n                                     recursive=True,\n                                     case_sensitive=False):\n        \"\"\"\n        Select file path by text pattern in absolute path.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u9009\u62e9\u7edd\u5bf9\u8def\u5f84\u4e2d\u5305\u542b\u6307\u5b9a\u5b50\u5b57\u7b26\u4e32\u7684\u6587\u4ef6\u3002\n        \"\"\"\n        if case_sensitive:\n            def filters(p):\n                return pattern in p.abspath\n        else:\n            pattern = pattern.lower()\n\n            def filters(p):\n                return pattern in p.abspath.lower()\n\n        return self.select_file(filters, recursive)\n```"], [282, "python", "Reaction._update_awareness", "```python\ndef _update_awareness(self):\n        \"\"\"Make sure all metabolites and genes that are associated with\n        this reaction are aware of it.\n\n        \"\"\"\n        for x in self._metabolites:\n            x._reaction.add(self)\n        for x in self._genes:\n            x._reaction.add(self)\n```"], [151, "python", "REST.conference_list", "```python\ndef conference_list(self, call_params):\n        \"\"\"REST Conference List Helper\n        \"\"\"\n        path = '/' + self.api_version + '/ConferenceList/'\n        method = 'POST'\n        return self.request(path, method, call_params)\n```"], [1, "python", "Timer.stop", "```python\ndef stop(self) -> float:\n        \"\"\"\n        Stop the timer\n\n        Returns:\n            The time the timer was stopped\n        \"\"\"\n        self.stop_time = time.time()\n        return self.stop_time - self.start_time - self.offset\n```"], [930, "python", "X86Cpu.ADD", "```python\ndef ADD(cpu, dest, src):\n        \"\"\"\n        Add.\n\n        Adds the first operand (destination operand) and the second operand (source operand)\n        and stores the result in the destination operand. When an immediate value is used as\n        an operand, it is sign-extended to the length of the destination operand format.\n        The ADD instruction does not distinguish between signed or unsigned operands. Instead,\n        the processor evaluates the result for both data types and sets the OF and CF flags to\n        indicate a carry in the signed or unsigned result, respectively. The SF flag indicates\n        the sign of the signed result::\n\n                DEST  =  DEST + SRC;\n\n        :param cpu: current CPU.\n        :param dest: destination operand.\n        :param src: source operand.\n        \"\"\"\n        cpu._ADD(dest, src, carry=False)\n```"], [266, "python", "DebugSatchel.list_server_specs", "```python\ndef list_server_specs(self, cpu=1, memory=1, hdd=1):\n        \"\"\"\n        Displays a list of common servers characteristics, like number\n        of CPU cores, amount of memory and hard drive capacity.\n        \"\"\"\n        r = self.local_renderer\n\n        cpu = int(cpu)\n        memory = int(memory)\n        hdd = int(hdd)\n\n        # CPU\n        if cpu:\n            cmd = 'cat /proc/cpuinfo | grep -i \"model name\"'\n            ret = r.run(cmd)\n            matches = map(str.strip, re.findall(r'model name\\s+:\\s*([^\\n]+)', ret, re.DOTALL|re.I))\n            cores = {}\n            for match in matches:\n                cores.setdefault(match, 0)\n                cores[match] += 1\n\n        # Memory\n        if memory:\n            cmd = 'dmidecode --type 17'\n            ret = r.sudo(cmd)\n            #print repr(ret)\n            matches = re.findall(r'Memory\\s+Device\\r\\n(.*?)(?:\\r\\n\\r\\n|$)', ret, flags=re.DOTALL|re.I)\n            #print len(matches)\n            #print matches[0]\n            memory_slot_dicts = []\n            for match in matches:\n                attrs = dict([(_a.strip(), _b.strip()) for _a, _b in re.findall(r'^([^:]+):\\s+(.*)$', match, flags=re.MULTILINE)])\n                #print attrs\n                memory_slot_dicts.append(attrs)\n            total_memory_gb = 0\n            total_slots_filled = 0\n            total_slots = len(memory_slot_dicts)\n            memory_types = set()\n            memory_forms = set()\n            memory_speeds = set()\n            for memory_dict in memory_slot_dicts:\n                try:\n                    size = int(round(float(re.findall(r'([0-9]+)\\s+MB', memory_dict['Size'])[0])/1024.))\n                    #print size\n                    total_memory_gb += size\n                    total_slots_filled += 1\n                except IndexError:\n                    pass\n                _v = memory_dict['Type']\n                if _v != 'Unknown':\n                    memory_types.add(_v)\n                _v = memory_dict['Form Factor']\n                if _v != 'Unknown':\n                    memory_forms.add(_v)\n                #_v = memory_dict['Speed']\n                #if _v != 'Unknown':\n                    #memory_speeds.add(_v)\n\n        # Storage\n        if hdd:\n            #cmd = 'ls /dev/*d* | grep \"/dev/[a-z]+d[a-z]$\"'\n            cmd = 'find /dev -maxdepth 1 | grep -E \"/dev/[a-z]+d[a-z]$\"'\n            devices = map(str.strip, r.run(cmd).split('\\n'))\n            total_drives = len(devices)\n            total_physical_storage_gb = 0\n            total_logical_storage_gb = 0\n            drive_transports = set()\n            for device in devices:\n                #cmd = 'udisks --show-info %s |grep -i \"  size:\"' % (device)\n                cmd = 'udisksctl info -b %s |grep -i \"  size:\"' % (device)\n                ret = r.run(cmd)\n                size_bytes = float(re.findall(r'size:\\s*([0-9]+)', ret, flags=re.I)[0].strip())\n                size_gb = int(round(size_bytes/1024/1024/1024))\n                #print device, size_gb\n                total_physical_storage_gb += size_gb\n\n                with self.settings(warn_only=True):\n                    cmd = 'hdparm -I %s|grep -i \"Transport:\"' % device\n                    ret = self.sudo(cmd)\n                    if ret and not ret.return_code:\n                        drive_transports.add(ret.split('Transport:')[-1].strip())\n\n            cmd = \"df | grep '^/dev/[mhs]d*' | awk '{{s+=$2}} END {{print s/1048576}}'\"\n            ret = r.run(cmd)\n            total_logical_storage_gb = float(ret)\n\n        if cpu:\n            print('-'*80)\n            print('CPU')\n            print('-'*80)\n            type_str = ', '.join(['%s x %i' % (_type, _count) for _type, _count in cores.items()])\n            print('Cores: %i' % sum(cores.values()))\n            print('Types: %s' % type_str)\n\n        if memory:\n            print('-'*80)\n            print('MEMORY')\n            print('-'*80)\n            print('Total: %s GB' % total_memory_gb)\n            print('Type: %s' % list_to_str_or_unknown(memory_types))\n            print('Form: %s' % list_to_str_or_unknown(memory_forms))\n            print('Speed: %s' % list_to_str_or_unknown(memory_speeds))\n            print('Slots: %i (%i filled, %i empty)' % (total_slots, total_slots_filled, total_slots - total_slots_filled))\n\n        if hdd:\n            print('-'*80)\n            print('STORAGE')\n            print('-'*80)\n            print('Total physical drives: %i' % total_drives)\n            print('Total physical storage: %s GB' % total_physical_storage_gb)\n            print('Total logical storage: %s GB' % total_logical_storage_gb)\n            print('Types: %s' % list_to_str_or_unknown(drive_transports))\n```"], [90, "python", "Controller.start", "```python\ndef start(self):\n        \"\"\"Start the controller.\"\"\"\n\n        if self.mode == \"manual\":\n            return\n\n        if self.ipython_dir != '~/.ipython':\n            self.ipython_dir = os.path.abspath(os.path.expanduser(self.ipython_dir))\n\n        if self.log:\n            stdout = open(os.path.join(self.ipython_dir, \"{0}.controller.out\".format(self.profile)), 'w')\n            stderr = open(os.path.join(self.ipython_dir, \"{0}.controller.err\".format(self.profile)), 'w')\n        else:\n            stdout = open(os.devnull, 'w')\n            stderr = open(os.devnull, 'w')\n\n        try:\n            opts = [\n                'ipcontroller',\n                '' if self.ipython_dir == '~/.ipython' else '--ipython-dir={}'.format(self.ipython_dir),\n                self.interfaces if self.interfaces is not None else '--ip=*',\n                '' if self.profile == 'default' else '--profile={0}'.format(self.profile),\n                '--reuse' if self.reuse else '',\n                '--location={}'.format(self.public_ip) if self.public_ip else '',\n                '--port={}'.format(self.port) if self.port is not None else ''\n            ]\n            if self.port_range is not None:\n                opts += [\n                    '--HubFactory.hb={0},{1}'.format(self.hb_ping, self.hb_pong),\n                    '--HubFactory.control={0},{1}'.format(self.control_client, self.control_engine),\n                    '--HubFactory.mux={0},{1}'.format(self.mux_client, self.mux_engine),\n                    '--HubFactory.task={0},{1}'.format(self.task_client, self.task_engine)\n                ]\n            logger.debug(\"Starting ipcontroller with '{}'\".format(' '.join([str(x) for x in opts])))\n            self.proc = subprocess.Popen(opts, stdout=stdout, stderr=stderr, preexec_fn=os.setsid)\n        except FileNotFoundError:\n            msg = \"Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env\"\n            logger.error(msg)\n            raise ControllerError(msg)\n        except Exception as e:\n            msg = \"IPPController failed to start: {0}\".format(e)\n            logger.error(msg)\n            raise ControllerError(msg)\n```"], [3, "python", "Resource.launch", "```python\ndef launch(self, workflow_job_template=None, monitor=False, wait=False,\n               timeout=None, extra_vars=None, **kwargs):\n        \"\"\"Launch a new workflow job based on a workflow job template.\n\n        Creates a new workflow job in Ansible Tower, starts it, and\n        returns back an ID in order for its status to be monitored.\n\n        =====API DOCS=====\n        Launch a new workflow job based on a workflow job template.\n\n        :param workflow_job_template: Primary key or name of the workflow job template to launch new job.\n        :type workflow_job_template: str\n        :param monitor: Flag that if set, immediately calls ``monitor`` on the newly launched workflow job rather\n                        than exiting with a success.\n        :type monitor: bool\n        :param wait: Flag that if set, monitor the status of the workflow job, but do not print while job is\n                     in progress.\n        :type wait: bool\n        :param timeout: If provided with ``monitor`` flag set, this attempt will time out after the given number\n                        of seconds.\n        :type timeout: int\n        :param extra_vars: yaml formatted texts that contains extra variables to pass on.\n        :type extra_vars: array of strings\n        :param `**kwargs`: Fields needed to create and launch a workflow job.\n        :returns: Result of subsequent ``monitor`` call if ``monitor`` flag is on; Result of subsequent ``wait``\n                  call if ``wait`` flag is on; loaded JSON output of the job launch if none of the two flags are on.\n        :rtype: dict\n\n        =====API DOCS=====\n        \"\"\"\n        if extra_vars is not None and len(extra_vars) > 0:\n            kwargs['extra_vars'] = parser.process_extra_vars(extra_vars)\n\n        debug.log('Launching the workflow job.', header='details')\n        self._pop_none(kwargs)\n        post_response = client.post('workflow_job_templates/{0}/launch/'.format(\n            workflow_job_template), data=kwargs).json()\n\n        workflow_job_id = post_response['id']\n        post_response['changed'] = True\n\n        if monitor:\n            return self.monitor(workflow_job_id, timeout=timeout)\n        elif wait:\n            return self.wait(workflow_job_id, timeout=timeout)\n\n        return post_response\n```"], [3, "python", "color_scale", "```python\ndef color_scale(color, level):\n    \"\"\"\n    Scale RGB tuple by level, 0 - 256\n    \"\"\"\n    return tuple([int(i * level) >> 8 for i in list(color)])\n```"], [115, "python", "EVM.CALLDATALOAD", "```python\ndef CALLDATALOAD(self, offset):\n        \"\"\"Get input data of current environment\"\"\"\n\n        if issymbolic(offset):\n            if solver.can_be_true(self._constraints, offset == self._used_calldata_size):\n                self.constraints.add(offset == self._used_calldata_size)\n            raise ConcretizeArgument(1, policy='SAMPLED')\n\n        self._use_calldata(offset, 32)\n\n        data_length = len(self.data)\n\n        bytes = []\n        for i in range(32):\n            try:\n                c = Operators.ITEBV(8, offset + i < data_length, self.data[offset + i], 0)\n            except IndexError:\n                # offset + i is concrete and outside data\n                c = 0\n\n            bytes.append(c)\n        return Operators.CONCAT(256, *bytes)\n```"], [2, "python", "cornice_enable_openapi_view", "```python\ndef cornice_enable_openapi_view(\n        config,\n        api_path='/api-explorer/swagger.json',\n        permission=NO_PERMISSION_REQUIRED,\n        route_factory=None, **kwargs):\n    \"\"\"\n    :param config:\n        Pyramid configurator object\n    :param api_path:\n        where to expose swagger JSON definition view\n    :param permission:\n        pyramid permission for those views\n    :param route_factory:\n        factory for context object for those routes\n    :param kwargs:\n        kwargs that will be passed to CorniceSwagger's `generate()`\n\n    This registers and configures the view that serves api definitions\n    \"\"\"\n    config.registry.settings['cornice_swagger.spec_kwargs'] = kwargs\n    config.add_route('cornice_swagger.open_api_path', api_path,\n                     factory=route_factory)\n    config.add_view('cornice_swagger.views.open_api_json_view',\n                    renderer='json', permission=permission,\n                    route_name='cornice_swagger.open_api_path')\n```"], [123, "python", "tui.shorthelp", "```python\ndef shorthelp(self, width=0):\n        \"\"\"Return brief help containing Title and usage instructions.\n        ARGS:\n        width = 0 <int>:\n            Maximum allowed page width. 0 means use default from\n            self.iMaxHelpWidth.\n\n        \"\"\"\n        out = []\n        out.append(self._wrap(self.docs['title'], width=width))\n        if self.docs['description']:\n            out.append(self._wrap(self.docs['description'], indent=2, width=width))\n        out.append('')\n        out.append(self._wrapusage(width=width))\n        out.append('')\n        return '\\n'.join(out)\n```"], [148, "python", "RecurringScheduleComponent.daily_periods", "```python\ndef daily_periods(self, range_start=datetime.date.min, range_end=datetime.date.max, exclude_dates=tuple()):\n        \"\"\"Returns an iterator of Period tuples for every day this schedule is in effect, between range_start\n        and range_end.\"\"\"\n        tz = self.timezone\n        period = self.period\n        weekdays = self.weekdays\n\n        current_date = max(range_start, self.start_date)\n        end_date = range_end\n        if self.end_date:\n            end_date = min(end_date, self.end_date)\n\n        while current_date <= end_date:\n            if current_date.weekday() in weekdays and current_date not in exclude_dates:\n                yield Period(\n                    tz.localize(datetime.datetime.combine(current_date, period.start)),\n                    tz.localize(datetime.datetime.combine(current_date, period.end))\n                )\n            current_date += datetime.timedelta(days=1)\n```"], [1, "python", "Connection.get_headers", "```python\ndef get_headers(self):\n        \"\"\" Get headers.\n\n        Returns:\n            tuple: Headers\n        \"\"\"\n        headers = {\n            \"User-Agent\": \"kFlame 1.0\"\n        }\n\n        password_url = self._get_password_url()\n        if password_url and password_url in self._settings[\"authorizations\"]:\n            headers[\"Authorization\"] = self._settings[\"authorizations\"][password_url]\n\n        return headers\n```"], [530, "python", "CQHttp.set_group_whole_ban", "```python\ndef set_group_whole_ban(self, *, group_id, enable=True):\n        \"\"\"\n        \u7fa4\u7ec4\u5168\u5458\u7981\u8a00\n\n        ------------\n\n        :param int group_id: \u7fa4\u53f7\n        :param bool enable: \u662f\u5426\u7981\u8a00\n        :return: None\n        :rtype: None\n        \"\"\"\n        return super().__getattr__('set_group_whole_ban') \\\n            (group_id=group_id, enable=enable)\n```"], [307, "python", "MemSizeLRUCache.delete", "```python\ndef delete(self, key):\n        \"\"\"\n        >>> c = MemSizeLRUCache()\n        >>> c.put(1, 1)\n        >>> c.mem()\n        24\n        >>> c.delete(1)\n        >>> c.mem()\n        0\n        \"\"\"\n        (_value, mem) = LRUCache.get(self, key)\n        self._mem -= mem\n        LRUCache.delete(self, key)\n```"], [114, "python", "phase_identification_parameter_phase", "```python\ndef phase_identification_parameter_phase(d2P_dVdT, V=None, dP_dT=None, dP_dV=None, d2P_dV2=None):\n    r'''Uses the Phase Identification Parameter concept developed in [1]_ and \n    [2]_ to determine if a chemical is a solid, liquid, or vapor given the \n    appropriate thermodynamic conditions.\n\n    The criteria for liquid is PIP > 1; for vapor, PIP <= 1.\n\n    For solids, PIP(solid) is defined to be d2P_dVdT. If it is larger than 0, \n    the species is a solid. It is less than 0 for all liquids and gases.\n\n    Parameters\n    ----------\n    d2P_dVdT : float\n        Second derivative of `P` with respect to both `V` and `T`, [Pa*mol/m^3/K]\n    V : float, optional\n        Molar volume at `T` and `P`, [m^3/mol]\n    dP_dT : float, optional\n        Derivative of `P` with respect to `T`, [Pa/K]\n    dP_dV : float, optional\n        Derivative of `P` with respect to `V`, [Pa*mol/m^3]\n    d2P_dV2 : float, optionsl\n        Second derivative of `P` with respect to `V`, [Pa*mol^2/m^6]\n\n    Returns\n    -------\n    phase : str\n        Either 's', 'l' or 'g'\n    \n    Notes\n    -----\n    The criteria for being a solid phase is checked first, which only\n    requires d2P_dVdT. All other inputs are optional for this reason.\n    However, an exception will be raised if the other inputs become \n    needed to determine if a species is a liquid or a gas.\n        \n    Examples\n    --------\n    Calculated for hexane from the PR EOS at 299 K and 1 MPa (liquid):\n    \n    >>> phase_identification_parameter_phase(-20518995218.2, 0.000130229900874, \n    ... 582169.397484, -3.66431747236e+12, 4.48067893805e+17)\n    'l'\n\n    References\n    ----------\n    .. [1] Venkatarathnam, G., and L. R. Oellrich. \"Identification of the Phase\n       of a Fluid Using Partial Derivatives of Pressure, Volume, and \n       Temperature without Reference to Saturation Properties: Applications in \n       Phase Equilibria Calculations.\" Fluid Phase Equilibria 301, no. 2 \n       (February 25, 2011): 225-33. doi:10.1016/j.fluid.2010.12.001.\n    .. [2] Jayanti, Pranava Chaitanya, and G. Venkatarathnam. \"Identification\n       of the Phase of a Substance from the Derivatives of Pressure, Volume and\n       Temperature, without Prior Knowledge of Saturation Properties: Extension\n       to Solid Phase.\" Fluid Phase Equilibria 425 (October 15, 2016): 269-277.\n       doi:10.1016/j.fluid.2016.06.001.\n    '''\n    if d2P_dVdT > 0:\n        return 's'\n    else:\n        PIP = phase_identification_parameter(V=V, dP_dT=dP_dT, dP_dV=dP_dV, \n                                             d2P_dV2=d2P_dV2, d2P_dVdT=d2P_dVdT)\n        return 'l' if PIP > 1 else 'g'\n```"], [306, "python", "profileit", "```python\ndef profileit(field='cumulative'):\n    \"\"\"\n    \u6d4b\u8bd5\u51fd\u6570\u8fd0\u884c\u6d88\u8017\u60c5\u51b5\n\n    :param field: \u8f93\u51fa\u5185\u5bb9\u6392\u5e8f\u65b9\u5f0f\u3002\n        \u53ef\u9009\u53c2\u6570\u4e3a \"stdname\", \"calls\", \"time\", \"cumulative\"\n    \"\"\"\n    def wrapper(func):\n        @wraps(func)\n        def inner(*args, **kwargs):\n            pro = Profile()\n            pro.runcall(func, *args, **kwargs)\n            stats = Stats(pro)\n            stats.strip_dirs()\n            stats.sort_stats(field)\n            print(\"Profile for {}()\".format(func.__name__))\n            stats.print_stats()\n            stats.print_callers()\n        return inner\n    return wrapper\n```"], [51, "python", "LoadBalancer.create", "```python\ndef create(self, *args, **kwargs):\n        \"\"\"\n        Creates a new LoadBalancer.\n\n        Note: Every argument and parameter given to this method will be\n        assigned to the object.\n\n        Args:\n            name (str): The Load Balancer's name\n            region (str): The slug identifier for a DigitalOcean region\n            algorithm (str, optional): The load balancing algorithm to be\n                used. Currently, it must be either \"round_robin\" or\n                \"least_connections\"\n            forwarding_rules (obj:`list`): A list of `ForwrdingRules` objects\n            health_check (obj, optional): A `HealthCheck` object\n            sticky_sessions (obj, optional): A `StickySessions` object\n            redirect_http_to_https (bool, optional): A boolean indicating\n                whether HTTP requests to the Load Balancer should be\n                redirected to HTTPS\n            droplet_ids (obj:`list` of `int`): A list of IDs representing\n                Droplets to be added to the Load Balancer (mutually\n                exclusive with 'tag')\n            tag (str): A string representing a DigitalOcean Droplet tag\n                (mutually exclusive with 'droplet_ids')\n        \"\"\"\n        rules_dict = [rule.__dict__ for rule in self.forwarding_rules]\n\n        params = {'name': self.name, 'region': self.region,\n                  'forwarding_rules': rules_dict,\n                  'redirect_http_to_https': self.redirect_http_to_https}\n\n        if self.droplet_ids and self.tag:\n            raise ValueError('droplet_ids and tag are mutually exclusive args')\n        elif self.tag:\n            params['tag'] = self.tag\n        else:\n            params['droplet_ids'] = self.droplet_ids\n\n        if self.algorithm:\n            params['algorithm'] = self.algorithm\n        if self.health_check:\n            params['health_check'] = self.health_check.__dict__\n        if self.sticky_sessions:\n            params['sticky_sessions'] = self.sticky_sessions.__dict__\n\n        data = self.get_data('load_balancers/', type=POST, params=params)\n\n        if data:\n            self.id = data['load_balancer']['id']\n            self.ip = data['load_balancer']['ip']\n            self.algorithm = data['load_balancer']['algorithm']\n            self.health_check = HealthCheck(\n                **data['load_balancer']['health_check'])\n            self.sticky_sessions = StickySesions(\n                **data['load_balancer']['sticky_sessions'])\n            self.droplet_ids = data['load_balancer']['droplet_ids']\n            self.status = data['load_balancer']['status']\n            self.created_at = data['load_balancer']['created_at']\n\n        return self\n```"], [39, "python", "PostgresDB.rename", "```python\ndef rename(self, from_name, to_name):\n        \"\"\"Renames an existing database.\"\"\"\n        log.info('renaming database from %s to %s' % (from_name, to_name))\n        self._run_stmt('alter database %s rename to %s' % (from_name, to_name))\n```"], [76, "python", "XmlElement.siblings", "```python\ndef siblings(self, name=None):\n        \"\"\"\n        Yields all siblings of this node (not including the node itself).\n\n        :param name: If specified, only consider elements with this tag name\n        \"\"\"\n        if self.parent and self.index:\n            for c in self.parent._children:\n                if c.index != self.index and (name is None or name == c.tagname):\n                    yield c\n```"], [99, "python", "KNNAnomalyClassifierRegion._recomputeRecordFromKNN", "```python\ndef _recomputeRecordFromKNN(self, record):\n    \"\"\"\n    returns the classified labeling of record\n    \"\"\"\n    inputs = {\n      \"categoryIn\": [None],\n      \"bottomUpIn\": self._getStateAnomalyVector(record),\n    }\n\n    outputs = {\"categoriesOut\": numpy.zeros((1,)),\n               \"bestPrototypeIndices\":numpy.zeros((1,)),\n               \"categoryProbabilitiesOut\":numpy.zeros((1,))}\n\n    # Only use points before record to classify and after the wait period.\n    classifier_indexes = numpy.array(\n        self._knnclassifier.getParameter('categoryRecencyList'))\n    valid_idx = numpy.where(\n        (classifier_indexes >= self.getParameter('trainRecords')) &\n        (classifier_indexes < record.ROWID)\n      )[0].tolist()\n\n    if len(valid_idx) == 0:\n      return None\n\n    self._knnclassifier.setParameter('inferenceMode', None, True)\n    self._knnclassifier.setParameter('learningMode', None, False)\n    self._knnclassifier.compute(inputs, outputs)\n    self._knnclassifier.setParameter('learningMode', None, True)\n\n    classifier_distances = self._knnclassifier.getLatestDistances()\n    valid_distances = classifier_distances[valid_idx]\n    if valid_distances.min() <= self._classificationMaxDist:\n      classifier_indexes_prev = classifier_indexes[valid_idx]\n      rowID = classifier_indexes_prev[valid_distances.argmin()]\n      indexID = numpy.where(classifier_indexes == rowID)[0][0]\n      category = self._knnclassifier.getCategoryList()[indexID]\n      return category\n    return None\n```"], [368, "python", "timestamp", "```python\ndef timestamp(t = None, forfilename=False):\n    \"\"\"Returns a human-readable timestamp given a Unix timestamp 't' or\n    for the current time. The Unix timestamp is the number of seconds since\n    start of epoch (1970-01-01 00:00:00).\n    When forfilename is True, then spaces and semicolons are replace with\n    hyphens. The returned string is usable as a (part of a) filename. \"\"\"\n\n    datetimesep = ' '\n    timesep     = ':'\n    if forfilename:\n        datetimesep = '-'\n        timesep     = '-'\n\n    return time.strftime('%Y-%m-%d' + datetimesep +\n                         '%H' + timesep + '%M' + timesep + '%S',\n                         time.localtime(t))\n```"], [122, "python", "DeploySatchel.lock", "```python\ndef lock(self):\n        \"\"\"\n        Marks the remote server as currently being deployed to.\n        \"\"\"\n        self.init()\n        r = self.local_renderer\n        if self.file_exists(r.env.lockfile_path):\n            raise exceptions.AbortDeployment('Lock file %s exists. Perhaps another deployment is currently underway?' % r.env.lockfile_path)\n        else:\n            self.vprint('Locking %s.' % r.env.lockfile_path)\n            r.env.hostname = socket.gethostname()\n            r.run_or_local('echo \"{hostname}\" > {lockfile_path}')\n```"], [192, "python", "md_dimension_info", "```python\ndef md_dimension_info(name, node):\n    \"\"\"Extract metadata Dimension Info from an xml node\"\"\"\n    def _get_value(child_name):\n        return getattr(node.find(child_name), 'text', None)\n\n    resolution = _get_value('resolution')\n    defaultValue = node.find(\"defaultValue\")\n    strategy = defaultValue.find(\"strategy\") if defaultValue is not None else None\n    strategy = strategy.text if strategy is not None else None\n    return DimensionInfo(\n        name,\n        _get_value('enabled') == 'true',\n        _get_value('presentation'),\n        int(resolution) if resolution else None,\n        _get_value('units'),\n        _get_value('unitSymbol'),\n        strategy,\n        _get_value('attribute'),\n        _get_value('endAttribute'),\n        _get_value('referenceValue'),\n        _get_value('nearestMatchEnabled')\n    )\n```"], [35, "python", "TicTacToe.k_in_row", "```python\ndef k_in_row(self, board, move, player, (delta_x, delta_y)):\n        \"Return true if there is a line through move on board for player.\"\n        x, y = move\n        n = 0 # n is number of moves in row\n        while board.get((x, y)) == player:\n            n += 1\n            x, y = x + delta_x, y + delta_y\n        x, y = move\n        while board.get((x, y)) == player:\n            n += 1\n            x, y = x - delta_x, y - delta_y\n        n -= 1 # Because we counted move itself twice\n        return n >= self.k\n```"], [2, "python", "keypoint_random_resize", "```python\ndef keypoint_random_resize(image, annos, mask=None, zoom_range=(0.8, 1.2)):\n    \"\"\"Randomly resize an image and corresponding keypoints.\n    The height and width of image will be changed independently, so the scale will be changed.\n\n    Parameters\n    -----------\n    image : 3 channel image\n        The given image for augmentation.\n    annos : list of list of floats\n        The keypoints annotation of people.\n    mask : single channel image or None\n        The mask if available.\n    zoom_range : tuple of two floats\n        The minimum and maximum factor to zoom in or out, e.g (0.5, 1) means zoom out 1~2 times.\n\n    Returns\n    ----------\n    preprocessed image, annos, mask\n\n    \"\"\"\n    height = image.shape[0]\n    width = image.shape[1]\n    _min, _max = zoom_range\n    scalew = np.random.uniform(_min, _max)\n    scaleh = np.random.uniform(_min, _max)\n\n    neww = int(width * scalew)\n    newh = int(height * scaleh)\n\n    dst = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)\n    if mask is not None:\n        mask = cv2.resize(mask, (neww, newh), interpolation=cv2.INTER_AREA)\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in annos:  # TODO : speed up with affine transform\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n    if mask is not None:\n        return dst, adjust_joint_list, mask\n    else:\n        return dst, adjust_joint_list, None\n```"], [1, "python", "numpy_array_2d_to_fits", "```python\ndef numpy_array_2d_to_fits(array_2d, file_path, overwrite=False):\n    \"\"\"Write a 2D NumPy array to a .fits file.\n\n    Before outputting a NumPy array, the array is flipped upside-down using np.flipud. This is so that the arrays \\\n    appear the same orientation as .fits files loaded in DS9.\n\n    Parameters\n    ----------\n    array_2d : ndarray\n        The 2D array that is written to fits.\n    file_path : str\n        The full path of the file that is output, including the file name and '.fits' extension.\n    overwrite : bool\n        If True and a file already exists with the input file_path the .fits file is overwritten. If False, an error \\\n        will be raised.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    array_2d = np.ones((5,5))\n    numpy_array_to_fits(array=array_2d, file_path='/path/to/file/filename.fits', overwrite=True)\n    \"\"\"\n    if overwrite and os.path.exists(file_path):\n        os.remove(file_path)\n\n    new_hdr = fits.Header()\n    hdu = fits.PrimaryHDU(np.flipud(array_2d), new_hdr)\n    hdu.writeto(file_path)\n```"], [5, "python", "StreamSASLHandler._process_sasl_challenge", "```python\ndef _process_sasl_challenge(self, stream, element):\n        \"\"\"Process incoming <sasl:challenge/> element.\n\n        [initiating entity only]\n        \"\"\"\n        if not self.authenticator:\n            logger.debug(\"Unexpected SASL challenge\")\n            return False\n\n        content = element.text.encode(\"us-ascii\")\n        ret = self.authenticator.challenge(a2b_base64(content))\n        if isinstance(ret, sasl.Response):\n            element = ElementTree.Element(RESPONSE_TAG)\n            element.text = ret.encode()\n        else:\n            element = ElementTree.Element(ABORT_TAG)\n\n        stream.write_element(element)\n\n        if isinstance(ret, sasl.Failure):\n            stream.disconnect()\n            raise SASLAuthenticationFailed(\"SASL authentication failed\")\n\n        return True\n```"]]}, "_runtime": 632.7489721775055, "_timestamp": 1571435653.8750713, "_step": 35}
